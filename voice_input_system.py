#!/usr/bin/env python3
"""
é«˜åº¦éŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ 
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ãƒ»æ„Ÿæƒ…åˆ†æãƒ»ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³å­¦ç¿’
"""

import streamlit as st
import numpy as np
import librosa
import soundfile as sf
import pyaudio
import wave
import threading
import time
import json
import tempfile
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import webrtcvad
from faster_whisper import WhisperModel
import pyworld as pw
from scipy import signal
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class VoiceEmotionAnalyzer:
    """éŸ³å£°æ„Ÿæƒ…åˆ†æå™¨"""
    
    def __init__(self):
        self.name = "voice_emotion_analyzer"
        self.description = "éŸ³å£°ã‹ã‚‰æ„Ÿæƒ…ãƒ»ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆ†æ"
        
        # æ„Ÿæƒ…åˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        self.emotion_labels = ["neutral", "happy", "sad", "angry", "tired", "excited"]
        
        # ç‰¹å¾´é‡ã®æ­£è¦åŒ–
        self.scaler = StandardScaler()
        
        # åˆ†æå±¥æ­´
        self.analysis_history = []
    
    def extract_voice_features(self, audio_data: np.ndarray, sample_rate: int) -> Dict:
        """éŸ³å£°ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        try:
            features = {}
            
            # 1. ãƒ”ãƒƒãƒï¼ˆåŸºæœ¬å‘¨æ³¢æ•°ï¼‰åˆ†æ
            f0, time_axis = pw.harvest(audio_data, sample_rate)
            f0_clean = f0[f0 > 0]  # æœ‰å£°éŸ³ã®ã¿
            
            if len(f0_clean) > 0:
                features['pitch_mean'] = np.mean(f0_clean)
                features['pitch_std'] = np.std(f0_clean)
                features['pitch_range'] = np.max(f0_clean) - np.min(f0_clean)
                features['pitch_slope'] = self._calculate_pitch_slope(f0_clean)
            else:
                # ç„¡å£°ã®å ´åˆ
                features['pitch_mean'] = 0
                features['pitch_std'] = 0
                features['pitch_range'] = 0
                features['pitch_slope'] = 0
            
            # 2. ãƒ†ãƒ³ãƒãƒ»ãƒªã‚ºãƒ åˆ†æ
            tempo, beats = librosa.beat.beat_track(y=audio_data, sr=sample_rate)
            features['tempo'] = tempo
            features['beat_regularity'] = self._calculate_beat_regularity(beats)
            
            # 3. éŸ³è‰²ãƒ»ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹å¾´
            mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)
            features['mfcc_mean'] = np.mean(mfccs, axis=1).tolist()
            features['mfcc_std'] = np.std(mfccs, axis=1).tolist()
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰
            spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)
            features['spectral_centroid_mean'] = np.mean(spectral_centroids)
            features['spectral_centroid_std'] = np.std(spectral_centroids)
            
            # 4. ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ»éŸ³é‡
            rms = librosa.feature.rms(y=audio_data)
            features['energy_mean'] = np.mean(rms)
            features['energy_std'] = np.std(rms)
            features['energy_range'] = np.max(rms) - np.min(rms)
            
            # 5. è©±ã—é€Ÿåº¦
            duration = len(audio_data) / sample_rate
            features['duration'] = duration
            features['speech_rate'] = self._estimate_speech_rate(audio_data, sample_rate)
            
            # 6. å£°è³ªç‰¹å¾´
            features['voice_quality'] = self._analyze_voice_quality(audio_data, sample_rate)
            
            return features
            
        except Exception as e:
            print(f"éŸ³å£°ç‰¹å¾´æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return self._get_default_features()
    
    def _calculate_pitch_slope(self, f0: np.ndarray) -> float:
        """ãƒ”ãƒƒãƒã®å‚¾ãã‚’è¨ˆç®—"""
        if len(f0) < 2:
            return 0.0
        
        x = np.arange(len(f0))
        slope, _ = np.polyfit(x, f0, 1)
        return slope
    
    def _calculate_beat_regularity(self, beats: np.ndarray) -> float:
        """ãƒ“ãƒ¼ãƒˆã®è¦å‰‡æ€§ã‚’è¨ˆç®—"""
        if len(beats) < 2:
            return 0.0
        
        intervals = np.diff(beats)
        return 1.0 / (np.std(intervals) + 1e-8)
    
    def _estimate_speech_rate(self, audio_data: np.ndarray, sample_rate: int) -> float:
        """è©±ã—é€Ÿåº¦ã‚’æ¨å®šï¼ˆéŸ³ç¯€/ç§’ï¼‰"""
        # ç°¡æ˜“çš„ãªéŸ³ç¯€æ¤œå‡º
        energy = librosa.feature.rms(y=audio_data)[0]
        threshold = np.mean(energy) + np.std(energy)
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒé–¾å€¤ã‚’è¶…ãˆã‚‹ç‚¹ã‚’æ¤œå‡º
        peaks = signal.find_peaks(energy, height=threshold)[0]
        
        if len(peaks) < 2:
            return 0.0
        
        duration = len(audio_data) / sample_rate
        return len(peaks) / duration
    
    def _analyze_voice_quality(self, audio_data: np.ndarray, sample_rate: int) -> Dict:
        """å£°è³ªã‚’åˆ†æ"""
        try:
            # ãƒãƒ¼ãƒ¢ãƒ‹ã‚¯ã‚¹ãƒ»ãƒã‚¤ã‚ºæ¯”
            f0, time_axis = pw.harvest(audio_data, sample_rate)
            sp = pw.cheaptrick(audio_data, f0, time_axis, sample_rate)
            ap = pw.d4c(audio_data, f0, time_axis, sample_rate)
            
            # HNRã®å¹³å‡å€¤
            hnr_values = []
            for i in range(len(f0)):
                if f0[i] > 0:
                    harmonic_energy = np.sum(sp[i]**2)
                    total_energy = np.sum(sp[i]**2 + ap[i]**2)
                    hnr = harmonic_energy / (total_energy + 1e-8)
                    hnr_values.append(hnr)
            
            if hnr_values:
                hnr_mean = np.mean(hnr_values)
                hnr_std = np.std(hnr_values)
            else:
                hnr_mean = 0.0
                hnr_std = 0.0
            
            return {
                'hnr_mean': hnr_mean,
                'hnr_std': hnr_std,
                'breathiness': self._calculate_breathiness(audio_data, sample_rate)
            }
            
        except Exception as e:
            print(f"å£°è³ªåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'hnr_mean': 0.0, 'hnr_std': 0.0, 'breathiness': 0.0}
    
    def _calculate_breathiness(self, audio_data: np.ndarray, sample_rate: int) -> float:
        """æ¯ã®æˆåˆ†ã‚’è¨ˆç®—"""
        # é«˜å‘¨æ³¢æˆåˆ†ã®å‰²åˆ
        high_freq = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, fmax=8000)
        low_freq = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, fmin=0, fmax=4000)
        
        high_energy = np.mean(high_freq)
        low_energy = np.mean(low_freq)
        
        return high_energy / (low_energy + 1e-8)
    
    def _get_default_features(self) -> Dict:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡"""
        return {
            'pitch_mean': 0.0, 'pitch_std': 0.0, 'pitch_range': 0.0, 'pitch_slope': 0.0,
            'tempo': 120.0, 'beat_regularity': 0.0,
            'mfcc_mean': [0.0]*13, 'mfcc_std': [0.0]*13,
            'spectral_centroid_mean': 0.0, 'spectral_centroid_std': 0.0,
            'energy_mean': 0.0, 'energy_std': 0.0, 'energy_range': 0.0,
            'duration': 0.0, 'speech_rate': 0.0,
            'voice_quality': {'hnr_mean': 0.0, 'hnr_std': 0.0, 'breathiness': 0.0}
        }
    
    def classify_emotion(self, features: Dict) -> Dict:
        """æ„Ÿæƒ…ã‚’åˆ†é¡ï¼ˆç°¡æ˜“ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰"""
        try:
            emotion_scores = {}
            
            # ãƒ”ãƒƒãƒã«åŸºã¥ãæ„Ÿæƒ…åˆ¤å®š
            pitch = features['pitch_mean']
            pitch_std = features['pitch_std']
            
            if pitch > 200 and pitch_std > 20:
                emotion_scores['excited'] = 0.8
                emotion_scores['happy'] = 0.6
            elif pitch > 150:
                emotion_scores['happy'] = 0.7
                emotion_scores['excited'] = 0.4
            elif pitch < 100 and pitch_std < 10:
                emotion_scores['tired'] = 0.8
                emotion_scores['sad'] = 0.5
            elif pitch < 120:
                emotion_scores['sad'] = 0.6
                emotion_scores['tired'] = 0.4
            else:
                emotion_scores['neutral'] = 0.7
            
            # ãƒ†ãƒ³ãƒã«åŸºã¥ãæ„Ÿæƒ…åˆ¤å®š
            tempo = features['tempo']
            speech_rate = features['speech_rate']
            
            if tempo > 140 or speech_rate > 4:
                emotion_scores['excited'] = emotion_scores.get('excited', 0) + 0.3
                emotion_scores['angry'] = emotion_scores.get('angry', 0) + 0.2
            elif tempo < 80 or speech_rate < 2:
                emotion_scores['tired'] = emotion_scores.get('tired', 0) + 0.3
                emotion_scores['sad'] = emotion_scores.get('sad', 0) + 0.2
            
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã«åŸºã¥ãæ„Ÿæƒ…åˆ¤å®š
            energy = features['energy_mean']
            if energy > 0.1:
                emotion_scores['excited'] = emotion_scores.get('excited', 0) + 0.2
                emotion_scores['angry'] = emotion_scores.get('angry', 0) + 0.2
            elif energy < 0.02:
                emotion_scores['tired'] = emotion_scores.get('tired', 0) + 0.2
                emotion_scores['sad'] = emotion_scores.get('sad', 0) + 0.1
            
            # å£°è³ªã«åŸºã¥ãæ„Ÿæƒ…åˆ¤å®š
            breathiness = features['voice_quality']['breathiness']
            if breathiness > 0.3:
                emotion_scores['tired'] = emotion_scores.get('tired', 0) + 0.2
            
            # ã‚¹ã‚³ã‚¢ã‚’æ­£è¦åŒ–
            total_score = sum(emotion_scores.values())
            if total_score > 0:
                emotion_scores = {k: v/total_score for k, v in emotion_scores.items()}
            else:
                emotion_scores['neutral'] = 1.0
            
            # æœ€ã‚‚é«˜ã„æ„Ÿæƒ…ã‚’å–å¾—
            dominant_emotion = max(emotion_scores, key=emotion_scores.get)
            
            return {
                'dominant_emotion': dominant_emotion,
                'emotion_scores': emotion_scores,
                'confidence': emotion_scores[dominant_emotion]
            }
            
        except Exception as e:
            print(f"æ„Ÿæƒ…åˆ†é¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                'dominant_emotion': 'neutral',
                'emotion_scores': {'neutral': 1.0},
                'confidence': 0.5
            }
    
    def analyze_voice(self, audio_data: np.ndarray, sample_rate: int) -> Dict:
        """éŸ³å£°ã‚’å®Œå…¨ã«åˆ†æ"""
        # ç‰¹å¾´é‡æŠ½å‡º
        features = self.extract_voice_features(audio_data, sample_rate)
        
        # æ„Ÿæƒ…åˆ†é¡
        emotion_result = self.classify_emotion(features)
        
        # çµæœã‚’çµ±åˆ
        analysis_result = {
            'timestamp': datetime.now().isoformat(),
            'features': features,
            'emotion': emotion_result,
            'intonation_profile': self._create_intonation_profile(features)
        }
        
        # å±¥æ­´ã«ä¿å­˜
        self.analysis_history.append(analysis_result)
        
        return analysis_result
    
    def _create_intonation_profile(self, features: Dict) -> Dict:
        """ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        return {
            'pitch_characteristics': {
                'mean': features['pitch_mean'],
                'variability': features['pitch_std'],
                'range': features['pitch_range'],
                'trend': features['pitch_slope']
            },
            'rhythm_characteristics': {
                'tempo': features['tempo'],
                'speech_rate': features['speech_rate'],
                'regularity': features['beat_regularity']
            },
            'voice_characteristics': {
                'energy': features['energy_mean'],
                'brightness': features['spectral_centroid_mean'],
                'breathiness': features['voice_quality']['breathiness']
            }
        }

class IntonationMirroringSystem:
    """ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒŸãƒ©ãƒ¼ãƒªãƒ³ã‚°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.name = "intonation_mirroring"
        self.description = "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å­¦ç¿’ãƒ»æ¨¡å€£"
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
        self.adaptation_log_file = "voice_adaptation_log.json"
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        self.user_voice_profile = {
            'pitch_mean': 150.0,
            'pitch_std': 20.0,
            'speech_rate': 3.0,
            'tempo': 120.0,
            'energy_mean': 0.05,
            'intonation_patterns': [],
            'emotion_responses': {}
        }
        
        # AIéŸ³å£°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.ai_voice_params = {
            'pitch_scale': 1.0,
            'speed_scale': 1.0,
            'volume_scale': 1.0,
            'intonation_emphasis': 1.0
        }
        
        # å­¦ç¿’å±¥æ­´
        self.learning_history = []
        
        self.load_adaptation_data()
    
    def load_adaptation_data(self):
        """é©å¿œãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if Path(self.adaptation_log_file).exists():
                with open(self.adaptation_log_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.user_voice_profile = data.get('user_profile', self.user_voice_profile)
                    self.ai_voice_params = data.get('ai_params', self.ai_voice_params)
                    self.learning_history = data.get('history', [])
        except Exception as e:
            print(f"é©å¿œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def save_adaptation_data(self):
        """é©å¿œãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            data = {
                'user_profile': self.user_voice_profile,
                'ai_params': self.ai_voice_params,
                'history': self.learning_history[-100:],  # æœ€æ–°100ä»¶
                'last_updated': datetime.now().isoformat()
            }
            with open(self.adaptation_log_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"é©å¿œãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def learn_from_user_voice(self, voice_analysis: Dict):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å£°ã‹ã‚‰å­¦ç¿’"""
        try:
            features = voice_analysis['features']
            emotion = voice_analysis['emotion']['dominant_emotion']
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
            self._update_user_profile(features)
            
            # æ„Ÿæƒ…ã¨ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ã®é–¢é€£ã‚’å­¦ç¿’
            self._learn_emotion_intonation_mapping(emotion, features)
            
            # AIéŸ³å£°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´
            self._adapt_ai_voice_parameters(features)
            
            # å­¦ç¿’å±¥æ­´ã«è¨˜éŒ²
            learning_record = {
                'timestamp': datetime.now().isoformat(),
                'user_features': features,
                'user_emotion': emotion,
                'ai_params_before': self.ai_voice_params.copy(),
                'adaptation_type': 'user_voice_learning'
            }
            
            self.learning_history.append(learning_record)
            self.save_adaptation_data()
            
            return True
            
        except Exception as e:
            print(f"éŸ³å£°å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _update_user_profile(self, features: Dict):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
        # æŒ‡æ•°ç§»å‹•å¹³å‡ã§æ»‘ã‚‰ã‹ã«æ›´æ–°
        alpha = 0.1  # å­¦ç¿’ç‡
        
        self.user_voice_profile['pitch_mean'] = (
            alpha * features['pitch_mean'] + 
            (1 - alpha) * self.user_voice_profile['pitch_mean']
        )
        
        self.user_voice_profile['pitch_std'] = (
            alpha * features['pitch_std'] + 
            (1 - alpha) * self.user_voice_profile['pitch_std']
        )
        
        self.user_voice_profile['speech_rate'] = (
            alpha * features['speech_rate'] + 
            (1 - alpha) * self.user_voice_profile['speech_rate']
        )
        
        self.user_voice_profile['tempo'] = (
            alpha * features['tempo'] + 
            (1 - alpha) * self.user_voice_profile['tempo']
        )
        
        self.user_voice_profile['energy_mean'] = (
            alpha * features['energy_mean'] + 
            (1 - alpha) * self.user_voice_profile['energy_mean']
        )
    
    def _learn_emotion_intonation_mapping(self, emotion: str, features: Dict):
        """æ„Ÿæƒ…ã¨ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ã®é–¢é€£ã‚’å­¦ç¿’"""
        if emotion not in self.user_voice_profile['emotion_responses']:
            self.user_voice_profile['emotion_responses'][emotion] = {
                'pitch_mean': features['pitch_mean'],
                'pitch_std': features['pitch_std'],
                'speech_rate': features['speech_rate'],
                'energy_mean': features['energy_mean'],
                'sample_count': 1
            }
        else:
            # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
            existing = self.user_voice_profile['emotion_responses'][emotion]
            alpha = 0.2
            
            existing['pitch_mean'] = (
                alpha * features['pitch_mean'] + 
                (1 - alpha) * existing['pitch_mean']
            )
            
            existing['pitch_std'] = (
                alpha * features['pitch_std'] + 
                (1 - alpha) * existing['pitch_std']
            )
            
            existing['speech_rate'] = (
                alpha * features['speech_rate'] + 
                (1 - alpha) * existing['speech_rate']
            )
            
            existing['energy_mean'] = (
                alpha * features['energy_mean'] + 
                (1 - alpha) * existing['energy_mean']
            )
            
            existing['sample_count'] += 1
    
    def _adapt_ai_voice_parameters(self, features: Dict):
        """AIéŸ³å£°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©å¿œ"""
        # ãƒ”ãƒƒãƒã®é©å¿œ
        user_pitch = features['pitch_mean']
        baseline_pitch = 150.0  # åŸºæº–ãƒ”ãƒƒãƒ
        
        pitch_ratio = user_pitch / baseline_pitch
        self.ai_voice_params['pitch_scale'] = np.clip(pitch_ratio, 0.5, 2.0)
        
        # è©±ã—é€Ÿåº¦ã®é©å¿œ
        user_speed = features['speech_rate']
        baseline_speed = 3.0  # åŸºæº–é€Ÿåº¦
        
        speed_ratio = user_speed / baseline_speed
        self.ai_voice_params['speed_scale'] = np.clip(speed_ratio, 0.5, 2.0)
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆéŸ³é‡ï¼‰ã®é©å¿œ
        user_energy = features['energy_mean']
        baseline_energy = 0.05  # åŸºæº–ã‚¨ãƒãƒ«ã‚®ãƒ¼
        
        energy_ratio = user_energy / baseline_energy
        self.ai_voice_params['volume_scale'] = np.clip(energy_ratio, 0.5, 2.0)
        
        # ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ã®å¼·èª¿
        pitch_variability = features['pitch_std']
        baseline_variability = 20.0
        
        variability_ratio = pitch_variability / baseline_variability
        self.ai_voice_params['intonation_emphasis'] = np.clip(variability_ratio, 0.5, 2.0)
    
    def get_adapted_voice_params(self, target_emotion: str = None) -> Dict:
        """é©å¿œã•ã‚ŒãŸéŸ³å£°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—"""
        params = self.ai_voice_params.copy()
        
        if target_emotion and target_emotion in self.user_voice_profile['emotion_responses']:
            emotion_profile = self.user_voice_profile['emotion_responses'][target_emotion]
            
            # æ„Ÿæƒ…ç‰¹æœ‰ã®èª¿æ•´ã‚’é©ç”¨
            emotion_pitch_ratio = emotion_profile['pitch_mean'] / 150.0
            params['pitch_scale'] *= emotion_pitch_ratio
            
            emotion_speed_ratio = emotion_profile['speech_rate'] / 3.0
            params['speed_scale'] *= emotion_speed_ratio
            
            emotion_energy_ratio = emotion_profile['energy_mean'] / 0.05
            params['volume_scale'] *= emotion_energy_ratio
        
        return params
    
    def get_learning_summary(self) -> Dict:
        """å­¦ç¿’ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        return {
            'total_interactions': len(self.learning_history),
            'user_voice_profile': self.user_voice_profile,
            'current_ai_params': self.ai_voice_params,
            'learned_emotions': list(self.user_voice_profile['emotion_responses'].keys()),
            'adaptation_level': self._calculate_adaptation_level()
        }
    
    def _calculate_adaptation_level(self) -> float:
        """é©å¿œãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—"""
        if len(self.learning_history) == 0:
            return 0.0
        
        # å­¦ç¿’å›æ•°ã«åŸºã¥ãé©å¿œãƒ¬ãƒ™ãƒ«
        interaction_count = len(self.learning_history)
        
        # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§é£½å’Œã•ã›ã‚‹
        adaptation_level = min(1.0, np.log10(interaction_count + 1) / np.log10(100))
        
        return adaptation_level

class RealTimeVoiceInput:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.name = "realtime_voice_input"
        self.description = "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã¨æ„Ÿæƒ…åˆ†æ"
        
        # Whisperãƒ¢ãƒ‡ãƒ«
        self.whisper_model = None
        self.model_loaded = False
        
        # éŸ³å£°æ„Ÿæƒ…åˆ†æå™¨
        self.emotion_analyzer = VoiceEmotionAnalyzer()
        
        # ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒŸãƒ©ãƒ¼ãƒªãƒ³ã‚°
        self.mirroring_system = IntonationMirroringSystem()
        
        # éŒ²éŸ³è¨­å®š
        self.format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        self.chunk = 1024
        self.record_seconds = 10
        
        # VADï¼ˆVoice Activity Detectionï¼‰
        self.vad = webrtcvad.Vad(2)  # ä¸­ç¨‹åº¦ã®æ„Ÿåº¦
        
        # ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰
        self.wake_word = "ã­ãˆç›¸æ£’"
        self.wake_word_detected = False
        
        # éŒ²éŸ³çŠ¶æ…‹
        self.is_recording = False
        self.is_listening = False
        self.audio_buffer = []
        
        # èªè­˜çµæœ
        self.last_recognition_result = None
        self.last_emotion_analysis = None
        
        self._load_whisper_model()
    
    def _load_whisper_model(self):
        """Whisperãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            # è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            self.whisper_model = WhisperModel("base", compute_type="int8")
            self.model_loaded = True
            print("âœ… Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        except Exception as e:
            print(f"âŒ Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.model_loaded = False
    
    def start_listening(self):
        """å¸¸æ™‚è´å–ã‚’é–‹å§‹"""
        if not self.is_listening:
            self.is_listening = True
            listening_thread = threading.Thread(target=self._listening_loop, daemon=True)
            listening_thread.start()
            return True
        return False
    
    def stop_listening(self):
        """å¸¸æ™‚è´å–ã‚’åœæ­¢"""
        self.is_listening = False
        return True
    
    def _listening_loop(self):
        """å¸¸æ™‚è´å–ãƒ«ãƒ¼ãƒ—"""
        p = pyaudio.PyAudio()
        
        try:
            stream = p.open(
                format=self.format,
                channels=self.channels,
                rate=self.rate,
                input=True,
                frames_per_buffer=self.chunk
            )
            
            print("ğŸ¤ å¸¸æ™‚è´å–é–‹å§‹...")
            
            while self.is_listening:
                try:
                    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                    data = stream.read(self.chunk)
                    audio_chunk = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
                    
                    # VADã§éŸ³å£°æ¤œå‡º
                    is_speech = self._detect_voice_activity(audio_chunk)
                    
                    if is_speech:
                        self.audio_buffer.append(audio_chunk)
                        
                        # ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
                        if not self.wake_word_detected:
                            self._check_wake_word()
                    else:
                        # ç„¡éŸ³åŒºé–“ã§ãƒãƒƒãƒ•ã‚¡ã‚’å‡¦ç†
                        if len(self.audio_buffer) > 0:
                            self._process_audio_buffer()
                            self.audio_buffer = []
                
                except Exception as e:
                    print(f"è´å–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    time.sleep(0.1)
            
        finally:
            stream.stop_stream()
            stream.close()
            p.terminate()
    
    def _detect_voice_activity(self, audio_chunk: np.ndarray) -> bool:
        """éŸ³å£°æ´»å‹•æ¤œå‡º"""
        try:
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’16bitã«å¤‰æ›
            audio_int16 = (audio_chunk * 32767).astype(np.int16)
            
            # VADã§åˆ¤å®š
            is_speech = self.vad.is_speech(audio_int16.tobytes(), self.rate)
            return is_speech
            
        except Exception as e:
            print(f"VADã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _check_wake_word(self):
        """ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œå‡º"""
        try:
            if len(self.audio_buffer) < self.rate * 2:  # 2ç§’ä»¥ä¸Šã®éŸ³å£°ãŒå¿…è¦
                return
            
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            audio_data = np.concatenate(self.audio_buffer)
            
            # Whisperã§èªè­˜
            if self.model_loaded:
                segments, _ = self.whisper_model.transcribe(audio_data, language="ja")
                
                for segment in segments:
                    text = segment.text.strip()
                    if self.wake_word in text:
                        self.wake_word_detected = True
                        print(f"ğŸ¯ ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: {text}")
                        
                        # VRMã‚¢ãƒã‚¿ãƒ¼ã«é€šçŸ¥
                        self._notify_vrm("wake_word_detected")
                        
                        break
            
        except Exception as e:
            print(f"ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _process_audio_buffer(self):
        """éŸ³å£°ãƒãƒƒãƒ•ã‚¡ã‚’å‡¦ç†"""
        try:
            if len(self.audio_buffer) < self.rate * 0.5:  # 0.5ç§’æœªæº€ã¯ç„¡è¦–
                return
            
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            audio_data = np.concatenate(self.audio_buffer)
            
            # Whisperã§èªè­˜
            if self.model_loaded:
                segments, _ = self.whisper_model.transcribe(audio_data, language="ja")
                
                for segment in segments:
                    text = segment.text.strip()
                    if text and len(text) > 1:
                        # æ„Ÿæƒ…åˆ†æ
                        emotion_analysis = self.emotion_analyzer.analyze_voice(audio_data, self.rate)
                        
                        # ãƒŸãƒ©ãƒ¼ãƒªãƒ³ã‚°å­¦ç¿’
                        self.mirroring_system.learn_from_user_voice(emotion_analysis)
                        
                        # çµæœã‚’ä¿å­˜
                        self.last_recognition_result = {
                            'text': text,
                            'timestamp': datetime.now().isoformat(),
                            'confidence': segment.avg_logprob
                        }
                        
                        self.last_emotion_analysis = emotion_analysis
                        
                        print(f"ğŸ¤ èªè­˜çµæœ: {text}")
                        print(f"ğŸ˜Š æ„Ÿæƒ…: {emotion_analysis['emotion']['dominant_emotion']}")
                        
                        # VRMã‚¢ãƒã‚¿ãƒ¼ã«é€šçŸ¥
                        self._notify_vrm("voice_input", {
                            'text': text,
                            'emotion': emotion_analysis['emotion']['dominant_emotion']
                        })
                        
                        break
            
        except Exception as e:
            print(f"éŸ³å£°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _notify_vrm(self, event_type: str, data: Dict = None):
        """VRMã‚¢ãƒã‚¿ãƒ¼ã«é€šçŸ¥"""
        # VRMé€£æºæ©Ÿèƒ½ã¯ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã§å®Ÿè£…
        pass
    
    def record_manual_input(self, duration: int = 5) -> Dict:
        """æ‰‹å‹•éŒ²éŸ³"""
        try:
            p = pyaudio.PyAudio()
            
            stream = p.open(
                format=self.format,
                channels=self.channels,
                rate=self.rate,
                input=True,
                frames_per_buffer=self.chunk
            )
            
            frames = []
            print(f"ğŸ¤ {duration}ç§’é–“éŒ²éŸ³é–‹å§‹...")
            
            for _ in range(int(self.rate / self.chunk * duration)):
                data = stream.read(self.chunk)
                frames.append(data)
            
            print("ğŸ¤ éŒ²éŸ³å®Œäº†")
            
            stream.stop_stream()
            stream.close()
            p.terminate()
            
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            audio_data = b''.join(frames)
            audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
            
            # Whisperã§èªè­˜
            result = {'text': '', 'emotion': None}
            
            if self.model_loaded:
                segments, _ = self.whisper_model.transcribe(audio_array, language="ja")
                
                for segment in segments:
                    text = segment.text.strip()
                    if text:
                        # æ„Ÿæƒ…åˆ†æ
                        emotion_analysis = self.emotion_analyzer.analyze_voice(audio_array, self.rate)
                        
                        # ãƒŸãƒ©ãƒ¼ãƒªãƒ³ã‚°å­¦ç¿’
                        self.mirroring_system.learn_from_user_voice(emotion_analysis)
                        
                        result = {
                            'text': text,
                            'emotion': emotion_analysis,
                            'confidence': segment.avg_logprob,
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        self.last_recognition_result = result
                        self.last_emotion_analysis = emotion_analysis
                        
                        break
            
            return result
            
        except Exception as e:
            print(f"æ‰‹å‹•éŒ²éŸ³ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'text': '', 'emotion': None, 'error': str(e)}
    
    def get_last_result(self) -> Dict:
        """æœ€å¾Œã®èªè­˜çµæœã‚’å–å¾—"""
        return {
            'recognition': self.last_recognition_result,
            'emotion': self.last_emotion_analysis,
            'adaptation_summary': self.mirroring_system.get_learning_summary()
        }
    
    def run(self, command: str) -> str:
        """ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        if command == "start_listening":
            if self.start_listening():
                return "å¸¸æ™‚è´å–ã‚’é–‹å§‹ã—ã¾ã—ãŸ"
            else:
                return "ã™ã§ã«è´å–ä¸­ã§ã™"
        
        elif command == "stop_listening":
            if self.stop_listening():
                return "å¸¸æ™‚è´å–ã‚’åœæ­¢ã—ã¾ã—ãŸ"
            else:
                return "è´å–ã—ã¦ã„ã¾ã›ã‚“"
        
        elif command.startswith("record"):
            try:
                parts = command.split()
                duration = int(parts[1]) if len(parts) > 1 else 5
                result = self.record_manual_input(duration)
                if result.get('text'):
                    return f"èªè­˜çµæœ: {result['text']} (æ„Ÿæƒ…: {result['emotion']['emotion']['dominant_emotion']})"
                else:
                    return "éŸ³å£°ãŒèªè­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"
            except:
                return "éŒ²éŸ³ã‚³ãƒãƒ³ãƒ‰å½¢å¼: record [ç§’æ•°]"
        
        elif command == "status":
            summary = self.mirroring_system.get_learning_summary()
            return f"éŸ³å£°å…¥åŠ›çŠ¶æ…‹: è´å–ä¸­={self.is_listening}, å­¦ç¿’å›æ•°={summary['total_interactions']}, é©å¿œãƒ¬ãƒ™ãƒ«={summary['adaptation_level']:.2f}"
        
        elif command == "last_result":
            result = self.get_last_result()
            if result['recognition']:
                return f"æœ€å¾Œã®èªè­˜: {result['recognition']['text']} (æ„Ÿæƒ…: {result['emotion']['emotion']['dominant_emotion']})"
            else:
                return "èªè­˜çµæœãŒã‚ã‚Šã¾ã›ã‚“"
        
        else:
            return "ã‚³ãƒãƒ³ãƒ‰å½¢å¼: start_listening, stop_listening, record [ç§’æ•°], status, last_result"
