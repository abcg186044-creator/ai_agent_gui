#!/usr/bin/env python3
"""
æ°¸ç¶šåŒ–å¯¾å¿œãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import requests
import time
import json
import logging
import shutil

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PersistentModelPreloader:
    def __init__(self):
        self.ollama_host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
        self.models_path = os.getenv('OLLAMA_MODELS_PATH', '/app/data/ollama')
        self.models_to_preload = [
            'llama3.2',
            'llama3.2-vision'
        ]
        self.warmup_prompts = [
            "ã“ã‚“ã«ã¡ã¯",
            "Hello, how are you?",
            "ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ",
            "What is AI?"
        ]
    
    def check_model_exists(self, model_name):
        """ãƒ¢ãƒ‡ãƒ«ãŒæ—¢å­˜ã™ã‚‹ã‹ç¢ºèª"""
        model_path = os.path.join(self.models_path, 'models', model_name)
        return os.path.exists(model_path) and os.path.isdir(model_path)
    
    def wait_for_ollama(self, timeout=300):
        """OllamaãŒèµ·å‹•ã™ã‚‹ã®ã‚’å¾…ã¤"""
        logger.info("ğŸ”„ Ollamaã®èµ·å‹•ã‚’å¾…ã£ã¦ã„ã¾ã™...")
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"{self.ollama_host}/api/tags", timeout=5)
                if response.status_code == 200:
                    logger.info("âœ… OllamaãŒèµ·å‹•ã—ã¾ã—ãŸ")
                    return True
            except requests.exceptions.RequestException:
                pass
            
            logger.info("â³ Ollamaèµ·å‹•ä¸­...")
            time.sleep(5)
        
        logger.error("âŒ Ollamaã®èµ·å‹•ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        return False
    
    def check_models(self):
        """å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª"""
        logger.info("ğŸ” ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèªã—ã¾ã™...")
        
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                available_models = [model['name'] for model in data.get('models', [])]
                
                missing_models = []
                for model in self.models_to_preload:
                    if model not in available_models:
                        missing_models.append(model)
                
                if missing_models:
                    logger.info(f"âš ï¸ æ¬ ã‘ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«: {missing_models}")
                    return False, missing_models
                else:
                    logger.info("âœ… å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
                    return True, []
            else:
                logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return False, []
        
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False, []
    
    def pull_models(self, missing_models):
        """æ¬ ã‘ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ«"""
        for model in missing_models:
            logger.info(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ« {model} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            try:
                response = requests.post(
                    f"{self.ollama_host}/api/pull",
                    json={"name": model},
                    timeout=600  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                )
                
                if response.status_code == 200:
                    logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ« {model} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                else:
                    logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ« {model} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {response.status_code}")
                    return False
                    
            except Exception as e:
                logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ« {model} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                return False
        
        return True
    
    def warmup_models(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—"""
        logger.info("ğŸ”¥ ãƒ¢ãƒ‡ãƒ«ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã™...")
        
        for model in self.models_to_preload:
            logger.info(f"ğŸ”¥ ãƒ¢ãƒ‡ãƒ« {model} ã‚’ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­...")
            
            for prompt in self.warmup_prompts:
                try:
                    response = requests.post(
                        f"{self.ollama_host}/api/generate",
                        json={
                            "model": model,
                            "prompt": prompt,
                            "stream": False
                        },
                        timeout=30
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        logger.info(f"âœ… {model}: {prompt[:30]}... -> {result.get('response', '')[:50]}...")
                    else:
                        logger.warning(f"âš ï¸ {model}: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•—")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ {model}: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                
                time.sleep(1)  # ãƒ¢ãƒ‡ãƒ«é–“ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³
    
    def setup_persistent_storage(self):
        """æ°¸ç¶šåŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        logger.info("ğŸ’¾ æ°¸ç¶šåŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™...")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        models_dir = os.path.join(self.models_path, 'models')
        os.makedirs(models_dir, exist_ok=True)
        
        # ChromaDBãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        chroma_path = os.getenv('CHROMA_DB_PATH', '/app/data/chroma')
        os.makedirs(chroma_path, exist_ok=True)
        
        logger.info("âœ… æ°¸ç¶šåŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
    
    def create_model_cache(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä½œæˆ"""
        logger.info("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¾ã™...")
        
        cache_info = {
            "preloaded_models": self.models_to_preload,
            "last_preload": time.time(),
            "version": "1.0"
        }
        
        cache_file = os.path.join(self.models_path, 'preload_cache.json')
        try:
            with open(cache_file, 'w') as f:
                json.dump(cache_info, f, indent=2)
            logger.info("âœ… ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def check_model_cache(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèª"""
        cache_file = os.path.join(self.models_path, 'preload_cache.json')
        
        if not os.path.exists(cache_file):
            return False
        
        try:
            with open(cache_file, 'r') as f:
                cache_info = json.load(f)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
            cached_models = cache_info.get('preloaded_models', [])
            if all(model in cached_models for model in self.models_to_preload):
                logger.info("âœ… ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã§ã™")
                return True
            else:
                logger.info("âš ï¸ ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åŠ¹ã§ã™")
                return False
                
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def preload(self):
        """ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“"""
        logger.info("ğŸš€ æ°¸ç¶šåŒ–å¯¾å¿œãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™...")
        
        # 1. æ°¸ç¶šåŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        self.setup_persistent_storage()
        
        # 2. Ollamaã®èµ·å‹•ã‚’å¾…ã¤
        if not self.wait_for_ollama():
            return False
        
        # 3. ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèª
        if self.check_model_cache():
            logger.info("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
            return True
        
        # 4. ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèª
        models_ok, missing_models = self.check_models()
        
        # 5. æ¬ ã‘ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if missing_models:
            if not self.pull_models(missing_models):
                return False
        
        # 6. ãƒ¢ãƒ‡ãƒ«ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        self.warmup_models()
        
        # 7. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä½œæˆ
        self.create_model_cache()
        
        logger.info("âœ… æ°¸ç¶šåŒ–å¯¾å¿œãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å®Œäº†")
        return True

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("ğŸ¯ æ°¸ç¶šåŒ–å¯¾å¿œ AI Agent System ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰")
    
    preloader = PersistentModelPreloader()
    
    try:
        success = preloader.preload()
        if success:
            logger.info("ğŸ‰ æ°¸ç¶šåŒ–å¯¾å¿œãƒ—ãƒªãƒ­ãƒ¼ãƒ‰æˆåŠŸ - AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½ã§ã™")
            return 0
        else:
            logger.error("âŒ æ°¸ç¶šåŒ–å¯¾å¿œãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å¤±æ•—")
            return 1
            
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
        return 0
    except Exception as e:
        logger.error(f"âŒ ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
