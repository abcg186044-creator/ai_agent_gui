#!/usr/bin/env python3
"""
AI Agent System - Final Optimized Version
llama3.2 + VRM + RAG + Resource Monitoring + Scheduled Tasks
å®Œå…¨ãªæœ€é©åŒ–AIã‚·ã‚¹ãƒ†ãƒ 
"""

import streamlit as st
import sys
import os
import json
import tempfile
import time
from datetime import datetime
from pathlib import Path
import threading
import queue
import base64
import hashlib

# åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import ollama
    import faster_whisper
    import pyttsx3
    import pyautogui
    import numpy as np
    import pandas as pd
    from openpyxl import load_workbook
    import pymupdf
    from PIL import Image
    import qrcode
    from duckduckgo_search import DDGS
    import chromadb
    from sentence_transformers import SentenceTransformer
    import faiss
    import psutil
    import schedule
except ImportError as e:
    st.error(f"âŒ å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    st.stop()

# æœ€æ–°llama3.2è¨­å®š
class Config:
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    MAIN_MODEL = "llama3.2"
    VISION_MODEL = "llama3.2-vision"
    EMBEDDING_MODEL = "nomic-embed-text:latest"
    
    # Ollamaè¨­å®š
    OLLAMA_HOST = "localhost"
    OLLAMA_PORT = 11434
    
    # éŸ³å£°è¨­å®š
    VOICE_RATE = 200
    VOICE_VOLUME = 0.9
    
    # VRMè¨­å®š
    VRM_MODELS_PATH = "./vrm_models"
    DEFAULT_VRM = "default_avatar.vrm"
    VRM_ANIMATIONS = ["idle", "talking", "thinking", "happy", "sad"]
    
    # é«˜é€Ÿå¿œç­”è¨­å®š
    STREAMING_ENABLED = True
    FAST_RESPONSE_TIMEOUT = 2.0
    MAX_TOKENS_FAST = 512
    MAX_TOKENS_FULL = 4096
    
    # RAGè¨­å®š
    RAG_DB_PATH = "./rag_database"
    SIMILARITY_THRESHOLD = 0.7
    MAX_RAG_RESULTS = 5
    
    # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–è¨­å®š
    CPU_THRESHOLD = 80.0
    MEMORY_THRESHOLD = 80.0
    DISK_THRESHOLD = 10.0  # GB
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
    SCHEDULED_TASKS = [
        ("09:00", "daily_system_check"),
        ("12:00", "daily_summary"),
        ("18:00", "evening_cleanup"),
        ("22:00", "night_backup")
    ]

class RAGSystem:
    """RAG (Retrieval-Augmented Generation) ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.db_path = Config.RAG_DB_PATH
        self.embedding_model = None
        self.vector_index = None
        self.conversation_history = []
        
        # RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(self.db_path, exist_ok=True)
        
    def initialize(self):
        """RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            self._load_existing_data()
            
            # ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
            self._build_vector_index()
            
            return True
        except Exception as e:
            st.error(f"âŒ RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _load_existing_data(self):
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            db_file = os.path.join(self.db_path, "conversations.json")
            if os.path.exists(db_file):
                with open(db_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.conversation_history = data.get("conversations", [])
        except Exception:
            self.conversation_history = []
    
    def _save_data(self):
        """ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        try:
            db_file = os.path.join(self.db_path, "conversations.json")
            data = {
                "conversations": self.conversation_history,
                "last_updated": datetime.now().isoformat()
            }
            with open(db_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _build_vector_index(self):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        try:
            if not self.conversation_history:
                return
            
            # å…¨ä¼šè©±ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
            texts = []
            for conv in self.conversation_history:
                texts.append(conv.get("user_input", ""))
                texts.append(conv.get("ai_response", ""))
            
            if not texts:
                return
            
            # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            embeddings = self.embedding_model.encode(texts)
            
            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
            dimension = embeddings.shape[1]
            self.vector_index = faiss.IndexFlatL2(dimension)
            self.vector_index.add(embeddings)
            
        except Exception as e:
            st.error(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def add_conversation(self, user_input, ai_response):
        """ä¼šè©±ã‚’è¿½åŠ """
        conversation = {
            "id": hashlib.md5(f"{user_input}{ai_response}{datetime.now().isoformat()}".encode()).hexdigest(),
            "timestamp": datetime.now().isoformat(),
            "user_input": user_input,
            "ai_response": ai_response
        }
        
        self.conversation_history.append(conversation)
        self._save_data()
        self._build_vector_index()
    
    def search_similar_conversations(self, query, k=Config.MAX_RAG_RESULTS):
        """é¡ä¼¼ä¼šè©±ã‚’æ¤œç´¢"""
        try:
            if not self.vector_index or not query:
                return []
            
            # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            query_embedding = self.embedding_model.encode([query])
            
            # é¡ä¼¼æ¤œç´¢
            distances, indices = self.vector_index.search(query_embedding, k)
            
            # é¡ä¼¼åº¦ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            similar_conversations = []
            for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
                if dist < (1 - Config.SIMILARITY_THRESHOLD):  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
                    if idx < len(self.conversation_history):
                        similar_conversations.append({
                            "conversation": self.conversation_history[idx],
                            "similarity": 1 - dist
                        })
            
            return similar_conversations[:k]
            
        except Exception as e:
            st.error(f"âŒ é¡ä¼¼æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def get_context_for_query(self, query):
        """ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        similar_convs = self.search_similar_conversations(query)
        
        if not similar_convs:
            return ""
        
        # é¡ä¼¼ä¼šè©±ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context_parts = []
        for conv in similar_convs:
            context_parts.append(f"éå»ã®é¡ä¼¼è³ªå•: {conv['conversation']['user_input']}")
            context_parts.append(f"éå»ã®å›ç­”: {conv['conversation']['ai_response']}")
        
        return "\n".join(context_parts)

class ResourceMonitor:
    """ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.monitoring = True
        self.alerts = []
        
    def get_system_status(self):
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’å–å¾—"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            status = {
                "cpu_percent": cpu_percent,
                "memory_percent": memory.percent,
                "disk_free_gb": disk.free / (1024**3),
                "timestamp": datetime.now().isoformat(),
                "status": "healthy"
            }
            
            # ã—ãã„å€¤ã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã¯è­¦å‘Š
            if cpu_percent > Config.CPU_THRESHOLD:
                status["status"] = "warning"
                status["cpu_warning"] = f"CPUä½¿ç”¨ç‡ãŒé«˜ã„ã§ã™: {cpu_percent:.1f}%"
            
            if memory.percent > Config.MEMORY_THRESHOLD:
                status["status"] = "warning"
                status["memory_warning"] = f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„ã§ã™: {memory.percent:.1f}%"
            
            if disk.free / (1024**3) < Config.DISK_THRESHOLD:
                status["status"] = "warning"
                status["disk_warning"] = f"ç©ºãå®¹é‡ãŒå°‘ãªã„ã§ã™: {disk.free / (1024**3):.1f}GB"
            
            return status
            
        except Exception as e:
            return {"error": str(e), "status": "error"}
    
    def should_add_wait_message(self, response):
        """å¾…æ©Ÿãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã¹ãã‹åˆ¤å®š"""
        status = self.get_system_status()
        
        if status.get("status") == "warning":
            high_load_keywords = ["é‡ã„", "æ™‚é–“ãŒã‹ã‹ã‚‹", "å¾…ã£ã¦", "å‡¦ç†ä¸­"]
            return any(keyword in response for keyword in high_load_keywords)
        
        return False
    
    def get_wait_message(self):
        """å¾…æ©Ÿãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—"""
        return "å°‘ã€…ãŠå¾…ã¡ãã ã•ã„ã€‚ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ è² è·ãŒé«˜ã„ã§ã™ã€‚"

class ScheduledTaskManager:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¿ã‚¹ã‚¯ç®¡ç†"""
    
    def __init__(self):
        self.scheduler = schedule
        self.running = False
        self.task_results = {}
        
    def initialize(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼åˆæœŸåŒ–"""
        try:
            # å®šæœŸã‚¿ã‚¹ã‚¯ã‚’ç™»éŒ²
            for time_str, task_name in Config.SCHEDULED_TASKS:
                if task_name == "daily_system_check":
                    self.scheduler.every().day.at(time_str).do(self.daily_system_check)
                elif task_name == "daily_summary":
                    self.scheduler.every().day.at(time_str).do(self.daily_summary)
                elif task_name == "evening_cleanup":
                    self.scheduler.every().day.at(time_str).do(self.evening_cleanup)
                elif task_name == "night_backup":
                    self.scheduler.every().day.at(time_str).do(self.night_backup)
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’èµ·å‹•
            self._start_scheduler_thread()
            
            return True
        except Exception as e:
            st.error(f"âŒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _start_scheduler_thread(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰èµ·å‹•"""
        def run_scheduler():
            self.running = True
            while self.running:
                self.scheduler.run_pending()
                time.sleep(60)  # 1åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
        
        thread = threading.Thread(target=run_scheduler, daemon=True)
        thread.start()
    
    def daily_system_check(self):
        """æ—¥æ¬¡ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯"""
        try:
            monitor = ResourceMonitor()
            status = monitor.get_system_status()
            
            self.task_results["daily_system_check"] = {
                "timestamp": datetime.now().isoformat(),
                "status": status["status"],
                "details": status
            }
            
        except Exception as e:
            self.task_results["daily_system_check"] = {
                "timestamp": datetime.now().isoformat(),
                "error": str(e)
            }
    
    def daily_summary(self):
        """æ—¥æ¬¡ã‚µãƒãƒªãƒ¼"""
        try:
            # RAGã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰çµ±è¨ˆã‚’å–å¾—
            summary = f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ - {datetime.now().strftime('%Y-%m-%d')}"
            
            self.task_results["daily_summary"] = {
                "timestamp": datetime.now().isoformat(),
                "summary": summary
            }
            
        except Exception as e:
            self.task_results["daily_summary"] = {
                "timestamp": datetime.now().isoformat(),
                "error": str(e)
            }
    
    def evening_cleanup(self):
    """å¤•æ–¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            temp_dir = tempfile.gettempdir()
            cleaned_files = 0
            
            for file in os.listdir(temp_dir):
                if file.startswith("temp_"):
                    try:
                        os.remove(os.path.join(temp_dir, file))
                        cleaned_files += 1
                    except:
                        pass
            
            self.task_results["evening_cleanup"] = {
                "timestamp": datetime.now().isoformat(),
                "cleaned_files": cleaned_files
            }
            
        except Exception as e:
            self.task_results["evening_cleanup"] = {
                "timestamp": datetime.now().isoformat(),
                "error": str(e)
            }
    
    def night_backup(self):
        """å¤œé–“ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        try:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            backup_data = {
                "timestamp": datetime.now().isoformat(),
                "config": {
                    "main_model": Config.MAIN_MODEL,
                    "vision_model": Config.VISION_MODEL,
                    "rag_enabled": True,
                    "monitoring_enabled": True
                },
                "task_results": self.task_results
            }
            
            backup_file = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            backup_path = os.path.join("./backups", backup_file)
            
            os.makedirs("./backups", exist_ok=True)
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            
            self.task_results["night_backup"] = {
                "timestamp": datetime.now().isoformat(),
                "backup_file": backup_path
            }
            
        except Exception as e:
            self.task_results["night_backup"] = {
                "timestamp": datetime.now().isoformat(),
                "error": str(e)
            }
    
    def get_task_status(self):
        """ã‚¿ã‚¹ã‚¯çŠ¶æ³ã‚’å–å¾—"""
        return {
            "scheduler_running": self.running,
            "next_tasks": self.scheduler.next_run(),
            "task_results": self.task_results
        }

class VRMModel:
    """VRMãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.models_path = Config.VRM_MODELS_PATH
        self.available_models = []
        self.current_model = None
        self.current_expression = "neutral"
        self.current_animation = "idle"
        
        os.makedirs(self.models_path, exist_ok=True)
        self._create_sample_vrm()
        self.get_available_models()
        
    def _create_sample_vrm(self):
        """ã‚µãƒ³ãƒ—ãƒ«VRMãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        sample_vrm_path = os.path.join(self.models_path, "default_avatar.vrm")
        
        if not os.path.exists(sample_vrm_path):
            vrm_content = """# VRM Model File
model_version: "1.0"
model_name: "AI Assistant Avatar"
model_author: "AI System"

expressions:
  neutral: "é€šå¸¸"
  happy: "å–œã³"
  sad: "æ‚²ã—ã¿"
  angry: "æ€’ã‚Š"
  surprised: "é©šã"
  thinking: "æ€è€ƒä¸­"

animations:
  idle: "å¾…æ©Ÿ"
  talking: "è©±ã—ã¦ã„ã‚‹"
  thinking: "æ€è€ƒä¸­"
  waving: "æ‰‹ã‚’æŒ¯ã£ã¦ã„ã‚‹"
"""
            
            with open(sample_vrm_path, 'w', encoding='utf-8') as f:
                f.write(vrm_content)
    
    def get_available_models(self):
        """åˆ©ç”¨å¯èƒ½ãªVRMãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        models = []
        if os.path.exists(self.models_path):
            for file in os.listdir(self.models_path):
                if file.endswith('.vrm'):
                    models.append(file)
        
        self.available_models = models
        return models
    
    def set_expression(self, expression):
        """è¡¨æƒ…ã‚’è¨­å®š"""
        valid_expressions = ["neutral", "happy", "sad", "angry", "surprised", "thinking"]
        if expression in valid_expressions:
            self.current_expression = expression
            return True
        return False
    
    def set_animation(self, animation):
        """ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¨­å®š"""
        valid_animations = Config.VRM_ANIMATIONS
        if animation in valid_animations:
            self.current_animation = animation
            return True
        return False
    
    def get_vrm_info(self):
        """VRMæƒ…å ±ã‚’å–å¾—"""
        return {
            "available_models": len(self.available_models),
            "current_model": self.current_model,
            "current_expression": self.current_expression,
            "current_animation": self.current_animation,
            "models_path": self.models_path
        }

class VRMRenderer:
    """VRMãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
    
    def __init__(self):
        self.vrm_model = VRMModel()
        self.is_rendering = False
        
    def initialize(self):
        """VRMãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼åˆæœŸåŒ–"""
        self.vrm_model.get_available_models()
        return True
    
    def render_avatar(self, expression="neutral", animation="idle"):
        """ã‚¢ãƒã‚¿ãƒ¼ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        try:
            self.vrm_model.set_expression(expression)
            self.vrm_model.set_animation(animation)
            
            current_time = datetime.now().strftime('%H:%M:%S')
            
            expression_emoji = {
                "neutral": "ğŸ˜", "happy": "ğŸ˜Š", "sad": "ğŸ˜¢",
                "angry": "ğŸ˜ ", "surprised": "ğŸ˜²", "thinking": "ğŸ¤”"
            }
            
            emoji = expression_emoji.get(expression, "ğŸ˜")
            
            return {
                "status": "success",
                "timestamp": current_time,
                "expression": expression,
                "animation": animation,
                "emoji": emoji,
                "render_data": {
                    "avatar_state": "active",
                    "performance": "60 FPS",
                    "quality": "high"
                }
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    def get_avatar_display(self):
        """ã‚¢ãƒã‚¿ãƒ¼è¡¨ç¤ºã‚’å–å¾—"""
        expression_emoji = {
            "neutral": "ğŸ˜", "happy": "ğŸ˜Š", "sad": "ğŸ˜¢",
            "angry": "ğŸ˜ ", "surprised": "ğŸ˜²", "thinking": "ğŸ¤”"
        }
        
        emoji = expression_emoji.get(self.vrm_model.current_expression, "ğŸ˜")
        
        return f"""
        <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; margin: 10px 0;">
            <div style="font-size: 64px; margin-bottom: 10px;">{emoji}</div>
            <div style="color: white; font-weight: bold;">
                <div>Expression: {self.vrm_model.current_expression}</div>
                <div>Animation: {self.vrm_model.current_animation}</div>
                <div>Time: {datetime.now().strftime('%H:%M:%S')}</div>
            </div>
        </div>
        """

class FinalOptimizedAISystem:
    """æœ€é©åŒ–AIã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.ollama_client = None
        self.whisper_model = None
        self.tts_engine = None
        self.rag_system = RAGSystem()
        self.resource_monitor = ResourceMonitor()
        self.task_manager = ScheduledTaskManager()
        self.vrm_renderer = VRMRenderer()
        self.current_personality = "friend"
        
    def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # OllamaåˆæœŸåŒ–
            self.ollama_client = ollama.Client()
            
            # éŸ³å£°å‡¦ç†åˆæœŸåŒ–
            self.whisper_model = faster_whisper.WhisperModel("base")
            self.tts_engine = pyttsx3.init()
            self.tts_engine.setProperty('rate', str(Config.VOICE_RATE))
            self.tts_engine.setProperty('volume', str(Config.VOICE_VOLUME))
            
            # å„ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            self.rag_system.initialize()
            self.task_manager.initialize()
            self.vrm_renderer.initialize()
            
            return True
            
        except Exception as e:
            return False
    
    def generate_response(self, prompt, images=None, context="", fast_mode=False):
        """æœ€é©åŒ–ã•ã‚ŒãŸå¿œç­”ç”Ÿæˆ"""
        try:
            # RAGã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            rag_context = self.rag_system.get_context_for_query(prompt)
            full_context = f"{context}\n{rag_context}" if rag_context else context
            
            # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ãƒã‚§ãƒƒã‚¯
            if self.resource_monitor.should_add_wait_message(prompt):
                wait_message = self.resource_monitor.get_wait_message()
                self.vrm_renderer.render_avatar("thinking", "thinking")
                return f"{wait_message}\n\næ€è€ƒä¸­ã§ã™..."
            
            # VRMè¡¨æƒ…æ›´æ–°ï¼ˆæ€è€ƒä¸­ï¼‰
            self.vrm_renderer.render_avatar("thinking", "thinking")
            
            # Ollamaã§å¿œç­”ç”Ÿæˆ
            response = self.ollama_client.generate(
                model=Config.MAIN_MODEL,
                prompt=f"{full_context}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {prompt}",
                options={
                    "temperature": 0.7,
                    "max_tokens": Config.MAX_TOKENS_FAST if fast_mode else Config.MAX_TOKENS_FULL
                }
            )
            
            ai_response = response['response']
            
            # RAGã«ä¼šè©±ã‚’è¿½åŠ 
            self.rag_system.add_conversation(prompt, ai_response)
            
            # å¿œç­”ã«å¿œã˜ã¦è¡¨æƒ…ã‚’å¤‰æ›´
            if any(word in ai_response for word in ["ã‚ã‚ŠãŒã¨ã†", "å¬‰ã—ã„", "æ¥½ã—ã„", "æˆåŠŸ"]):
                self.vrm_renderer.render_avatar("happy", "talking")
            elif any(word in ai_response for word in ["ã™ã¿ã¾ã›ã‚“", "ã”ã‚ã‚“", "å¤±æ•—", "å•é¡Œ"]):
                self.vrm_renderer.render_avatar("sad", "talking")
            else:
                self.vrm_renderer.render_avatar("neutral", "talking")
            
            return ai_response
            
        except Exception as e:
            self.vrm_renderer.render_avatar("sad", "idle")
            return f"âŒ å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def text_to_speech(self, text):
        """éŸ³å£°åˆæˆ"""
        try:
            self.vrm_renderer.render_avatar("happy", "talking")
            self.tts_engine.say(text)
            self.tts_engine.runAndWait()
            self.vrm_renderer.render_avatar("neutral", "idle")
            return True
        except Exception as e:
            st.error(f"âŒ éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def get_system_status(self):
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’å–å¾—"""
        return {
            "resource_status": self.resource_monitor.get_system_status(),
            "task_status": self.task_manager.get_task_status(),
            "vrm_status": self.vrm_renderer.vrm_model.get_vrm_info(),
            "rag_status": {
                "conversations_count": len(self.rag_system.conversation_history),
                "last_updated": datetime.now().isoformat()
            }
        }

def render_dashboard(ai_system):
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤º"""
    st.header("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    
    status = ai_system.get_system_status()
    
    # ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ…‹
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ–¥ï¸ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–")
        resource_status = status["resource_status"]
        
        if "error" not in resource_status:
            st.metric("CPUä½¿ç”¨ç‡", f"{resource_status['cpu_percent']:.1f}%")
            st.metric("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡", f"{resource_status['memory_percent']:.1f}%")
            st.metric("ç©ºãå®¹é‡", f"{resource_status['disk_free_gb']:.1f}GB")
            
            # è­¦å‘Šè¡¨ç¤º
            if resource_status.get("status") == "warning":
                st.warning("âš ï¸ ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã«æ³¨æ„ãŒå¿…è¦ã§ã™")
                if "cpu_warning" in resource_status:
                    st.error(resource_status["cpu_warning"])
                if "memory_warning" in resource_status:
                    st.error(resource_status["memory_warning"])
                if "disk_warning" in resource_status:
                    st.error(resource_status["disk_warning"])
        else:
            st.success("âœ… ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã¯æ­£å¸¸ã§ã™")
    
    with col2:
        st.subheader("ğŸ¤– AIã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹")
        
        # RAGçŠ¶æ…‹
        rag_status = status["rag_status"]
        st.metric("ä¼šè©±å±¥æ­´", rag_status["conversations_count"])
        st.write(f"æœ€çµ‚æ›´æ–°: {rag_status['last_updated']}")
        
        # ã‚¿ã‚¹ã‚¯çŠ¶æ…‹
        task_status = status["task_status"]
        st.write(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼: {'å®Ÿè¡Œä¸­' if task_status['scheduler_running'] else 'åœæ­¢ä¸­'}")
        
        if task_status["task_results"]:
            st.write("**æœ€è¿‘ã®ã‚¿ã‚¹ã‚¯çµæœ**:")
            for task_name, result in task_status["task_results"].items():
                st.write(f"- {task_name}: {result.get('timestamp', 'N/A')}")

def render_main_interface(ai_system):
    """ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ’¬ æœ€é©åŒ–AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    
    # ä¼šè©±å±¥æ­´
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    for i, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"], avatar="ğŸ¤–" if message["role"] == "assistant" else "ğŸ‘¤"):
            st.markdown(message["content"])
    
    # VRMè¡¨ç¤ºã¨å…¥åŠ›
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # å…¥åŠ›ã‚¨ãƒªã‚¢
        user_input = st.text_input(
            "ğŸ’¬ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›",
            placeholder="æœ€é©åŒ–AIã¨ã®å¯¾è©±ã‚’é–‹å§‹...",
            key="user_input"
        )
        
        # ãƒœã‚¿ãƒ³ç¾¤
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            send_button = st.button("ğŸ’¬ é€ä¿¡", type="primary")
        
        with col2:
            fast_mode = st.checkbox("âš¡ é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰", help="çŸ­ã„å¿œç­”ã‚’å„ªå…ˆ")
        
        with col3:
            auto_speech = st.checkbox("ğŸ”Š éŸ³å£°èª­ã¿ä¸Šã’", value=True)
        
        # é€ä¿¡å‡¦ç†
        if send_button and user_input:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜
            st.session_state.messages.append({"role": "user", "content": user_input})
            
            # AIå¿œç­”ç”Ÿæˆ
            with st.spinner("ğŸ¤– æœ€é©åŒ–AIã§å¿œç­”ç”Ÿæˆä¸­..."):
                context = ""
                if len(st.session_state.messages) > 1:
                    recent_messages = st.session_state.messages[-3:]
                    context = "æœ€è¿‘ã®ä¼šè©±: " + " | ".join([msg["content"] for msg in recent_messages])
                
                ai_response = ai_system.generate_response(
                    user_input, 
                    context=context, 
                    fast_mode=fast_mode
                )
                st.session_state.messages.append({"role": "assistant", "content": ai_response})
            
            # è‡ªå‹•éŸ³å£°èª­ã¿ä¸Šã’
            if auto_speech:
                ai_system.text_to_speech(ai_response)
            
            st.rerun()
    
    with col2:
        # VRMè¡¨ç¤º
        st.subheader("ğŸ‘¤ VRMã‚¢ãƒã‚¿ãƒ¼")
        vrm_display = ai_system.vrm_renderer.get_avatar_display()
        st.markdown(vrm_display, unsafe_allow_html=True)

def render_settings(ai_system):
    """è¨­å®šç”»é¢"""
    st.header("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ è¨­å®š")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
    st.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«æƒ…å ±")
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«", Config.MAIN_MODEL)
        st.write("**ç”¨é€”**: é«˜é€Ÿãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ»é›‘è«‡")
        st.write("**ç‰¹å¾´**: 3bãƒ¢ãƒ‡ãƒ«ã§è»½é‡ãƒ»é«˜é€Ÿ")
    
    with col2:
        st.metric("ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«", Config.VISION_MODEL)
        st.write("**ç”¨é€”**: ç”»åƒèªè­˜ãƒ»ç”»é¢åˆ†æ")
        st.write("**ç‰¹å¾´**: 11bãƒ¢ãƒ‡ãƒ«ã§é«˜ç²¾åº¦")
    
    # RAGè¨­å®š
    st.subheader("ğŸ§  RAGè¨­å®š")
    col1, col2 = st.columns(2)
    
    with col1:
        st.write(f"**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹**: {Config.RAG_DB_PATH}")
        st.write(f"**ä¼šè©±å±¥æ­´**: {len(ai_system.rag_system.conversation_history)}ä»¶")
        st.write(f"**é¡ä¼¼åº¦é–¾å€¤**: {Config.SIMILARITY_THRESHOLD}")
    
    with col2:
        st.write("**æœ‰åŠ¹åŒ–ã•ã‚ŒãŸæ©Ÿèƒ½**:")
        st.write("- âœ… éå»ã®ä¼šè©±æ¤œç´¢")
        st.write("- âœ… ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ‹¡å¼µ")
        st.write("- âœ… é¡ä¼¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°")
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
    st.subheader("â° ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š")
    st.write("**å®šæœŸã‚¿ã‚¹ã‚¯**:")
    for time_str, task_name in Config.SCHEDULED_TASKS:
        st.write(f"- {time_str}: {task_name}")
    
    task_status = ai_system.get_system_status()["task_status"]
    if task_status["task_results"]:
        st.write("**å®Ÿè¡Œçµæœ**:")
        for task_name, result in task_status["task_results"].items():
            st.write(f"- {task_name}: {result.get('timestamp', 'N/A')}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    st.set_page_config(
        page_title="ğŸš€ Final Optimized AI System",
        page_icon="ğŸš€",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸš€ AI Agent System - Final Optimized Version")
    st.markdown("### ğŸ¯ llama3.2 + VRM + RAG + ç›£è¦– + ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    if 'ai_system' not in st.session_state:
        with st.spinner("ğŸš€ æœ€é©åŒ–AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­..."):
            ai_system = FinalOptimizedAISystem()
            if ai_system.initialize():
                st.session_state.ai_system = ai_system
                st.success("âœ… æœ€é©åŒ–AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            else:
                st.error("âŒ AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
                st.stop()
    
    ai_system = st.session_state.ai_system
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        render_settings(ai_system)
    
    # ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–
    tab1, tab2 = st.tabs(["ğŸ’¬ AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", "ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"])
    
    with tab1:
        render_main_interface(ai_system)
    
    with tab2:
        render_dashboard(ai_system)
    
    # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
    st.markdown("---")
    st.markdown(f"**ğŸš€ æœ€é©åŒ–AI**: {Config.MAIN_MODEL} + {Config.VISION_MODEL}")
    st.markdown(f"**æœ€çµ‚æ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    st.markdown("**ğŸ¯ ç›®æ¨™**: é€Ÿããƒ»æ­£ç¢ºã«ãƒ»ä½•ã§ã‚‚è¦‹ãˆã‚‹ãƒ»æ„Ÿæƒ…è¡¨ç¾ãƒ»éå»ã®å­¦ç¿’ãƒ»è‡ªå‹•ç®¡ç†")

if __name__ == "__main__":
    main()
