#!/usr/bin/env python3
"""
Vision AI Integration Test Script
"""

import ollama
import pyautogui
import tempfile
import os
from datetime import datetime

def test_vision_system():
    """ãƒ“ã‚¸ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ” Vision AI Integration Test")
    print("=" * 50)
    
    try:
        # Ollamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        client = ollama.Client()
        print("âœ… Ollama client initialized")
        
        # åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ç¢ºèª
        models = client.list()
        vision_models = [m['name'] for m in models if 'vision' in m['name'].lower()]
        
        print(f"ğŸ“‹ Available vision models: {vision_models}")
        
        if not vision_models:
            print("âŒ No vision models found")
            return False
        
        # ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ†ã‚¹ãƒˆ
        print("\nğŸ“¸ Testing screen capture...")
        screenshot = pyautogui.screenshot()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        temp_path = f"test_vision_{timestamp}.png"
        screenshot.save(temp_path)
        print(f"âœ… Screen captured: {temp_path}")
        
        # ãƒ“ã‚¸ãƒ§ãƒ³åˆ†æãƒ†ã‚¹ãƒˆ
        print("\nğŸ‘ï¸ Testing vision analysis...")
        response = client.generate(
            model="llama3.2-vision",
            prompt="ã“ã®ç”»é¢ã«ã¤ã„ã¦ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„",
            images=[temp_path]
        )
        
        print("âœ… Vision analysis completed")
        print(f"ğŸ“Š Analysis result: {response['response'][:200]}...")
        
        # OCRãƒ†ã‚¹ãƒˆ
        print("\nğŸ“ Testing OCR functionality...")
        ocr_response = client.generate(
            model="llama3.2-vision",
            prompt="ã“ã®ç”»åƒã‹ã‚‰ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„",
            images=[temp_path]
        )
        
        print("âœ… OCR completed")
        print(f"ğŸ“ OCR result: {ocr_response['response'][:200]}...")
        
        # UIè¦ç´ åˆ†æãƒ†ã‚¹ãƒˆ
        print("\nğŸ¨ Testing UI element analysis...")
        ui_response = client.generate(
            model="llama3.2-vision",
            prompt="ã“ã®ç”»é¢ã®UIè¦ç´ ï¼ˆãƒœã‚¿ãƒ³ã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã©ï¼‰ã‚’åˆ†æã—ã¦ãã ã•ã„",
            images=[temp_path]
        )
        
        print("âœ… UI analysis completed")
        print(f"ğŸ¨ UI analysis result: {ui_response['response'][:200]}...")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        try:
            os.unlink(temp_path)
            print(f"\nğŸ—‘ï¸ Temporary file deleted: {temp_path}")
        except:
            pass
        
        print("\n" + "=" * 50)
        print("ğŸ‰ Vision AI Integration Test Completed Successfully!")
        print("âœ… All vision features are working correctly")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Test failed: {str(e)}")
        return False

def test_model_availability():
    """ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯èƒ½æ€§ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ” Model Availability Test")
    print("-" * 30)
    
    try:
        client = ollama.Client()
        models = client.list()
        
        print("ğŸ“‹ Available models:")
        for model in models:
            name = model.get('name', 'Unknown')
            size = model.get('size', 'Unknown')
            modified = model.get('modified', 'Unknown')
            print(f"  - {name} ({size}) - {modified}")
        
        # llama3.2-visionã®ç¢ºèª
        vision_available = any('llama3.2-vision' in m.get('name', '') for m in models)
        
        if vision_available:
            print("\nâœ… llama3.2-vision model is available")
        else:
            print("\nâŒ llama3.2-vision model not found")
        
        return vision_available
        
    except Exception as e:
        print(f"âŒ Model availability test failed: {str(e)}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ Vision AI System Integration Test")
    print("=" * 60)
    
    # ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯èƒ½æ€§ç¢ºèª
    model_available = test_model_availability()
    
    if not model_available:
        print("\nâŒ Please ensure llama3.2-vision model is installed")
        print("Run: ollama pull llama3.2-vision")
        return
    
    # ãƒ“ã‚¸ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
    vision_test_passed = test_vision_system()
    
    if vision_test_passed:
        print("\nğŸ¯ Next Steps:")
        print("1. Run: streamlit run vision_enhanced_app.py")
        print("2. Open browser to http://localhost:8501")
        print("3. Test vision features in the web interface")
        print("4. Upload images or capture screen for analysis")
    else:
        print("\nâš ï¸ Some tests failed. Please check the error messages above.")

if __name__ == "__main__":
    main()
