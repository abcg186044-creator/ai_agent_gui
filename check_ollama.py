#!/usr/bin/env python3
"""
Ollamaæ¥ç¶šãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import requests
import subprocess
import sys
import time

def check_ollama_status():
    """Ollamaã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    print("ğŸ¤– Ollamaæ¥ç¶šãƒã‚§ãƒƒã‚¯ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # 1. Ollamaãƒ—ãƒ­ã‚»ã‚¹ã®ç¢ºèª
    print("\n1ï¸âƒ£ Ollamaãƒ—ãƒ­ã‚»ã‚¹ã®ç¢ºèª...")
    try:
        result = subprocess.run(['tasklist'], capture_output=True, text=True)
        if 'ollama' in result.stdout.lower():
            print("âœ… Ollamaãƒ—ãƒ­ã‚»ã‚¹ãŒå®Ÿè¡Œä¸­ã§ã™")
        else:
            print("âŒ Ollamaãƒ—ãƒ­ã‚»ã‚¹ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print("ğŸ’¡ Ollamaã‚’èµ·å‹•ã—ã¦ãã ã•ã„")
            return False
    except Exception as e:
        print(f"âŒ ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # 2. Ollama APIæ¥ç¶šãƒã‚§ãƒƒã‚¯
    print("\n2ï¸âƒ£ Ollama APIæ¥ç¶šãƒã‚§ãƒƒã‚¯...")
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… Ollama APIã«æ¥ç¶šã§ãã¾ã—ãŸ")
            
            # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤º
            data = response.json()
            models = data.get('models', [])
            if models:
                print(f"ğŸ“¦ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {len(models)}å€‹")
                for model in models:
                    print(f"  - {model['name']}")
            else:
                print("âš ï¸ ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                print("ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        else:
            print(f"âŒ Ollama APIæ¥ç¶šã‚¨ãƒ©ãƒ¼: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("âŒ Ollama APIã«æ¥ç¶šã§ãã¾ã›ã‚“")
        print("ğŸ’¡ OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        return False
    except Exception as e:
        print(f"âŒ APIãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # 3. llama3.2ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
    print("\n3ï¸âƒ£ llama3.2ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª...")
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        data = response.json()
        models = data.get('models', [])
        
        llama32_found = False
        for model in models:
            if 'llama3.2' in model['name']:
                print(f"âœ… llama3.2ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {model['name']}")
                llama32_found = True
        
        if not llama32_found:
            print("âŒ llama3.2ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print("ğŸ’¡ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„:")
            print("   ollama pull llama3.2")
            return False
            
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    print("\nğŸ‰ Ollamaã®ãƒã‚§ãƒƒã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    return True

def start_ollama():
    """Ollamaã‚’èµ·å‹•"""
    print("\nğŸš€ Ollamaã‚’èµ·å‹•ã—ã¾ã™...")
    try:
        subprocess.Popen(['ollama', 'serve'], shell=True)
        print("âœ… Ollamaã‚’èµ·å‹•ã—ã¾ã—ãŸ")
        print("â³ 5ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰å†ãƒã‚§ãƒƒã‚¯ã—ã¾ã™...")
        time.sleep(5)
        return True
    except FileNotFoundError:
        print("âŒ OllamaãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("ğŸ’¡ Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: https://ollama.com/download")
        return False
    except Exception as e:
        print(f"âŒ Ollamaèµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if not check_ollama_status():
        print("\nğŸ”„ Ollamaã®èµ·å‹•ã‚’è©¦ã¿ã¾ã™...")
        if start_ollama():
            check_ollama_status()
        else:
            print("\nâŒ Ollamaã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ")
            print("ğŸ’¡ æ‰‹å‹•ã§Ollamaã‚’èµ·å‹•ã—ã¦ãã ã•ã„")
            sys.exit(1)
    
    print("\nâœ… AI Agent Systemã‚’èµ·å‹•ã§ãã¾ã™ï¼")

if __name__ == "__main__":
    main()
