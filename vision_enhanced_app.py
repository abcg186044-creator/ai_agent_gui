#!/usr/bin/env python3
"""
AI Agent System - ãƒ“ã‚¸ãƒ§ãƒ³æ©Ÿèƒ½å¼·åŒ–ç‰ˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
llama3.2-visionã‚’çµ±åˆã—ãŸå®Œå…¨ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
"""

import streamlit as st
import sys
import os
import json
import tempfile
import time
from datetime import datetime
from pathlib import Path

# åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import ollama
    import faster_whisper
    import pyttsx3
    import pyautogui
    import numpy as np
    import pandas as pd
    from openpyxl import load_workbook
    import pymupdf
    from PIL import Image
    import qrcode
    from duckduckgo_search import DDGS
    import chromadb
    from sentence_transformers import SentenceTransformer
    import faiss
    import psutil
    import schedule
    import base64
except ImportError as e:
    st.error(f"âŒ å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    st.stop()

# è¨­å®š
class Config:
    VISION_MODEL = "llama3.2-vision"
    TEXT_MODEL = "llama3.2"  # åŸºæœ¬llama3.2ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    OLLAMA_HOST = "localhost"
    OLLAMA_PORT = 11434
    VOICE_RATE = 200
    VOICE_VOLUME = 0.9

class EnhancedAISystem:
    """å¼·åŒ–AIã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.ollama_client = None
        self.whisper_model = None
        self.tts_engine = None
        self.knowledge_db = None
        self.memory_data = None
        self.current_personality = "friend"
        
    def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # OllamaåˆæœŸåŒ–
            self.ollama_client = ollama.Client()
            
            # éŸ³å£°å‡¦ç†åˆæœŸåŒ–
            self.whisper_model = faster_whisper.WhisperModel("base")
            self.tts_engine = pyttsx3.init()
            self.tts_engine.setProperty('rate', str(Config.VOICE_RATE))
            self.tts_engine.setProperty('volume', str(Config.VOICE_VOLUME))
            
            # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
            self.knowledge_db = chromadb.PersistentClient(path="./knowledge_base")
            
            # ãƒ¡ãƒ¢ãƒªDBåˆæœŸåŒ–
            self.memory_data = self._load_memory_db()
            
            return True
        except Exception as e:
            st.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _load_memory_db(self):
        """ãƒ¡ãƒ¢ãƒªDBèª­ã¿è¾¼ã¿"""
        try:
            if os.path.exists("./memory_db.json"):
                with open("./memory_db.json", 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                return {
                    "conversations": [],
                    "user_profile": {
                        "name": None,
                        "os": None,
                        "tech_stack": [],
                        "preferences": [],
                        "projects": [],
                        "last_updated": None
                    },
                    "learning_data": {
                        "common_questions": [],
                        "preferred_responses": [],
                        "technical_level": "beginner"
                    }
                }
        except Exception:
            return {"conversations": [], "user_profile": {}, "learning_data": {}}
    
    def capture_and_analyze_screen(self, prompt="ã“ã®ç”»é¢ã«ã¤ã„ã¦è©³ç´°ã«åˆ†æã—ã¦ãã ã•ã„"):
        """ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£ã¨ãƒ“ã‚¸ãƒ§ãƒ³åˆ†æ"""
        try:
            with st.spinner("ğŸ“¸ ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£ä¸­..."):
                screenshot = pyautogui.screenshot()
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                temp_path = f"temp_vision_{timestamp}.png"
                screenshot.save(temp_path)
            
            with st.spinner("ğŸ‘ï¸ ãƒ“ã‚¸ãƒ§ãƒ³AIåˆ†æä¸­..."):
                response = self.ollama_client.generate(
                    model=Config.VISION_MODEL,
                    prompt=prompt,
                    images=[temp_path]
                )
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            try:
                os.unlink(temp_path)
            except:
                pass
            
            return response['response']
            
        except Exception as e:
            return f"âŒ ç”»é¢åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def analyze_uploaded_image(self, image_file, prompt):
        """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒã®åˆ†æ"""
        try:
            with st.spinner("ğŸ‘ï¸ ç”»åƒåˆ†æä¸­..."):
                response = self.ollama_client.generate(
                    model=Config.VISION_MODEL,
                    prompt=prompt,
                    images=[image_file]
                )
            return response['response']
        except Exception as e:
            return f"âŒ ç”»åƒåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def extract_text_from_screen(self):
        """ç”»é¢ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆOCRï¼‰"""
        try:
            ocr_prompt = """ã“ã®ç”»åƒã‹ã‚‰ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
            èª­ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£ç¢ºã«ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä¿ã£ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
            ãƒœã‚¿ãƒ³ã€ãƒ©ãƒ™ãƒ«ã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼é …ç›®ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã©ã€ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã¦ãã ã•ã„ã€‚"""
            
            return self.capture_and_analyze_screen(ocr_prompt)
        except Exception as e:
            return f"âŒ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def hybrid_analysis(self, text_prompt, image_path=None):
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒï¼‰"""
        try:
            if image_path is None:
                with st.spinner("ğŸ“¸ ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£ä¸­..."):
                    screenshot = pyautogui.screenshot()
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    image_path = f"temp_hybrid_{timestamp}.png"
                    screenshot.save(image_path)
            
            hybrid_prompt = f"ä»¥ä¸‹ã®ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’çµ±åˆã—ã¦å›ç­”ã—ã¦ãã ã•ã„:\n\nãƒ†ã‚­ã‚¹ãƒˆ: {text_prompt}\n\nç”»åƒ:"
            
            with st.spinner("ğŸ§  ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰AIåˆ†æä¸­..."):
                response = self.ollama_client.generate(
                    model=Config.VISION_MODEL,
                    prompt=hybrid_prompt,
                    images=[image_path]
                )
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            try:
                if image_path and "temp_" in image_path:
                    os.unlink(image_path)
            except:
                pass
            
            return response['response']
            
        except Exception as e:
            return f"âŒ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def generate_text_response(self, prompt, context=""):
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®å¿œç­”ç”Ÿæˆ"""
        try:
            personality_prompts = {
                "friend": f"ã‚ãªãŸã¯è¦ªå‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ã—ã¦ã€ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã«{context}ã‚’è€ƒæ…®ã—ãªãŒã‚‰ç­”ãˆã¦ãã ã•ã„ã€‚{prompt}",
                "copy": f"ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†èº«ã¨ã—ã¦ã€{context}ã‚’èƒŒæ™¯ã«{prompt}ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
                "expert": f"ã‚ãªãŸã¯å°‚é–€å®¶ã¨ã—ã¦ã€æä¾›ã•ã‚ŒãŸè³‡æ–™ã«åŸºã¥ã{context}ã‚’è€ƒæ…®ã—ãªãŒã‚‰æ­£ç¢ºãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚{prompt}"
            }
            
            adjusted_prompt = personality_prompts.get(self.current_personality, prompt)
            
            response = self.ollama_client.generate(
                model=Config.TEXT_MODEL,
                prompt=adjusted_prompt,
                options={
                    "temperature": 0.7,
                    "max_tokens": 4096
                }
            )
            
            return response['response']
        except Exception as e:
            return f"âŒ å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def text_to_speech(self, text):
        """éŸ³å£°åˆæˆ"""
        try:
            self.tts_engine.say(text)
            self.tts_engine.runAndWait()
            return True
        except Exception as e:
            st.error(f"âŒ éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

def render_main_interface():
    """ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ’¬ AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    
    # ä¼šè©±å±¥æ­´è¡¨ç¤º
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    for i, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"], avatar="ğŸ¤–" if message["role"] == "assistant" else "ğŸ‘¤"):
            st.markdown(message["content"])
    
    # å…¥åŠ›ã‚¨ãƒªã‚¢
    col1, col2, col3 = st.columns([3, 1, 1])
    
    with col1:
        user_input = st.text_input(
            "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›",
            placeholder="AIã¨ã®å¯¾è©±ã‚’é–‹å§‹...",
            key="user_input"
        )
    
    with col2:
        if st.button("ğŸ“¸ ç”»é¢åˆ†æ", help="ç¾åœ¨ã®ç”»é¢ã‚’AIã§åˆ†æ"):
            with st.spinner("ç”»é¢åˆ†æä¸­..."):
                result = ai_system.capture_and_analyze_screen()
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": f"ğŸ“¸ **ç”»é¢åˆ†æçµæœ**:\n\n{result}"
                })
                st.rerun()
    
    with col3:
        if st.button("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º", help="ç”»é¢ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"):
            with st.spinner("ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸­..."):
                result = ai_system.extract_text_from_screen()
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": f"ğŸ“ **ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºçµæœ**:\n\n{result}"
                })
                st.rerun()
    
    # é€ä¿¡ãƒœã‚¿ãƒ³
    if st.button("ğŸ’¬ é€ä¿¡", type="primary"):
        if user_input:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜
            st.session_state.messages.append({"role": "user", "content": user_input})
            
            # AIå¿œç­”ç”Ÿæˆ
            with st.spinner("AIå¿œç­”ç”Ÿæˆä¸­..."):
                context = ""
                if len(st.session_state.messages) > 1:
                    recent_messages = st.session_state.messages[-3:]
                    context = "æœ€è¿‘ã®ä¼šè©±: " + " | ".join([msg["content"] for msg in recent_messages])
                
                ai_response = ai_system.generate_text_response(user_input, context)
                st.session_state.messages.append({"role": "assistant", "content": ai_response})
            
            # è‡ªå‹•éŸ³å£°èª­ã¿ä¸Šã’
            if st.checkbox("ğŸ”Š éŸ³å£°èª­ã¿ä¸Šã’", value=True):
                ai_system.text_to_speech(ai_response)
            
            st.rerun()

def render_vision_interface():
    """ãƒ“ã‚¸ãƒ§ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ‘ï¸ ãƒ“ã‚¸ãƒ§ãƒ³AI")
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "ğŸ“ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
        type=['png', 'jpg', 'jpeg', 'bmp', 'gif'],
        key="vision_image_file"
    )
    
    if uploaded_file:
        # ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        image = Image.open(uploaded_file)
        st.image(image, caption="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ", use_column_width=True)
        
        # åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        analysis_type = st.selectbox(
            "ğŸ” åˆ†æã‚¿ã‚¤ãƒ—",
            ["è©³ç´°èª¬æ˜", "ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º", "UIè¦ç´ åˆ†æ", "ã‚¨ãƒ©ãƒ¼æ¤œå‡º", "æ“ä½œæ‰‹é †èª¬æ˜"],
            key="analysis_type"
        )
        
        prompts = {
            "è©³ç´°èª¬æ˜": "ã“ã®ç”»åƒã«ã¤ã„ã¦è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„",
            "ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º": "ã“ã®ç”»åƒã‹ã‚‰ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„",
            "UIè¦ç´ åˆ†æ": "ã“ã®ç”»é¢ã®UIè¦ç´ ï¼ˆãƒœã‚¿ãƒ³ã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã©ï¼‰ã‚’åˆ†æã—ã¦ãã ã•ã„",
            "ã‚¨ãƒ©ãƒ¼æ¤œå‡º": "ã“ã®ç”»åƒã«ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚„è­¦å‘Šã€å•é¡Œç‚¹ãŒãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„",
            "æ“ä½œæ‰‹é †èª¬æ˜": "ã“ã®ç”»é¢ã®æ“ä½œæ–¹æ³•ã‚’ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§èª¬æ˜ã—ã¦ãã ã•ã„"
        }
        
        custom_prompt = st.text_area(
            "ğŸ” ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
            placeholder="ä¸Šè¨˜ã®åˆ†æã‚¿ã‚¤ãƒ—ä»¥å¤–ã®ç‹¬è‡ªã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›",
            height=100,
            key="custom_prompt"
        )
        
        # åˆ†æå®Ÿè¡Œ
        final_prompt = custom_prompt if custom_prompt else prompts[analysis_type]
        
        if st.button("ğŸ‘ï¸ ç”»åƒã‚’åˆ†æ", type="primary"):
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            
            result = ai_system.analyze_uploaded_image(tmp_file_path, final_prompt)
            
            st.subheader("ğŸ“Š ç”»åƒåˆ†æçµæœ")
            st.write(result)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            try:
                os.unlink(tmp_file_path)
            except:
                pass

def render_hybrid_interface():
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ§  ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“¸ ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£")
        if st.button("ğŸ“¸ ç”»é¢ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£", key="capture_hybrid"):
            with st.spinner("ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£ä¸­..."):
                screenshot = pyautogui.screenshot()
                st.session_state.hybrid_image = screenshot
                st.success("âœ… ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£å®Œäº†")
                st.image(screenshot, caption="ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ãŸç”»é¢", use_column_width=True)
    
    with col2:
        st.subheader("ğŸ’¬ åˆ†æãƒ†ã‚­ã‚¹ãƒˆ")
        hybrid_prompt = st.text_area(
            "ğŸ’¬ ç”»é¢ã«ã¤ã„ã¦ã®è³ªå•ã‚„æŒ‡ç¤º",
            placeholder="ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ãŸç”»é¢ã«ã¤ã„ã¦ã©ã®ã‚ˆã†ãªåˆ†æã‚’ã—ã¾ã™ã‹ï¼Ÿ",
            height=150,
            key="hybrid_prompt"
        )
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æå®Ÿè¡Œ
    if st.button("ğŸ§  ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æ", type="primary", key="hybrid_analysis"):
        if 'hybrid_image' in st.session_state and hybrid_prompt:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as tmp_file:
                screenshot = st.session_state.hybrid_image
                screenshot.save(tmp_file.name)
                tmp_file_path = tmp_file.name
            
            result = ai_system.hybrid_analysis(hybrid_prompt, tmp_file_path)
            
            st.subheader("ğŸ§  ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æçµæœ")
            st.write(result)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            try:
                os.unlink(tmp_file_path)
            except:
                pass
        else:
            st.warning("âš ï¸ ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£ã¨ãƒ†ã‚­ã‚¹ãƒˆã®ä¸¡æ–¹ãŒå¿…è¦ã§ã™")

def render_settings():
    """è¨­å®šç”»é¢"""
    st.header("âš™ï¸ è¨­å®š")
    
    # äººæ ¼é¸æŠ
    personalities = {
        "friend": "ğŸ‘¥ è¦ªå‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
        "copy": "ğŸª åˆ†èº«", 
        "expert": "ğŸ§‘â€ğŸ« ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ"
    }
    
    selected_personality = st.selectbox(
        "äººæ ¼é¸æŠ",
        list(personalities.keys()),
        format_func=lambda x: personalities[x],
        index=list(personalities.keys()).index(ai_system.current_personality)
    )
    
    if selected_personality != ai_system.current_personality:
        ai_system.current_personality = selected_personality
        st.success(f"äººæ ¼ã‚’ã€Œ{personalities[selected_personality]}ã€ã«å¤‰æ›´ã—ã¾ã—ãŸ")
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    st.subheader("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
    
    # CPUã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    cpu_percent = psutil.cpu_percent()
    memory = psutil.virtual_memory()
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("CPUä½¿ç”¨ç‡", f"{cpu_percent}%")
    with col2:
        st.metric("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡", f"{memory.percent}%")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
    st.write(f"**ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«**: {Config.VISION_MODEL}")
    st.write(f"**ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«**: {Config.TEXT_MODEL}")
    
    try:
        models = ai_system.ollama_client.list()
        model_names = [model.get('name', 'Unknown') for model in models]
        st.write(f"**åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«**: {', '.join(model_names)}")
    except:
        st.write("âŒ ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    st.set_page_config(
        page_title="ğŸ¤– AI Agent Vision Enhanced",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ¤– AI Agent System - Vision Enhanced")
    st.markdown("### ğŸš€ llama3.2-vision + ç”»é¢èªè­˜ã®å®Œå…¨çµ±åˆ")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    if 'ai_system' not in st.session_state:
        st.session_state.ai_system = EnhancedAISystem()
        if st.session_state.ai_system.initialize():
            st.success("âœ… å¼·åŒ–AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        else:
            st.error("âŒ å¼·åŒ–AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
            st.stop()
    
    global ai_system
    ai_system = st.session_state.ai_system
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        render_settings()
    
    # ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–
    tab1, tab2, tab3 = st.tabs(["ğŸ’¬ AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", "ğŸ‘ï¸ ãƒ“ã‚¸ãƒ§ãƒ³AI", "ğŸ§  ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æ"])
    
    with tab1:
        render_main_interface()
    
    with tab2:
        render_vision_interface()
    
    with tab3:
        render_hybrid_interface()
    
    # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
    st.markdown("---")
    st.markdown(f"**æœ€çµ‚æ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    st.markdown("**ğŸš€ llama3.2-visionã§é«˜åº¦ãªè¦–è¦šçš„AIå¯¾è©±ã‚’å®Ÿç¾**")

if __name__ == "__main__":
    main()
