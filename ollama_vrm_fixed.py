import streamlit as st
import numpy as np
import tempfile
import json
import requests
import time
import asyncio
import subprocess
import os
from datetime import datetime
from browser_audio_component_fixed import audio_recorder_component

# Ollamaé€£æºï¼ˆDockerå†…å®Œçµå‹ï¼‰
OLLAMA_HOST = "localhost"
OLLAMA_PORT = "11434"
OLLAMA_MODEL = "llama3.1:8b"

class OllamaIntegration:
    def __init__(self):
        self.base_url = f"http://{OLLAMA_HOST}:{OLLAMA_PORT}"
        self.model = OLLAMA_MODEL
    
    def check_connection(self):
        """Ollamaæ¥ç¶šç¢ºèª"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def generate_response(self, prompt, personality="friend"):
        """äººæ ¼ã«å¿œã˜ãŸAIå¿œç­”ç”Ÿæˆ"""
        # äººæ ¼åˆ¥ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        personality_prompts = {
            "friend": "ã‚ãªãŸã¯è¦ªåˆ‡ã§ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦ªå‹ã¨ã—ã¦ã€åˆ†ã‹ã‚Šã‚„ã™ãæ¥½ã—ãä¼šè©±ã—ã¦ãã ã•ã„ã€‚",
            "copy": "ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†èº«ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨åŒã˜è¦–ç‚¹ã§ã€å…±æ„Ÿã—ãªãŒã‚‰å¿œç­”ã—ã¦ãã ã•ã„ã€‚",
            "expert": "ã‚ãªãŸã¯å°‚é–€å®¶ã§ã™ã€‚æ­£ç¢ºã§è©³ç´°ãªæƒ…å ±ã‚’æä¾›ã—ã€çš„ç¢ºãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã ã•ã„ã€‚"
        }
        
        system_prompt = personality_prompts.get(personality, personality_prompts["friend"])
        full_prompt = f"{system_prompt}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {prompt}\nå¿œç­”: "
        
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "max_tokens": 1000
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                return f"APIã‚¨ãƒ©ãƒ¼: {response.status_code}"
                
        except Exception as e:
            return f"Ollamaã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def pull_model(self):
        """ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆDockerãƒ“ãƒ«ãƒ‰æ™‚ã«å®Ÿè¡Œæ¸ˆã¿ã®ãŸã‚ä¸è¦ï¼‰"""
        return True
    
    def check_models_loaded(self):
        """ãƒ¢ãƒ‡ãƒ«ãŒVRAMã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [model["name"] for model in models]
                return {
                    "llama3.1:8b": "llama3.1:8b" in model_names,
                    "llama3.2:latest": "llama3.2:latest" in model_names,
                    "llama3.2-vision:latest": "llama3.2-vision:latest" in model_names,
                    "total_models": len(models)
                }
            return {}
        except:
            return {}

class VRMAvatarController:
    def __init__(self):
        self.current_personality = "friend"
        self.expressions = {
            "friend": "happy",
            "copy": "joy", 
            "expert": "neutral"
        }
        # VRMãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèª
        self.vrm_path = self._find_vrm_file()
    
    def _find_vrm_file(self):
        """VRMãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        import os
        from pathlib import Path
        
        # æ¤œç´¢ãƒ‘ã‚¹ã®å„ªå…ˆé †ä½
        search_paths = [
            Path(r"C:\Users\GALLE\Desktop\EzoMomonga_Free") / "avatar.vrm",  # 1. æŒ‡å®šãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ï¼ˆå„ªå…ˆï¼‰
            Path(__file__).parent / "static" / "avatar.vrm",           # 2. ã‚¢ãƒ—ãƒªç”¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            Path(r"C:\Users\GALLE\Desktop\EzoMomonga_Free") / "EzoMomonga_Free.vrm",  # 3. å…ƒãƒ•ã‚¡ã‚¤ãƒ«å
            Path(r"C:\Users\GALLE\Desktop\EzoMomonga_Free") / "VRM_Sample_Basic.glb",  # 4. glbãƒ•ã‚¡ã‚¤ãƒ«
            Path(__file__).parent / "static" / "EzoMomonga_Free.vrm",  # 5. ã‚¢ãƒ—ãƒªç”¨ã‚³ãƒ”ãƒ¼
            Path(__file__).parent / "static" / "VRM_Sample_Basic.glb",  # 6. ã‚¢ãƒ—ãƒªç”¨glb
        ]
        
        for vrm_path in search_paths:
            if vrm_path.exists():
                print(f"âœ… VRMãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã¾ã—ãŸ: {vrm_path}")
                # staticãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ /static/ ãƒ‘ã‚¹ã‚’è¿”ã™
                if "static" in str(vrm_path):
                    return f"/static/{vrm_path.name}"
                else:
                    # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯staticã«ã‚³ãƒ”ãƒ¼ã—ã¦å‚ç…§
                    static_file = Path(__file__).parent / "static" / vrm_path.name
                    try:
                        import shutil
                        shutil.copy2(vrm_path, static_file)
                        print(f"ğŸ“ VRMãƒ•ã‚¡ã‚¤ãƒ«ã‚’staticã«ã‚³ãƒ”ãƒ¼: {static_file}")
                        return f"/static/{vrm_path.name}"
                    except Exception as e:
                        print(f"âŒ VRMãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ”ãƒ¼ã«å¤±æ•—: {str(e)}")
                        continue
        
        print("âš ï¸ VRMãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return "/static/avatar.vrm"
    
    def update_personality(self, personality):
        """äººæ ¼åˆ‡ã‚Šæ›¿ãˆ"""
        self.current_personality = personality
        return self.expressions.get(personality, "neutral")
    
    def set_personality(self, personality):
        """äººæ ¼ã‚’è¨­å®šï¼ˆupdate_personalityã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰"""
        return self.update_personality(personality)
    
    def get_vrm_html(self):
        """VRMè¡¨ç¤ºç”¨HTML"""
        return f"""
        <div id="vrm-container" style="width: 100%; height: 400px; border: 1px solid #ddd; border-radius: 8px; overflow: hidden;">
            <canvas id="vrm-canvas"></canvas>
        </div>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@1.0.0/lib/three-vrm.min.js"></script>
        
        <script>
        let scene, camera, renderer, vrmModel;
        let currentExpression = "{self.expressions[self.current_personality]}";
        
        async function initVRM() {{
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(45, 1, 0.1, 1000);
            camera.position.set(0, 1.2, 3);
            
            renderer = new THREE.WebGLRenderer({{ canvas: document.getElementById('vrm-canvas'), antialias: true }});
            renderer.setSize(400, 400);
            renderer.setClearColor(0xf0f0f0);
            
            // ç…§æ˜
            const light = new THREE.DirectionalLight(0xffffff, 1);
            light.position.set(1, 1, 1);
            scene.add(light);
            
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.4);
            scene.add(ambientLight);
            
            // VRMãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            try {{
                const loader = new THREE.VRMLoader();
                const gltf = await loader.loadAsync('{self.vrm_path}');
                vrmModel = await THREE.VRMUtils.importVRM(gltf);
                scene.add(vrmModel);
                
                // è¡¨æƒ…è¨­å®š
                if (vrmModel.blendShapeProxy) {{
                    vrmModel.blendShapeProxy.setValue(currentExpression, 1.0);
                }}
                
                animate();
            }} catch (error) {{
                console.error('VRMèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼:', error);
                // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡å˜ãª3Dã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¡¨ç¤º
                const geometry = new THREE.BoxGeometry(1, 2, 1);
                const material = new THREE.MeshBasicMaterial({{ color: 0x4CAF50 }});
                const cube = new THREE.Mesh(geometry, material);
                scene.add(cube);
                animate();
            }}
        }}
        
        function animate() {{
            requestAnimationFrame(animate);
            
            if (vrmModel && vrmModel.update) {{
                vrmModel.update(clock.getDelta());
            }}
            
            renderer.render(scene, camera);
        }}
        
        function updateExpression(expression) {{
            if (vrmModel && vrmModel.blendShapeProxy) {{
                // ã™ã¹ã¦ã®è¡¨æƒ…ã‚’ãƒªã‚»ãƒƒãƒˆ
                vrmModel.blendShapeProxy.clear();
                // æ–°ã—ã„è¡¨æƒ…ã‚’è¨­å®š
                vrmModel.blendShapeProxy.setValue(expression, 1.0);
            }}
            currentExpression = expression;
        }}
        
        // åˆæœŸåŒ–
        initVRM();
        
        // è¡¨æƒ…æ›´æ–°é–¢æ•°ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å…¬é–‹
        window.updateVRMExpression = updateExpression;
        </script>
        """

def speech_to_text(audio_data, sample_rate):
    """éŸ³å£°èªè­˜"""
    try:
        from faster_whisper import WhisperModel
        model = WhisperModel("base", compute_type="float32")
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            import wave
            with wave.open(temp_file.name, 'wb') as wav_file:
                wav_file.setnchannels(1)
                wav_file.setsampwidth(2)
                wav_file.setframerate(sample_rate)
                wav_file.writeframes((audio_data * 32767).astype(np.int16).tobytes())
            
            segments, info = model.transcribe(temp_file.name, language="ja")
            transcription = ""
            for segment in segments:
                transcription += segment.text + " "
            
            return transcription.strip()
            
    except Exception as e:
        return f"éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {str(e)}"

def text_to_speech(text, voice_character="female"):
    """éŸ³å£°åˆæˆ"""
    try:
        import pyttsx3
        engine = pyttsx3.init()
        
        # éŸ³å£°ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š
        voices = engine.getProperty('voices')
        if voice_character == "female" and len(voices) > 1:
            engine.setProperty('voice', voices[1].id)
        elif voice_character == "male" and len(voices) > 0:
            engine.setProperty('voice', voices[0].id)
        
        engine.setProperty('rate', 200)
        engine.setProperty('volume', 0.9)
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        filename = f"tts_output_{int(time.time())}.mp3"
        engine.save_to_file(text, filename)
        engine.runAndWait()
        
        return filename
        
    except Exception as e:
        return f"éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {str(e)}"

def save_conversation(conversation_history, personality):
    """ä¼šè©±å±¥æ­´ä¿å­˜"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"conversation_{personality}_{timestamp}.json"
    
    data = {
        "timestamp": timestamp,
        "personality": personality,
        "conversation": conversation_history
    }
    
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filename

def main():
    # ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œè¨­å®š
    st.set_page_config(
        page_title="AI Agent System",
        page_icon="ğŸ¤–", 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œCSS
    st.markdown("""
    <style>
    /* ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œã‚¹ã‚¿ã‚¤ãƒ« */
    @media (max-width: 768px) {
        .stSelectbox > div > div > select {
            font-size: 16px !important;
        }
        .stButton > button {
            font-size: 16px !important;
            padding: 12px 24px !important;
        }
        .stTextInput > div > input {
            font-size: 16px !important;
        }
        .stTextArea > div > textarea {
            font-size: 16px !important;
        }
        .element-container {
            padding: 0.5rem !important;
        }
        .main .block-container {
            padding-top: 1rem !important;
            padding-bottom: 1rem !important;
        }
    }
    
    /* ã‚¿ãƒƒãƒå¯¾å¿œ */
    .stButton > button {
        touch-action: manipulation;
        -webkit-tap-highlight-color: transparent;
    }
    
    /* VRMã‚³ãƒ³ãƒ†ãƒŠã®ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œ */
    #vrm-container {
        max-width: 100%;
        height: auto;
        aspect-ratio: 1/1;
    }
    
    #vrm-canvas {
        width: 100% !important;
        height: 100% !important;
    }
    
    /* éŸ³å£°ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã®ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œ */
    .audio-controls {
        display: flex;
        flex-direction: column;
        gap: 10px;
        padding: 10px;
    }
    
    .audio-controls button {
        width: 100%;
        margin: 5px 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.title("ğŸ¤– Ollama + VRM + éŸ³å£°èªè­˜ AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
    
    # ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±è¡¨ç¤º
    client_ip = st.experimental_get_query_params().get("client_ip", ["Unknown"])[0]
    user_agent = st.experimental_get_query_params().get("user_agent", ["Unknown"])[0]
    
    # ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹æ¤œå‡º
    is_mobile = "Mobile" in user_agent or "Android" in user_agent or "iPhone" in user_agent
    
    if is_mobile:
        st.info("ğŸ“± ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã„ã¾ã™ã€‚ã‚¿ãƒƒãƒæ“ä½œã«æœ€é©åŒ–ã•ã‚ŒãŸè¡¨ç¤ºã§ã™ã€‚")
    else:
        st.info("ğŸ–¥ï¸ ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã„ã¾ã™ã€‚")
    
    # Tailscaleã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±
    if "tailscale" in client_ip.lower() or "100." in client_ip.split('.')[0]:
        st.success("ğŸŒ TailscaleçµŒç”±ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã„ã¾ã™ã€‚å®‰å…¨ãªãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆæ¥ç¶šãŒç¢ºç«‹ã•ã‚Œã¦ã„ã¾ã™ã€‚")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹åˆæœŸåŒ–
    if "conversation_history" not in st.session_state:
        st.session_state.conversation_history = []
    if "current_personality" not in st.session_state:
        st.session_state.current_personality = "friend"
    if "ollama" not in st.session_state:
        st.session_state.ollama = OllamaIntegration()
    if "vrm_controller" not in st.session_state:
        st.session_state.vrm_controller = VRMAvatarController()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šè¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # äººæ ¼åˆ‡ã‚Šæ›¿ãˆ
        st.subheader("ğŸ­ äººæ ¼åˆ‡ã‚Šæ›¿ãˆ")
        personalities = {
            "friend": {"name": "è¦ªå‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢", "color": "#4CAF50", "icon": "ğŸ˜Š"},
            "copy": {"name": "åˆ†èº«", "color": "#2196F3", "icon": "ğŸª"},
            "expert": {"name": "ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ", "color": "#9C27B0", "icon": "ğŸ“"}
        }
        
        for key, info in personalities.items():
            if st.button(f"{info['icon']} {info['name']}", key=f"personality_{key}"):
                st.session_state.current_personality = key
                expression = st.session_state.vrm_controller.update_personality(key)
                # VRMè¡¨æƒ…æ›´æ–°ï¼ˆJavaScriptå‘¼ã³å‡ºã—ï¼‰
                st.components.v1.html(f"""
                <script>
                if (window.updateVRMExpression) {{
                    window.updateVRMExpression('{expression}');
                }}
                </script>
                """, height=0)
                st.rerun()
        
        # ç¾åœ¨ã®äººæ ¼è¡¨ç¤º
        current_info = personalities[st.session_state.current_personality]
        st.markdown(f"**ç¾åœ¨ã®äººæ ¼:** {current_info['icon']} {current_info['name']}")
        
        # Ollamaè¨­å®š
        st.subheader("ğŸ¤– Ollamaè¨­å®š")
        if st.button("ğŸ” Ollamaæ¥ç¶šç¢ºèª"):
            if st.session_state.ollama.check_connection():
                st.success("âœ… Ollamaã«æ¥ç¶šã•ã‚Œã¦ã„ã¾ã™")
                
                # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ç¢ºèª
                models_status = st.session_state.ollama.check_models_loaded()
                if models_status:
                    st.subheader("ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰çŠ¶æ³")
                    for model_name, is_loaded in models_status.items():
                        if model_name != "total_models":
                            status = "âœ… ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿" if is_loaded else "âŒ æœªãƒ­ãƒ¼ãƒ‰"
                            st.write(f"{model_name}: {status}")
                    st.write(f"ç·ãƒ¢ãƒ‡ãƒ«æ•°: {models_status.get('total_models', 0)}")
                else:
                    st.warning("âš ï¸ ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“")
            else:
                st.error("âŒ Ollamaã«æ¥ç¶šã§ãã¾ã›ã‚“")
                st.info("ã‚³ãƒ³ãƒ†ãƒŠå†…ã®Ollamaã‚µãƒ¼ãƒ“ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
        st.subheader("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        if st.button("ğŸ” ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª"):
            try:
                # GPUæƒ…å ±ï¼ˆDockerå†…ã‹ã‚‰å–å¾—ï¼‰
                try:
                    gpu_info = subprocess.run(["nvidia-smi", "--query-gpu=memory.used,memory.total", "--format=csv,noheader,nounits"], 
                                            capture_output=True, text=True, timeout=10)
                    if gpu_info.returncode == 0:
                        st.success("âœ… GPUæƒ…å ±å–å¾—")
                        st.code(gpu_info.stdout)
                        gpu_mode = "GPU"
                    else:
                        st.warning("âš ï¸ GPUæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ (CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­)")
                        gpu_mode = "CPU"
                except (subprocess.TimeoutExpired, FileNotFoundError) as e:
                    st.warning("âš ï¸ GPUæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ (CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­)")
                    gpu_mode = "CPU"
                
                # Ollamaãƒ—ãƒ­ã‚»ã‚¹ç¢ºèª
                try:
                    ollama_process = subprocess.run(["pgrep", "-f", "ollama"], capture_output=True, text=True, timeout=5)
                    if ollama_process.returncode == 0:
                        st.success("âœ… Ollamaãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œä¸­")
                    else:
                        st.error("âŒ Ollamaãƒ—ãƒ­ã‚»ã‚¹ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“")
                except subprocess.TimeoutExpired:
                    st.warning("âš ï¸ Ollamaãƒ—ãƒ­ã‚»ã‚¹ç¢ºèªã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                    
                # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±
                try:
                    import psutil
                    cpu_percent = psutil.cpu_percent(interval=1)
                    memory = psutil.virtual_memory()
                    disk = psutil.disk_usage('/')
                    
                    st.write(f"**å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰:** {gpu_mode}")
                    st.write(f"**CPUä½¿ç”¨ç‡:** {cpu_percent}%")
                    st.write(f"**ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡:** {memory.percent}%")
                    st.write(f"**åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª:** {memory.available / (1024**3):.1f}GB")
                    st.write(f"**ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡:** {disk.percent}%")
                    st.write(f"**åˆ©ç”¨å¯èƒ½ãƒ‡ã‚£ã‚¹ã‚¯:** {disk.free / (1024**3):.1f}GB")
                    
                    # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
                    st.write("**å®Ÿè¡Œä¸­ã®ãƒ—ãƒ­ã‚»ã‚¹:**")
                    for proc in psutil.process_iter(['pid', 'name', 'cpu_percent']):
                        try:
                            if 'ollama' in proc.info['name'].lower() or 'streamlit' in proc.info['name'].lower() or 'fastapi' in proc.info['name'].lower():
                                st.write(f"- {proc.info['name']} (PID: {proc.info['pid']}, CPU: {proc.info['cpu_percent']}%)")
                        except (psutil.NoSuchProcess, psutil.AccessDenied):
                            continue
                            
                except ImportError:
                    st.info("psutilãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                except Exception as e:
                    st.warning(f"ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    
            except Exception as e:
                st.error(f"ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
        st.subheader("ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ç®¡ç†")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ“¥ llama3.1:8b", help="llama3.1:8bãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
                with st.spinner("llama3.1:8bã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                    try:
                        result = subprocess.run(["ollama", "pull", "llama3.1:8b"], 
                                            capture_output=True, text=True, timeout=300)
                        if result.returncode == 0:
                            st.success("âœ… llama3.1:8bãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                        else:
                            st.error(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {result.stderr}")
                    except subprocess.TimeoutExpired:
                        st.error("âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                    except Exception as e:
                        st.error(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        with col2:
            if st.button("ğŸ“¥ llama3.2", help="llama3.2ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
                with st.spinner("llama3.2ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                    try:
                        result = subprocess.run(["ollama", "pull", "llama3.2:latest"], 
                                            capture_output=True, text=True, timeout=300)
                        if result.returncode == 0:
                            st.success("âœ… llama3.2ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                        else:
                            st.error(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {result.stderr}")
                    except subprocess.TimeoutExpired:
                        st.error("âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                    except Exception as e:
                        st.error(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        with col3:
            if st.button("ğŸ“¥ llama3.2-vision", help="llama3.2-visionãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
                with st.spinner("llama3.2-visionã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                    try:
                        result = subprocess.run(["ollama", "pull", "llama3.2-vision:latest"], 
                                            capture_output=True, text=True, timeout=300)
                        if result.returncode == 0:
                            st.success("âœ… llama3.2-visionãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                        else:
                            st.error(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {result.stderr}")
                    except subprocess.TimeoutExpired:
                        st.error("âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                    except Exception as e:
                        st.error(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if st.button("ğŸ“¦ å…¨ãƒ¢ãƒ‡ãƒ«ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", help="ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€åº¦ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
            models = ["llama3.1:8b", "llama3.2:latest", "llama3.2-vision:latest"]
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, model in enumerate(models):
                status_text.text(f"{model}ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­... ({i+1}/{len(models)})")
                try:
                    result = subprocess.run(["ollama", "pull", model], 
                                        capture_output=True, text=True, timeout=300)
                    if result.returncode == 0:
                        st.success(f"âœ… {model}ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                    else:
                        st.error(f"âŒ {model}ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {result.stderr}")
                except subprocess.TimeoutExpired:
                    st.error(f"âŒ {model}ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                except Exception as e:
                    st.error(f"âŒ {model}ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
                
                progress_bar.progress((i + 1) / len(models))
            
            status_text.text("å…¨ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")
        
        # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º
        if st.button("ğŸ“‹ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º"):
            try:
                result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
                if result.returncode == 0:
                    st.success("âœ… åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
                    st.code(result.stdout)
                else:
                    st.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—å¤±æ•—: {result.stderr}")
            except Exception as e:
                st.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ä¼šè©±å±¥æ­´ç®¡ç†
        st.subheader("ğŸ’¬ ä¼šè©±å±¥æ­´")
        if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"):
            st.session_state.conversation_history = []
            st.rerun()
        
        if st.button("ğŸ’¾ å±¥æ­´ã‚’ä¿å­˜"):
            if st.session_state.conversation_history:
                filename = save_conversation(
                    st.session_state.conversation_history,
                    st.session_state.current_personality
                )
                st.success(f"ä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
            else:
                st.warning("ä¿å­˜ã™ã‚‹ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # çµ±è¨ˆæƒ…å ±
        st.subheader("ğŸ“Š çµ±è¨ˆ")
        st.write(f"ä¼šè©±æ•°: {len(st.session_state.conversation_history)}")
        if st.session_state.conversation_history:
            user_messages = [msg for msg in st.session_state.conversation_history if msg["role"] == "user"]
            ai_messages = [msg for msg in st.session_state.conversation_history if msg["role"] == "assistant"]
            st.write(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€: {len(user_messages)}")
            st.write(f"AIå¿œç­”: {len(ai_messages)}")
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ™ï¸ éŸ³å£°å…¥åŠ›")
        
        # å…¥åŠ›æ–¹æ³•é¸æŠ
        input_method = st.radio(
            "å…¥åŠ›æ–¹æ³•ã‚’é¸æŠ:",
            ["ğŸ™ï¸ éŸ³å£°å…¥åŠ›", "ğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›", "ğŸ¤– è‡ªå‹•å¿œç­”"],
            horizontal=True,
            help="éŸ³å£°ã€ãƒ†ã‚­ã‚¹ãƒˆã€ã¾ãŸã¯è‡ªå‹•å¿œç­”ã§AIã¨å¯¾è©±ã§ãã¾ã™"
        )
        
        if input_method == "ğŸ™ï¸ éŸ³å£°å…¥åŠ›":
            # éŸ³å£°éŒ²éŸ³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
            audio_data, sample_rate = audio_recorder_component(key="ollama_audio")
            
            # éŸ³å£°èªè­˜ãƒœã‚¿ãƒ³
            if st.button("ğŸ¤ éŸ³å£°èªè­˜", help="éŒ²éŸ³ã—ãŸéŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"):
                if audio_data is not None:
                    with st.spinner("éŸ³å£°èªè­˜ä¸­..."):
                        try:
                            # faster-whisperã§éŸ³å£°èªè­˜
                            from faster_whisper import WhisperModel
                            
                            # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
                            model = WhisperModel("base", compute_type="int8")
                            
                            # éŸ³å£°èªè­˜å®Ÿè¡Œ
                            segments, info = model.transcribe(audio_data, language="ja")
                            
                            # èªè­˜çµæœã‚’çµåˆ
                            recognized_text = " ".join([segment.text for segment in segments])
                            
                            if recognized_text.strip():
                                st.session_state.recognized_text = recognized_text
                                st.success(f"èªè­˜çµæœ: {recognized_text}")
                            else:
                                st.warning("éŸ³å£°ãŒèªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                                 
                        except Exception as e:
                            st.error(f"éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
                else:
                    st.warning("éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚éŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚")
        
        else:  # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›
            # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚¨ãƒªã‚¢
            user_text = st.text_area(
                "ğŸ’¬ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›:",
                value=st.session_state.get("user_input_text", ""),
                height=100,
                placeholder="ã“ã“ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
                help="AIã¨ã®å¯¾è©±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¾ã™"
            )
            
            # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
            st.session_state.user_input_text = user_text
            
            # ãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡ãƒœã‚¿ãƒ³
            if st.button("ğŸ“¤ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡", help="å…¥åŠ›ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’AIã«é€ä¿¡"):
                if user_text.strip():
                    st.session_state.recognized_text = user_text.strip()
                    st.success(f"é€ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {user_text.strip()}")
                else:
                    st.warning("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
        else:
                    if input_method == "ğŸ¤– è‡ªå‹•å¿œç­”":
                    st.subheader("ğŸ¤– è‡ªå‹•å¿œç­”ãƒ¢ãƒ¼ãƒ‰")
                    st.write("AIãŒè‡ªå‹•çš„ã«ä¼šè©±ã‚’é–‹å§‹ã—ã€ç¶™ç¶šçš„ã«å¿œç­”ã—ã¾ã™ã€‚")
            
                    # è‡ªå‹•å¿œç­”è¨­å®š
                    col_auto1, col_auto2 = st.columns([2, 1])
                    with col_auto1:
                        auto_topic = st.selectbox(
                            "ä¼šè©±ãƒˆãƒ”ãƒƒã‚¯ã‚’é¸æŠ:",
                            ["å¤©æ°—ã«ã¤ã„ã¦", "æœ€æ–°ã®æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹", "è‡ªå·±ç´¹ä»‹", "é›‘è«‡", "å°‚é–€çš„ãªç›¸è«‡"],
                            help="AIãŒè‡ªå‹•çš„ã«è©±é¡Œã‚’æä¾›ã—ã¾ã™"
                        )
            
                    with col_auto2:
                        auto_count = st.number_input(
                            "å¿œç­”å›æ•°:",
                            min_value=1,
                            max_value=10,
                            value=3,
                            help="è‡ªå‹•å¿œç­”ã®å›æ•°ã‚’è¨­å®š"
                        )
            
                    # è‡ªå‹•å¿œç­”é–‹å§‹ãƒœã‚¿ãƒ³
                    if st.button("ğŸš€ è‡ªå‹•å¿œç­”é–‹å§‹", help="AIãŒè‡ªå‹•çš„ã«ä¼šè©±ã‚’é–‹å§‹ã—ã¾ã™"):
                        with st.spinner("AIãŒè‡ªå‹•å¿œç­”ã‚’ç”Ÿæˆä¸­..."):
                            try:
                                # åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
                                topic_prompts = {
                                    "å¤©æ°—ã«ã¤ã„ã¦": "ä»Šæ—¥ã®å¤©æ°—ã«ã¤ã„ã¦è‡ªç„¶ãªä¼šè©±ã‚’å§‹ã‚ã¦ãã ã•ã„ã€‚å¤©æ°—ã®è©±é¡Œã‹ã‚‰é–¢é€£ã™ã‚‹è©±é¡Œã«åºƒã’ã¦ãã ã•ã„ã€‚",
                                    "æœ€æ–°ã®æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹": "æœ€æ–°ã®æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦èˆˆå‘³æ·±ã„è©±é¡Œã‚’æä¾›ã—ã€è§£èª¬ã—ã¦ãã ã•ã„ã€‚",
                                    "è‡ªå·±ç´¹ä»‹": "è‡ªå·±ç´¹ä»‹ã‚’ã—ã¦ãã ã•ã„ã€‚ã‚ãªãŸã®èƒ½åŠ›ã‚„ç‰¹å¾´ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                                    "é›‘è«‡": "æ¥½ã—ã„é›‘è«‡ã‚’ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¥½ã—ã¾ã›ã‚‹ã‚ˆã†ãªè©±é¡Œã‚’é¸ã‚“ã§ãã ã•ã„ã€‚",
                                    "å°‚é–€çš„ãªç›¸è«‡": "å°‚é–€å®¶ã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç›¸è«‡ã—ãŸã„ã§ã‚ã‚ã†å°‚é–€çš„ãªè³ªå•ã¨å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
                                }
                        
                                # äººæ ¼ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
                                personality = st.session_state.current_personality
                                personalities = {
                                    "friend": {
                                        "name": "è¦ªå‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
                                        "prompt": "ã‚ãªãŸã¯è¦ªã—ã¿ã‚„ã™ã„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªå£èª¿ã§ã€æŠ€è¡“çš„ãªã“ã¨ã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                                        "icon": "ğŸ‘¨â€ğŸ’»"
                                    },
                                    "copy": {
                                        "name": "åˆ†èº«",
                                        "prompt": "ã‚ãªãŸã¯ç§ã®åˆ†èº«ã§ã™ã€‚ç§ã®è€ƒãˆæ–¹ã‚„è©±ã—æ–¹ã‚’çœŸä¼¼ã—ã¦ã€å…±æ„Ÿçš„ã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚",
                                        "icon": "ğŸª"
                                    },
                                    "expert": {
                                        "name": "å°‚é–€å®¶",
                                        "prompt": "ã‚ãªãŸã¯AIã®å°‚é–€å®¶ã§ã™ã€‚æ­£ç¢ºã§è©³ç´°ãªæƒ…å ±ã‚’ã€å°‚é–€ç”¨èªã‚’é©åˆ‡ã«ä½¿ã„ãªãŒã‚‰æä¾›ã—ã¦ãã ã•ã„ã€‚",
                                        "icon": "ğŸ“"
                                    }
                                }
                        
                                current_personality = personalities[personality]
                                base_prompt = topic_prompts[auto_topic]
                        
                                # è‡ªå‹•å¿œç­”ç”Ÿæˆ
                                auto_responses = []
                                for i in range(auto_count):
                                    # ä¼šè©±å±¥æ­´ã‚’æ•´å½¢
                                    conversation_history = st.session_state.conversation_history[-5:]
                                    history_text = ""
                                    for conv in conversation_history:
                                        history_text += f"User: {conv['user']}\nAssistant: {conv['assistant']}\n"
                            
                                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
                                    if i == 0:
                                        prompt = f"""{current_personality['prompt']}

        {base_prompt}

        {history_text}
        Assistant:"""
                                    else:
                                        # å‰ã®å¿œç­”ã‹ã‚‰æ¬¡ã®è©±é¡Œã‚’ç”Ÿæˆ
                                        prev_response = auto_responses[-1] if auto_responses else ""
                                        prompt = f"""{current_personality['prompt']}

        å‰ã®å¿œç­”ã‹ã‚‰è‡ªç„¶ã«ä¼šè©±ã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚æ–°ã—ã„è¦–ç‚¹ã‚„é–¢é€£ã™ã‚‹è©±é¡Œã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

        å‰ã®å¿œç­”: {prev_response}

        {history_text}
        Assistant:"""
                            
                                    # Ollamaã§å¿œç­”ç”Ÿæˆ
                                    response = st.session_state.ollama.generate_response(prompt)
                            
                                    if response:
                                        auto_responses.append(response)
                                
                                        # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
                                        st.session_state.conversation_history.append({
                                            "user": f"è‡ªå‹•å¿œç­” {i+1} ({auto_topic})",
                                            "assistant": response,
                                            "personality": personality,
                                            "timestamp": datetime.now().isoformat()
                                        })
                        
                                # è‡ªå‹•å¿œç­”çµæœã‚’è¡¨ç¤º
                                st.success(f"âœ… è‡ªå‹•å¿œç­”ã‚’ {len(auto_responses)} ä»¶ç”Ÿæˆã—ã¾ã—ãŸï¼")
                        
                                for i, response in enumerate(auto_responses):
                                    with st.expander(f"ğŸ¤– è‡ªå‹•å¿œç­” {i+1}"):
                                        st.write(response)
                        
                                # VRMã‚¢ãƒã‚¿ãƒ¼è¡¨æƒ…æ›´æ–°
                                st.session_state.vrm_controller.set_personality(personality)
                        
                            except Exception as e:
                                st.error(f"è‡ªå‹•å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        
                # èªè­˜çµæœãƒ»å…¥åŠ›çµæœè¡¨ç¤º
                if "recognized_text" in st.session_state and st.session_state.recognized_text:
                    st.subheader("ğŸ’­ å…¥åŠ›å†…å®¹")
                    st.write(st.session_state.recognized_text)
            
                    # AIå¿œç­”ç”Ÿæˆ
                    if st.button("ğŸ¤– AIå¿œç­”ç”Ÿæˆ", help="å…¥åŠ›å†…å®¹ã«å¯¾ã™ã‚‹AIå¿œç­”ã‚’ç”Ÿæˆ"):
                        with st.spinner("AIå¿œç­”ã‚’ç”Ÿæˆä¸­..."):
                            try:
                                # äººæ ¼ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
                                personality = st.session_state.current_personality
                                personalities = {
                                    "friend": {
                                        "name": "è¦ªå‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
                                        "prompt": "ã‚ãªãŸã¯è¦ªã—ã¿ã‚„ã™ã„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªå£èª¿ã§ã€æŠ€è¡“çš„ãªã“ã¨ã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                                        "icon": "ğŸ‘¨â€ğŸ’»"
                                    },
                                    "copy": {
                                        "name": "åˆ†èº«",
                                        "prompt": "ã‚ãªãŸã¯ç§ã®åˆ†èº«ã§ã™ã€‚ç§ã®è€ƒãˆæ–¹ã‚„è©±ã—æ–¹ã‚’çœŸä¼¼ã—ã¦ã€å…±æ„Ÿçš„ã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚",
                                        "icon": "ğŸª"
                                    },
                                    "expert": {
                                        "name": "å°‚é–€å®¶",
                                        "prompt": "ã‚ãªãŸã¯AIã®å°‚é–€å®¶ã§ã™ã€‚æ­£ç¢ºã§è©³ç´°ãªæƒ…å ±ã‚’ã€å°‚é–€ç”¨èªã‚’é©åˆ‡ã«ä½¿ã„ãªãŒã‚‰æä¾›ã—ã¦ãã ã•ã„ã€‚",
                                        "icon": "ğŸ“"
                                    }
                                }
                        
                                current_personality = personalities[personality]
                        
                                # ä¼šè©±å±¥æ­´ã‚’æ•´å½¢
                                conversation_history = st.session_state.conversation_history[-5:]  # ç›´è¿‘5ä»¶ã‚’ä½¿ç”¨
                                history_text = ""
                                for conv in conversation_history:
                                    history_text += f"User: {conv['user']}\nAssistant: {conv['assistant']}\n"
                        
                                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
                                prompt = f"""{current_personality['prompt']}

        {history_text}User: {st.session_state.recognized_text}
        Assistant:"""
                        
                                # Ollamaã§å¿œç­”ç”Ÿæˆ
                                response = st.session_state.ollama.generate_response(prompt)
                        
                                if response:
                                    # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
                                    st.session_state.conversation_history.append({
                                        "user": st.session_state.recognized_text,
                                        "assistant": response,
                                        "personality": personality,
                                        "timestamp": datetime.now().isoformat()
                                    })
                            
                                    # å¿œç­”è¡¨ç¤º
                                    st.subheader(f"ğŸ¤– {current_personality['name']}ã®å¿œç­”")
                                    st.write(response)
                            
                                    # VRMã‚¢ãƒã‚¿ãƒ¼è¡¨æƒ…æ›´æ–°
                                    st.session_state.vrm_controller.set_personality(personality)
                            
                                    # éŸ³å£°åˆæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                                    if st.button("ğŸ”Š å¿œç­”ã‚’éŸ³å£°ã§å†ç”Ÿ", key="tts_button"):
                                        try:
                                            import pyttsx3
                                            engine = pyttsx3.init()
                                            engine.say(response)
                                            engine.runAndWait()
                                            st.success("éŸ³å£°å†ç”Ÿå®Œäº†")
                                        except Exception as e:
                                            st.error(f"éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                            
                                    # å…¥åŠ›ã‚’ã‚¯ãƒªã‚¢
                                    st.session_state.recognized_text = ""
                                    if input_method == "ğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›":
                                        st.session_state.user_input_text = ""
                        
                                else:
                                    st.error("AIå¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                            
                            except Exception as e:
                                st.error(f"AIå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    
            with col2:
                st.header("ğŸ¤– VRMã‚¢ãƒã‚¿ãƒ¼")
        
                # VRMã‚¢ãƒã‚¿ãƒ¼è¡¨ç¤º
                vrm_html = st.session_state.vrm_controller.get_vrm_html()
                st.components.v1.html(vrm_html, height=450)
        
                # äººæ ¼æƒ…å ±è¡¨ç¤º
                st.subheader("ğŸ­ ç¾åœ¨ã®äººæ ¼")
                personalities = {
                    "friend": {"name": "è¦ªå‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢", "color": "#4CAF50", "icon": "ğŸ˜Š"},
                    "copy": {"name": "åˆ†èº«", "color": "#2196F3", "icon": "ğŸª"},
                    "expert": {"name": "ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ", "color": "#9C27B0", "icon": "ğŸ“"}
                }
                current_info = personalities[st.session_state.current_personality]
                st.markdown(f"""
                <div style="padding: 10px; border-radius: 8px; background-color: {current_info['color']}20; border: 2px solid {current_info['color']};">
                    <h3 style="color: {current_info['color']}; margin: 0;">{current_info['icon']} {current_info['name']}</h3>
                    <p style="margin: 5px 0;">è¡¨æƒ…: {st.session_state.vrm_controller.expressions[st.session_state.current_personality]}</p>
                </div>
                """, unsafe_allow_html=True)
    
            # ä¼šè©±å±¥æ­´è¡¨ç¤º
            st.header("ğŸ’¬ ä¼šè©±å±¥æ­´")
    
            if st.session_state.conversation_history:
                for i, msg in enumerate(reversed(st.session_state.conversation_history[-10:])):
                    if msg["role"] == "user":
                        st.markdown(f"ğŸ‘¤ **ã‚ãªãŸ**: {msg['content']}")
                    else:
                        st.markdown(f"ğŸ¤– **AI**: {msg['content']}")
                    st.divider()
            else:
                st.info("ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚éŸ³å£°å…¥åŠ›ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã§ä¼šè©±ã‚’å§‹ã‚ã¦ãã ã•ã„ã€‚")
    
            # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
            st.markdown("---")
            st.markdown("### ğŸ“‹ ä½¿ã„æ–¹")
            st.markdown("""
            1. **ğŸ”§ ãƒã‚¤ã‚¯ãƒ†ã‚¹ãƒˆ**: ãƒã‚¤ã‚¯ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã‹ç¢ºèª
            2. **ğŸ­ äººæ ¼é¸æŠ**: 3ã¤ã®äººæ ¼ã‹ã‚‰é¸æŠ
            3. **ğŸ¤ éŸ³å£°å…¥åŠ›**: éŸ³å£°ã‚’éŒ²éŸ³ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
            4. **ğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›**: ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦å¯¾è©±
            5. **ğŸ¤– AIå¿œç­”**: Ollamaã§å¿œç­”ç”Ÿæˆ
            6. **ğŸ¤– VRMè¡¨ç¤º**: 3Dã‚¢ãƒã‚¿ãƒ¼ã§è¡¨æƒ…è¡¨ç¤º
            7. **ğŸ’¾ ä¿å­˜**: ä¼šè©±å±¥æ­´ã‚’ä¿å­˜
            """)
    
            # æŠ€è¡“æƒ…å ±
            with st.expander("ğŸ”§ æŠ€è¡“æƒ…å ±"):
                st.markdown("""
                **ä½¿ç”¨æŠ€è¡“:**
                - WebRTC/MediaRecorder API (éŸ³å£°éŒ²éŸ³)
                - faster-whisper (éŸ³å£°èªè­˜)
                - Ollama + llama3.1:8b (AIå¿œç­”)
                - pyttsx3 (éŸ³å£°åˆæˆ)
                - Three.js + three-vrm (3Dã‚¢ãƒã‚¿ãƒ¼)
                - Streamlit (UIãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯)
        
                **ç‰¹å¾´:**
                - ãƒ­ãƒ¼ã‚«ãƒ«AIãƒ¢ãƒ‡ãƒ« (Ollama)
                - 3Dã‚¢ãƒã‚¿ãƒ¼é€£æº (VRM)
                - ãƒãƒ«ãƒäººæ ¼ã‚·ã‚¹ãƒ†ãƒ 
                - éŸ³å£°å¯¾è©±æ©Ÿèƒ½
                - ä¼šè©±å±¥æ­´ç®¡ç†
                """)

        if __name__ == "__main__":
            main()
