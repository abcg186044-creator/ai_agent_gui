#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ollama APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
"""

import requests
import json
import time
import threading
from queue import Queue

class OllamaClient:
    def __init__(self, base_url="http://localhost:11434", model="llama3.1:8b", timeout=180):
        self.base_url = base_url
        self.model = model
        self.timeout = timeout
        self.progress_queue = Queue()
        self.is_generating = False

    def _progress_reporter(self, callback):
        """é€”ä¸­å ±å‘Šã‚’è¡Œã†ã‚¹ãƒ¬ãƒƒãƒ‰"""
        import time as time_module
        start_time = time_module.time()
        progress_steps = [
            "ğŸ” Ollamaã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šä¸­...",
            "ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ä¸­...",
            "ğŸ¤– AIãƒ¢ãƒ‡ãƒ«ãŒæ€è€ƒã‚’é–‹å§‹...",
            "ğŸ§  è¨€èªãƒ¢ãƒ‡ãƒ«ãŒå¿œç­”ã‚’ç”Ÿæˆä¸­...",
            "âš¡ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ•´å½¢ä¸­...",
            "âœ… å¿œç­”ç”Ÿæˆå®Œäº†"
        ]
        step = 0
        while self.is_generating and step < len(progress_steps):
            elapsed = time_module.time() - start_time
            progress_percent = min((elapsed / self.timeout) * 100, 95)
            callback({
                "step": progress_steps[step],
                "progress": progress_percent,
                "elapsed": elapsed,
                "remaining": max(self.timeout - elapsed, 0)
            })
            step += 1
            time_module.sleep(10)
        if self.is_generating:
            callback({
                "step": "ğŸ”„ ã¾ã å¿œç­”ã‚’ç”Ÿæˆä¸­...ã‚‚ã†å°‘ã€…ãŠå¾…ã¡ãã ã•ã„",
                "progress": 95,
                "elapsed": time_module.time() - start_time,
                "remaining": max(self.timeout - (time_module.time() - start_time), 0)
            })

    def generate_response(self, prompt, max_tokens=1000, progress_callback=None):
        """Ollama APIã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆï¼ˆé€”ä¸­å ±å‘Šä»˜ãï¼‰"""
        self.is_generating = True
        progress_thread = None
        
        if progress_callback:
            # é€”ä¸­å ±å‘Šã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
            progress_thread = threading.Thread(
                target=self._progress_reporter, 
                args=(progress_callback,),
                daemon=True
            )
            progress_thread.start()
        """Ollama APIã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ"""
        try:
            url = f"{self.base_url}/api/generate"
            
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": max_tokens
                }
            }
            
            print(f"ğŸ” Ollama APIå‘¼ã³å‡ºã—: {self.base_url}")
            print(f"ğŸ” ãƒ¢ãƒ‡ãƒ«: {self.model}")
            print(f"ğŸ” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)} æ–‡å­—")
            
            response = requests.post(
                url,
                json=payload,
                timeout=self.timeout,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                print(f"âŒ APIã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return f"APIã‚¨ãƒ©ãƒ¼: {response.status_code}"
                
        except requests.exceptions.Timeout:
            print(f"âŒ Ollama APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{self.timeout}ç§’ï¼‰")
            return f"AIå¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{self.timeout}ç§’ï¼‰ã€‚æ™‚é–“ã‚’ç½®ã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        except Exception as e:
            print(f"âŒ Ollama APIã‚¨ãƒ©ãƒ¼: {str(e)}")
            return f"APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"
        finally:
            self.is_generating = False
