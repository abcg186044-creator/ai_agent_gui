import streamlit as st
import requests
import json
import time
import os
import logging
from datetime import datetime
import uuid

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MemoryAwareAIAgent:
    def __init__(self):
        self.ollama_host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
        self.model = os.getenv('OLLAMA_MODEL', 'llama3.2')
        self.wait_timeout = int(os.getenv('OLLAMA_WAIT_TIMEOUT', '30'))
        self.memory_enabled = os.getenv('MEMORY_ENABLED', 'true').lower() == 'true'
        self.chroma_path = os.getenv('CHROMA_DB_PATH', '/app/data/chroma')
        
        # è¨˜æ†¶é–¢é€£ã®ãƒ‘ã‚¹
        self.memory_path = os.path.join(self.chroma_path, 'memory')
        self.conversation_path = os.path.join(self.chroma_path, 'conversations')
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®ç”Ÿæˆ
        if 'conversation_id' not in st.session_state:
            st.session_state.conversation_id = str(uuid.uuid4())
        
        # è¨˜æ†¶èª­ã¿è¾¼ã¿æ¸ˆã¿ãƒ•ãƒ©ã‚°
        if 'memory_loaded' not in st.session_state:
            st.session_state.memory_loaded = False
        
        # ä¼šè©±å±¥æ­´ã®åˆæœŸåŒ–
        if 'messages' not in st.session_state:
            st.session_state.messages = []
        
        # è¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åˆæœŸåŒ–
        if 'memory_context' not in st.session_state:
            st.session_state.memory_context = ""
    
    def wait_for_ollama(self):
        """OllamaãŒèµ·å‹•ã™ã‚‹ã®ã‚’å¾…ã¤"""
        logger.info("ğŸ”„ Waiting for Ollama to start...")
        
        start_time = time.time()
        while time.time() - start_time < self.wait_timeout:
            try:
                response = requests.get(f"{self.ollama_host}/api/tags", timeout=5)
                if response.status_code == 200:
                    logger.info("âœ… Ollama is ready")
                    return True
            except requests.exceptions.RequestException:
                pass
            
            logger.info("â³ Waiting for Ollama...")
            time.sleep(3)
        
        logger.error("âŒ Ollama startup timeout")
        return False
    
    def check_model(self):
        """ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª"""
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                available_models = [model['name'] for model in data.get('models', [])]
                return self.model in available_models
            return False
        except:
            return False
    
    def load_memory_context(self):
        """è¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
        if not self.memory_enabled or st.session_state.memory_loaded:
            return
        
        memory_file = os.path.join(self.memory_path, 'memory_summary.json')
        
        if os.path.exists(memory_file):
            try:
                with open(memory_file, 'r', encoding='utf-8') as f:
                    memory_data = json.load(f)
                
                # è¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
                context_parts = []
                
                if memory_data.get('user_preferences'):
                    context_parts.append("## User Preferences:")
                    for key, value in memory_data['user_preferences'].items():
                        context_parts.append(f"- {key}: {value}")
                
                if memory_data.get('important_topics'):
                    context_parts.append("\n## Important Topics:")
                    for topic in memory_data['important_topics']:
                        context_parts.append(f"- {topic}")
                
                if memory_data.get('last_updated'):
                    context_parts.append(f"\n## Memory Last Updated: {memory_data['last_updated']}")
                
                st.session_state.memory_context = "\n".join(context_parts)
                st.session_state.memory_loaded = True
                
                logger.info("ğŸ“š Memory context loaded")
                return True
                
            except Exception as e:
                logger.error(f"âŒ Failed to load memory context: {e}")
        
        st.session_state.memory_loaded = True
        return False
    
    def save_conversation(self, title=None):
        """ä¼šè©±ã‚’ä¿å­˜"""
        if not self.memory_enabled or not st.session_state.messages:
            return False
        
        try:
            # ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
            conversation_data = {
                'id': st.session_state.conversation_id,
                'title': title or f"Conversation {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                'timestamp': datetime.now().isoformat(),
                'messages': st.session_state.messages,
                'message_count': len(st.session_state.messages)
            }
            
            # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
            os.makedirs(self.conversation_path, exist_ok=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            file_name = f"conversation_{st.session_state.conversation_id}.json"
            file_path = os.path.join(self.conversation_path, file_name)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(conversation_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ Conversation saved: {file_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to save conversation: {e}")
            return False
    
    def generate_response(self, prompt):
        """AIå¿œç­”ã‚’ç”Ÿæˆ"""
        try:
            # è¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
            full_prompt = prompt
            if st.session_state.memory_context:
                full_prompt = f"""You are an AI assistant with long-term memory. Here is your memory context about the user:

{st.session_state.memory_context}

Current conversation:
{prompt}

Please respond naturally while keeping the memory context in mind."""
            
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "max_tokens": 1000
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', 'å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“')
            else:
                return f"ã‚¨ãƒ©ãƒ¼: {response.status_code}"
                
        except Exception as e:
            return f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def display_memory_status(self):
        """è¨˜æ†¶çŠ¶æ…‹ã‚’è¡¨ç¤º"""
        if not self.memory_enabled:
            st.warning("âš ï¸ è¨˜æ†¶æ©Ÿèƒ½ãŒç„¡åŠ¹ã«ãªã£ã¦ã„ã¾ã™")
            return
        
        if st.session_state.memory_loaded:
            st.success("âœ… è¨˜æ†¶ãŒèª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸ")
            if st.session_state.memory_context:
                with st.expander("ğŸ“š è¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"):
                    st.text(st.session_state.memory_context)
        else:
            st.info("ğŸ“š è¨˜æ†¶ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    def display_conversation_controls(self):
        """ä¼šè©±åˆ¶å¾¡ã‚’è¡¨ç¤º"""
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button("ğŸ’¾ ä¼šè©±ã‚’ä¿å­˜", help="ç¾åœ¨ã®ä¼šè©±ã‚’è¨˜æ†¶ã«ä¿å­˜ã—ã¾ã™"):
                if self.save_conversation():
                    st.success("âœ… ä¼šè©±ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                else:
                    st.error("âŒ ä¼šè©±ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        with col2:
            if st.button("ğŸ—‘ï¸ ä¼šè©±ã‚’ã‚¯ãƒªã‚¢", help="ç¾åœ¨ã®ä¼šè©±ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™"):
                st.session_state.messages = []
                st.session_state.conversation_id = str(uuid.uuid4())
                st.rerun()
        
        with col3:
            title = st.text_input("ä¼šè©±ã‚¿ã‚¤ãƒˆãƒ«", help="ä¼šè©±ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", key="conversation_title")
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        st.set_page_config(
            page_title="AI Agent System - Memory Enabled",
            page_icon="ğŸ§ ",
            layout="centered"
        )
        
        st.title("ğŸ§  AI Agent System - Memory Enabled")
        st.markdown("---")
        
        # AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–
        if 'ai_agent' not in st.session_state:
            st.session_state.ai_agent = self
        
        ai_agent = st.session_state.ai_agent
        
        # è¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®èª­ã¿è¾¼ã¿
        ai_agent.load_memory_context()
        
        # è¨˜æ†¶çŠ¶æ…‹ã®è¡¨ç¤º
        ai_agent.display_memory_status()
        
        # ä¼šè©±åˆ¶å¾¡ã®è¡¨ç¤º
        ai_agent.display_conversation_controls()
        
        # Ollamaã®çŠ¶æ…‹ç¢ºèª
        with st.spinner("ğŸ”„ Ollamaã®çŠ¶æ…‹ã‚’ç¢ºèªä¸­..."):
            if not ai_agent.wait_for_ollama():
                st.error("âŒ OllamaãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
                st.stop()
        
        # ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
        if not ai_agent.check_model():
            st.warning(f"âš ï¸ ãƒ¢ãƒ‡ãƒ« '{ai_agent.model}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.info("ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã¯ã‚¤ãƒ¡ãƒ¼ã‚¸å†…ã«çµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã¯ãšã§ã™")
            st.stop()
        
        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        st.success(f"âœ… OllamaãŒæ­£å¸¸ã«èµ·å‹•ã—ã¾ã—ãŸ (ãƒ¢ãƒ‡ãƒ«: {ai_agent.model})")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
        if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # AIå¿œç­”
            with st.chat_message("assistant"):
                with st.spinner("ğŸ¤– è€ƒãˆä¸­..."):
                    response = ai_agent.generate_response(prompt)
                st.markdown(response)
            
            # AIå¿œç­”ã‚’è¿½åŠ 
            st.session_state.messages.append({"role": "assistant", "content": response})
            
            # è‡ªå‹•ä¿å­˜ï¼ˆ10ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã”ã¨ï¼‰
            if len(st.session_state.messages) % 10 == 0:
                ai_agent.save_conversation()
        
        # ã‚µã‚¤ãƒ‰ãƒãƒ¼æƒ…å ±
        with st.sidebar:
            st.header("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
            
            # Ollamaæƒ…å ±
            st.subheader("ğŸ¤– Ollama")
            st.code(f"Host: {ai_agent.ollama_host}")
            st.code(f"Model: {ai_agent.model}")
            
            # è¨˜æ†¶æƒ…å ±
            st.subheader("ğŸ§  è¨˜æ†¶")
            st.code(f"Memory Enabled: {ai_agent.memory_enabled}")
            st.code(f"Conversation ID: {st.session_state.conversation_id}")
            st.code(f"Messages: {len(st.session_state.messages)}")
            
            # è¨˜æ†¶çµ±è¨ˆ
            if ai_agent.memory_enabled:
                try:
                    memory_file = os.path.join(ai_agent.memory_path, 'memory_summary.json')
                    if os.path.exists(memory_file):
                        with open(memory_file, 'r', encoding='utf-8') as f:
                            memory_data = json.load(f)
                        
                        st.subheader("ğŸ“Š è¨˜æ†¶çµ±è¨ˆ")
                        st.code(f"Preferences: {len(memory_data.get('user_preferences', {}))}")
                        st.code(f"Topics: {len(memory_data.get('important_topics', []))}")
                        st.code(f"Last Updated: {memory_data.get('last_updated', 'Never')}")
                except:
                    st.error("è¨˜æ†¶çµ±è¨ˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒãƒ³ãƒ‰
            st.subheader("ğŸ”§ ç®¡ç†ã‚³ãƒãƒ³ãƒ‰")
            st.code("docker logs ai-ollama --tail=20")
            st.code("docker exec -it ai-ollama bash")
            st.code("curl -f http://localhost:11434/api/tags")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    agent = MemoryAwareAIAgent()
    agent.run()

if __name__ == "__main__":
    main()
