import streamlit as st
import requests
import json
import time
import os
import logging
from datetime import datetime

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FixedAIAgent:
    def __init__(self):
        self.ollama_host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
        self.model = os.getenv('OLLAMA_MODEL', 'llama3.2')
        self.wait_timeout = int(os.getenv('OLLAMA_WAIT_TIMEOUT', '120'))
        
    def wait_for_ollama(self):
        """OllamaãŒèµ·å‹•ã™ã‚‹ã®ã‚’å¾…ã¤"""
        logger.info("ğŸ”„ Ollamaã®èµ·å‹•ã‚’å¾…ã£ã¦ã„ã¾ã™...")
        
        start_time = time.time()
        while time.time() - start_time < self.wait_timeout:
            try:
                response = requests.get(f"{self.ollama_host}/api/tags", timeout=5)
                if response.status_code == 200:
                    logger.info("âœ… OllamaãŒèµ·å‹•ã—ã¾ã—ãŸ")
                    return True
            except requests.exceptions.RequestException:
                pass
            
            logger.info("â³ Ollamaèµ·å‹•ä¸­...")
            time.sleep(5)
        
        logger.error("âŒ Ollamaã®èµ·å‹•ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        return False
    
    def check_model(self):
        """ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª"""
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                available_models = [model['name'] for model in data.get('models', [])]
                return self.model in available_models
            return False
        except:
            return False
    
    def generate_response(self, prompt):
        """AIå¿œç­”ã‚’ç”Ÿæˆ"""
        try:
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', 'å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“')
            else:
                return f"ã‚¨ãƒ©ãƒ¼: {response.status_code}"
                
        except Exception as e:
            return f"ã‚¨ãƒ©ãƒ¼: {str(e)}"

def main():
    st.set_page_config(
        page_title="AI Agent System - Fixed",
        page_icon="ğŸ¤–",
        layout="centered"
    )
    
    st.title("ğŸ¤– AI Agent System - Fixed")
    st.markdown("---")
    
    # AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–
    if 'ai_agent' not in st.session_state:
        st.session_state.ai_agent = FixedAIAgent()
    
    ai_agent = st.session_state.ai_agent
    
    # Ollamaã®çŠ¶æ…‹ç¢ºèª
    with st.spinner("ğŸ”„ Ollamaã®çŠ¶æ…‹ã‚’ç¢ºèªä¸­..."):
        if not ai_agent.wait_for_ollama():
            st.error("âŒ OllamaãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
            st.stop()
    
    # ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
    if not ai_agent.check_model():
        st.warning(f"âš ï¸ ãƒ¢ãƒ‡ãƒ« '{ai_agent.model}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.info("ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„:")
        st.code(f"docker exec -it ai-ollama ollama pull {ai_agent.model}")
        st.stop()
    
    # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    st.success(f"âœ… OllamaãŒæ­£å¸¸ã«èµ·å‹•ã—ã¾ã—ãŸ (ãƒ¢ãƒ‡ãƒ«: {ai_agent.model})")
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # AIå¿œç­”
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– è€ƒãˆä¸­..."):
                response = ai_agent.generate_response(prompt)
            st.markdown(response)
        
        # AIå¿œç­”ã‚’è¿½åŠ 
        st.session_state.messages.append({"role": "assistant", "content": response})
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼æƒ…å ±
    with st.sidebar:
        st.header("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        
        # Ollamaæƒ…å ±
        st.subheader("ğŸ¤– Ollama")
        st.code(f"Host: {ai_agent.ollama_host}")
        st.code(f"Model: {ai_agent.model}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
        try:
            response = requests.get(f"{ai_agent.ollama_host}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                st.subheader("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«")
                for model in models:
                    st.code(f"â€¢ {model}")
        except:
            st.error("ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒãƒ³ãƒ‰
        st.subheader("ğŸ”§ ç®¡ç†ã‚³ãƒãƒ³ãƒ‰")
        st.code("docker logs ai-ollama --tail=20")
        st.code("docker exec -it ai-ollama bash")
        st.code("curl -f http://localhost:11434/api/tags")

if __name__ == "__main__":
    main()
