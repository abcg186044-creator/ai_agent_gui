#!/usr/bin/env python3
"""
Fixed Smart Voice AI Agent - PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³ç«¶åˆä¿®æ­£ç‰ˆ
"""

import streamlit as st
import time
import threading
import numpy as np
import requests
import json
import queue
import tempfile
import wave
import os
import sys
import importlib

# ä¿®æ­£ç‰ˆå‹•çš„ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append('/app/scripts')
try:
    from dynamic_installer_fixed import install_package, auto_install_missing_packages, DynamicInstallerFixed
except ImportError:
    st.error("âŒ ä¿®æ­£ç‰ˆå‹•çš„ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    sys.exit(1)

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å‹•çš„ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§è€ƒæ…®ï¼‰
def install_required_packages_fixed():
    """å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å‹•çš„ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§è€ƒæ…®ï¼‰"""
    # PyTorché–¢é€£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®äº’æ›æ€§ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    pytorch_packages = {
        'torch': '2.1.0',
        'torchaudio': '2.1.0',
        'torchvision': '0.16.0'
    }
    
    # ãã®ä»–ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
    other_packages = [
        'sounddevice',
        'faster-whisper',
        'pyttsx3'
    ]
    
    installer = DynamicInstallerFixed()
    
    # ã¾ãšPyTorché–¢é€£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    st.info("ğŸ”§ Installing PyTorch packages with compatible versions...")
    for package, version in pytorch_packages.items():
        try:
            import_name = package.replace('-', '_')
            importlib.import_module(import_name)
            st.success(f"âœ… {package} is already installed")
        except ImportError:
            st.info(f"ğŸ“¦ Installing {package}=={version}...")
            success, message = install_package(package, version)
            if success:
                st.success(f"âœ… {message}")
            else:
                st.error(f"âŒ {message}")
                return False
    
    # æ¬¡ã«ãã®ä»–ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    st.info("ğŸ”§ Installing other required packages...")
    for package in other_packages:
        try:
            import_name = package.replace('-', '_')
            importlib.import_module(import_name)
            st.success(f"âœ… {package} is already installed")
        except ImportError:
            st.info(f"ğŸ“¦ Installing {package}...")
            success, message = install_package(package)
            if success:
                st.success(f"âœ… {message}")
            else:
                st.error(f"âŒ {message}")
                return False
    
    return True

# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’è©¦è¡Œ
if not install_required_packages_fixed():
    st.error("âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ")
    st.stop()

# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå®‰å…¨ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
def safe_import_with_retry(package_name, import_name=None, max_retries=3):
    """å®‰å…¨ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤"""
    if import_name is None:
        import_name = package_name.replace('-', '_')
    
    for attempt in range(max_retries):
        try:
            module = importlib.import_module(import_name)
            print(f"âœ… {package_name} imported successfully")
            return module
        except ImportError as e:
            if attempt < max_retries - 1:
                print(f"âš ï¸ {package_name} import failed, retrying... ({attempt + 1}/{max_retries})")
                time.sleep(1)
                importlib.invalidate_caches()
            else:
                st.error(f"âŒ {package_name}ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return None

# å„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å®‰å…¨ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    sounddevice = safe_import_with_retry('sounddevice', 'sd')
    if sounddevice is None:
        st.error("âŒ sounddeviceã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
except Exception as e:
    st.error(f"âŒ sounddeviceã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)

try:
    faster_whisper = safe_import_with_retry('faster-whisper', 'faster_whisper')
    if faster_whisper is None:
        st.error("âŒ faster-whisperã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
except Exception as e:
    st.error(f"âŒ faster-whisperã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)

try:
    torch = safe_import_with_retry('torch', 'torch')
    if torch is None:
        st.error("âŒ torchã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
except Exception as e:
    st.error(f"âŒ torchã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)

try:
    torchaudio = safe_import_with_retry('torchaudio', 'torchaudio')
    if torchaudio is None:
        st.error("âŒ torchaudioã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
except Exception as e:
    st.error(f"âŒ torchaudioã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)

# è¨­å®š
class Config:
    MAIN_MODEL = "llama3.2"
    WHISPER_MODEL = "large-v3"
    AUDIO_SAMPLE_RATE = 16000
    AUDIO_CHANNELS = 1
    AUDIO_FORMAT = "int16"
    
    # ã‚¹ãƒãƒ¼ãƒˆãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°è¨­å®š
    VAD_SILENCE_THRESHOLD = 0.5
    MIN_SPEECH_DURATION = 2.0  # æœ€å°ç™ºè©±æ™‚é–“ï¼ˆç§’ï¼‰
    MAX_PAUSE_DURATION = 2.0   # æœ€å¤§ä¼‘æ­¢æ™‚é–“ï¼ˆç§’ï¼‰
    BUFFER_TIMEOUT = 5.0       # ãƒãƒƒãƒ•ã‚¡ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
    
    # UIè¨­å®š
    NODDING_INTERVAL = 1.0  # ç›¸æ§Œé–“éš”ï¼ˆç§’ï¼‰

class AIAgent:
    """AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - requestsä½¿ç”¨ç‰ˆ"""
    
    def __init__(self):
        self.base_url = "http://localhost:11434"
        self.timeout = 30
    
    def generate_response(self, prompt, model="llama3.2"):
        """AIå¿œç­”ã‚’ç”Ÿæˆï¼ˆrequestsä½¿ç”¨ï¼‰"""
        try:
            data = {
                "model": model,
                "prompt": prompt,
                "stream": False
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=data,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                return f"âŒ å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}"
                
        except Exception as e:
            return f"âŒ å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"

class SmartVoiceBuffer:
    """ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.is_active = False
        self.audio_buffer = []
        self.speech_segments = []
        self.last_speech_end = None
        self.current_segment_start = None
        self.total_duration = 0.0
        
    def start_segment(self):
        """éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–‹å§‹"""
        self.current_segment_start = time.time()
        
    def end_segment(self):
        """éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ‚äº†"""
        if self.current_segment_start:
            duration = time.time() - self.current_segment_start
            if duration >= Config.MIN_SPEECH_DURATION:
                self.speech_segments.append({
                    "start": self.current_segment_start,
                    "end": time.time(),
                    "duration": duration,
                    "audio_data": self.audio_buffer.copy()
                })
                self.total_duration += duration
            
            self.last_speech_end = time.time()
            self.current_segment_start = None
            self.audio_buffer = []
    
    def add_audio_data(self, audio_data):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        self.audio_buffer.extend(audio_data)
    
    def should_process_speech(self):
        """éŸ³å£°å‡¦ç†ã™ã¹ãã‹åˆ¤å®š"""
        if not self.speech_segments:
            return False
        
        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ‚äº†ã‹ã‚‰æ™‚é–“ã‚’ãƒã‚§ãƒƒã‚¯
        if self.last_speech_end:
            time_since_last_speech = time.time() - self.last_speech_end
            return time_since_last_speech >= Config.MAX_PAUSE_DURATION
        
        return False
    
    def get_combined_audio(self):
        """çµåˆã•ã‚ŒãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if not self.speech_segments:
            return None
        
        # å…¨ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
        combined_audio = []
        for segment in self.speech_segments:
            combined_audio.extend(segment["audio_data"])
        
        return combined_audio
    
    def get_speech_info(self):
        """éŸ³å£°æƒ…å ±ã‚’å–å¾—"""
        if not self.speech_segments:
            return None
        
        return {
            "segments_count": len(self.speech_segments),
            "total_duration": self.total_duration,
            "first_segment": self.speech_segments[0] if self.speech_segments else None,
            "last_segment": self.speech_segments[-1] if self.speech_segments else None
        }
    
    def reset(self):
        """ãƒãƒƒãƒ•ã‚¡ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.speech_segments = []
        self.audio_buffer = []
        self.last_speech_end = None
        self.current_segment_start = None
        self.total_duration = 0.0

class SmartVoiceInputHandler:
    """ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°å…¥åŠ›ãƒãƒ³ãƒ‰ãƒ©"""
    
    def __init__(self):
        self.is_recording = False
        self.audio_queue = queue.Queue()
        self.whisper_model = None
        self.vad_model = None
        self.voice_buffer = SmartVoiceBuffer()
        self.processing_thread = None
        self.last_nodding_time = 0
        
    def initialize(self):
        """éŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            self.whisper_model = faster_whisper.WhisperModel(
                Config.WHISPER_MODEL,
                device="cuda" if self._check_cuda() else "cpu",
                compute_type="float32"
            )
            
            # VADãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            try:
                self.vad_model, utils = torch.hub.load(
                    'snakers4/silero-vad',
                    'silero_vad',
                    force_reload=True
                )
                self.vad_utils = utils
            except Exception as vad_error:
                st.warning(f"âš ï¸ VADãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {vad_error}")
                self.vad_model = None
            
            return True
        except Exception as e:
            st.error(f"âŒ éŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _check_cuda(self):
        """CUDAåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            return torch.cuda.is_available()
        except:
            return False
    
    def start_recording(self):
        """éŒ²éŸ³é–‹å§‹"""
        if self.is_recording:
            return False
        
        self.is_recording = True
        self.voice_buffer.reset()
        
        # éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        def audio_callback(indata, frame_count, time_info, status):
            if status:
                st.error(f"âŒ éŸ³å£°å…¥åŠ›ã‚¨ãƒ©ãƒ¼: {status}")
            self.audio_queue.put(indata.copy())
        
        try:
            self.processing_thread = threading.Thread(
                target=self._smart_record_audio,
                args=(audio_callback,),
                daemon=True
            )
            self.processing_thread.start()
            return True
        except Exception as e:
            st.error(f"âŒ éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.is_recording = False
            return False
    
    def _smart_record_audio(self, callback):
        """ã‚¹ãƒãƒ¼ãƒˆéŒ²éŸ³å‡¦ç†"""
        try:
            with sounddevice.InputStream(
                samplerate=Config.AUDIO_SAMPLE_RATE,
                channels=Config.AUDIO_CHANNELS,
                dtype=Config.AUDIO_FORMAT,
                blocksize=1024,
                callback=callback
            ) as stream:
                while self.is_recording:
                    try:
                        audio_data = self.audio_queue.get(timeout=1.0)
                        
                        if audio_data is not None and len(audio_data) > 0:
                            # VADã§éŸ³å£°æ´»å‹•æ¤œå‡º
                            if self.vad_model is not None:
                                audio_tensor = torch.from_numpy(np.array(audio_data, dtype=np.float32))
                                speech_prob = self.vad_model(audio_tensor, Config.AUDIO_SAMPLE_RATE).item()
                            else:
                                # ç°¡æ˜“çš„ãªéŸ³å£°æ¤œå‡º
                                audio_array = np.array(audio_data)
                                energy = np.sqrt(np.mean(audio_array**2))
                                speech_prob = 1.0 if energy > 0.01 else 0.0
                            
                            if speech_prob > Config.VAD_SILENCE_THRESHOLD:
                                if not self.voice_buffer.current_segment_start:
                                    self.voice_buffer.start_segment()
                                
                                self.voice_buffer.add_audio_data(audio_data)
                            else:
                                if self.voice_buffer.current_segment_start:
                                    self.voice_buffer.end_segment()
                                
                                if self.voice_buffer.should_process_speech():
                                    self._process_buffered_speech()
                    
                    except queue.Empty:
                        continue
                    except Exception as e:
                        st.error(f"âŒ éŸ³å£°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        continue
                        
        except Exception as e:
            st.error(f"âŒ éŒ²éŸ³ã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.is_recording = False
    
    def _process_buffered_speech(self):
        """ãƒãƒƒãƒ•ã‚¡ã•ã‚ŒãŸéŸ³å£°ã‚’å‡¦ç†"""
        try:
            combined_audio = self.voice_buffer.get_combined_audio()
            speech_info = self.voice_buffer.get_speech_info()
            
            if combined_audio and speech_info:
                # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’WAVãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›
                with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                    with wave.open(tmp_file.name, 'wb') as wf:
                        wf.setnchannels(Config.AUDIO_CHANNELS)
                        wf.setsampwidth(2)  # 16-bit
                        wf.setframerate(Config.AUDIO_SAMPLE_RATE)
                        wf.writeframes(combined_audio)
                        wf.close()
                    
                    # Whisperã§è»¢è¨˜
                    result = self.whisper_model.transcribe(
                        tmp_file.name,
                        language="ja",
                        word_timestamps=True,
                        temperature=0.0,
                        beam_size=5
                    )
                    
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    os.unlink(tmp_file.name)
                    
                    # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    st.session_state.last_transcription = result
                    st.session_state.speech_info = speech_info
                    
                    # ãƒãƒƒãƒ•ã‚¡ã‚’ãƒªã‚»ãƒƒãƒˆ
                    self.voice_buffer.reset()
                    
                    return result
            
        except Exception as e:
            st.error(f"âŒ ãƒãƒƒãƒ•ã‚¡å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def stop_recording(self):
        """éŒ²éŸ³åœæ­¢"""
        if not self.is_recording:
            return False
        
        self.is_recording = False
        
        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†
        if self.voice_buffer.current_segment_start:
            self.voice_buffer.end_segment()
        
        # æ®‹ã‚Šã®ãƒãƒƒãƒ•ã‚¡ã‚’å‡¦ç†
        if self.voice_buffer.speech_segments:
            self._process_buffered_speech()
        
        if self.processing_thread:
            self.processing_thread.join(timeout=5)
            self.processing_thread = None
        
        return True
    
    def get_status(self):
        """ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        return {
            "is_recording": self.is_recording,
            "buffer_segments": len(self.voice_buffer.speech_segments),
            "total_duration": self.voice_buffer.total_duration,
            "last_speech_end": self.voice_buffer.last_speech_end
        }

class SmartVoiceAIAgent:
    """ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.ai_agent = AIAgent()
        self.voice_input = SmartVoiceInputHandler()
        self.current_conversation = []
        
    def initialize(self):
        """AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–"""
        try:
            # éŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            if not self.voice_input.initialize():
                return False
            
            return True
        except Exception as e:
            return False
    
    def process_voice_input(self):
        """éŸ³å£°å…¥åŠ›å‡¦ç†"""
        try:
            if self.voice_input.is_recording:
                status = self.voice_input.get_status()
                
                # å¾…æ©ŸçŠ¶æ…‹ã®è¡¨ç¤º
                if status["last_speech_end"]:
                    time_since_last_speech = time.time() - status["last_speech_end"]
                    if time_since_last_speech < Config.MAX_PAUSE_DURATION:
                        return "ã¾ã èã„ã¦ã‚‹ã‚ˆ...ç¶šãã‚’å¾…æ©Ÿä¸­ã§ã™ã€‚"
                    else:
                        return "ã¾ã èã„ã¦ã‚‹ã‚ˆ...æ–°ã—ã„ç™ºè©±ã‚’å¾…ã£ã¦ã„ã¾ã™ã€‚"
                else:
                    return "ã¾ã èã„ã¦ã‚‹ã‚ˆ...è©±ã—ã¦ãã ã•ã„ã€‚"
            
            return None
            
        except Exception as e:
            return f"âŒ éŸ³å£°å…¥åŠ›å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def generate_response(self, transcription_text):
        """AIå¿œç­”ç”Ÿæˆ"""
        try:
            if not transcription_text:
                return "éŸ³å£°ãŒèªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            
            # llama3.2ã§å¿œç­”ç”Ÿæˆ
            prompt = f"""ã‚ãªãŸã¯ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°å…¥åŠ›ã«åŸºã¥ã„ã¦ã€è‡ªç„¶ã§ä¸å¯§ãªå¿œç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°å…¥åŠ›: {transcription_text}

éŸ³å£°ã®ç‰¹å¾´:
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(self.voice_input.voice_buffer.speech_segments)}
- ç·æ™‚é–“: {self.voice_input.voice_buffer.total_duration:.1f}ç§’

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒšãƒ¼ã‚¹ã‚’å°Šé‡ã—ã€é©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚è‡ªç„¶ãªå¯¾è©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"""
            
            response = self.ai_agent.generate_response(prompt)
            
            return response
            
        except Exception as e:
            return f"âŒ å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"

def render_smart_voice_interface(ai_agent):
    """ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ¤ï¸ ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ  - PyTorchä¿®æ­£ç‰ˆ")
    
    # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
    input_method = st.radio(
        "ğŸ¯ å…¥åŠ›æ–¹æ³•ã‚’é¸æŠ",
        ["ğŸ¤ï¸ éŸ³å£°å…¥åŠ›", "âŒ¨ï¸ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›"],
        horizontal=True
    )
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        if input_method == "ğŸ¤ï¸ éŸ³å£°å…¥åŠ›":
            st.subheader("ğŸ¤ï¸ éŸ³å£°éŒ²éŸ³")
            
            # éŒ²éŸ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ¤ï¸ éŒ²éŸ³é–‹å§‹", key="start_smart_recording"):
                    if ai_agent.voice_input.start_recording():
                        st.success("âœ… éŒ²éŸ³é–‹å§‹")
                        st.session_state.recording_status = "recording"
                    else:
                        st.error("âŒ éŒ²éŸ³é–‹å§‹å¤±æ•—")
            
            with col2:
                if st.button("â¹ï¸ éŒ²éŸ³åœæ­¢", key="stop_smart_recording"):
                    if ai_agent.voice_input.stop_recording():
                        st.success("âœ… éŒ²éŸ³åœæ­¢")
                        st.session_state.recording_status = "stopped"
                    else:
                        st.error("âŒ éŒ²éŸ³åœæ­¢å¤±æ•—")
            
            # éŒ²éŸ³çŠ¶æ…‹è¡¨ç¤º
            if st.session_state.get("recording_status") == "recording":
                st.info("ğŸ”´ éŒ²éŸ³ä¸­...")
                
                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
                status = ai_agent.voice_input.get_status()
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°", status["buffer_segments"])
                
                with col2:
                    st.metric("ç·æ™‚é–“", f"{status['total_duration']:.1f}ç§’")
                
                with col3:
                    if status["last_speech_end"]:
                        time_since = time.time() - status["last_speech_end"]
                        st.metric("å‰å›ç™ºè©±ã‹ã‚‰", f"{time_since:.1f}ç§’å‰")
                    else:
                        st.metric("çŠ¶æ…‹", "ç™ºè©±ä¸­")
            
            # è»¢è¨˜çµæœè¡¨ç¤º
            if st.session_state.get("last_transcription"):
                transcription = st.session_state.last_transcription
                speech_info = st.session_state.get("speech_info", {})
                
                st.subheader("ğŸ“ éŸ³å£°è»¢è¨˜çµæœ")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write(f"**èªè­˜ãƒ†ã‚­ã‚¹ãƒˆ**: {transcription['text']}")
                    st.write(f"**å‡¦ç†æ™‚é–“**: {transcription.get('time', 'N/A')}ç§’")
                    
                    if speech_info:
                        st.write(f"**ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°**: {speech_info['segments_count']}")
                        st.write(f"**ç·æ™‚é–“**: {speech_info['total_duration']:.1f}ç§’")
                
                with col2:
                    # AIå¿œç­”ç”Ÿæˆ
                    if st.button("ğŸ¤– AIå¿œç­”ç”Ÿæˆ", key="generate_ai_response_voice"):
                        with st.spinner("ğŸ¤– AIå¿œç­”ç”Ÿæˆä¸­..."):
                            ai_response = ai_agent.generate_response(transcription['text'])
                            st.session_state.ai_response = ai_response
                            st.success("âœ… AIå¿œç­”ç”Ÿæˆå®Œäº†")
                    
                    # AIå¿œç­”è¡¨ç¤º
                    if st.session_state.get("ai_response"):
                        st.subheader("ğŸ¤– AIå¿œç­”")
                        st.write(st.session_state.ai_response)
        
        elif input_method == "âŒ¨ï¸ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›":
            st.subheader("âŒ¨ï¸ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›")
            
            # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›æ¬„
            user_input = st.text_area(
                "ğŸ’¬ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
                key="text_input",
                height=100,
                placeholder="ã“ã“ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›..."
            )
            
            # å…¥åŠ›ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ“¤ é€ä¿¡", key="send_text", type="primary"):
                    if user_input.strip():
                        with st.spinner("ğŸ¤– AIå¿œç­”ç”Ÿæˆä¸­..."):
                            ai_response = ai_agent.generate_response(user_input)
                            st.session_state.text_ai_response = ai_response
                            st.session_state.last_text_input = user_input
                            st.success("âœ… AIå¿œç­”ç”Ÿæˆå®Œäº†")
                    else:
                        st.warning("âš ï¸ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            
            with col2:
                if st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢", key="clear_text"):
                    st.session_state.text_input = ""
                    st.session_state.text_ai_response = ""
                    st.success("âœ… å…¥åŠ›ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
            
            # ãƒ†ã‚­ã‚¹ãƒˆAIå¿œç­”è¡¨ç¤º
            if st.session_state.get("text_ai_response"):
                st.subheader("ğŸ¤– AIå¿œç­”")
                st.write(st.session_state.text_ai_response)
    
    with col2:
        st.subheader("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        
        # PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
        st.write("**PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±**:")
        st.write(f"- torch: {torch.__version__}")
        st.write(f"- torchaudio: {torchaudio.__version__}")
        
        # CUDAæƒ…å ±
        if torch.cuda.is_available():
            st.write(f"- CUDA: åˆ©ç”¨å¯èƒ½")
            st.write(f"- GPUæ•°: {torch.cuda.device_count()}")
        else:
            st.write("- CUDA: åˆ©ç”¨ä¸å¯")
        
        # ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°è¨­å®š
        st.write("**ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°è¨­å®š**:")
        st.write(f"- æœ€å°ç™ºè©±æ™‚é–“: {Config.MIN_SPEECH_DURATION}ç§’")
        st.write(f"- æœ€å¤§ä¼‘æ­¢æ™‚é–“: {Config.MAX_PAUSE_DURATION}ç§’")
        st.write(f"- VADé–¾å€¤: {Config.VAD_SILENCE_THRESHOLD}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    st.set_page_config(
        page_title="ğŸ¤ï¸ Fixed Smart Voice AI Agent v2",
        page_icon="ğŸ¤ï¸",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ¤ï¸ Fixed Smart Voice AI Agent v2")
    st.markdown("### PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³ç«¶åˆä¿®æ­£ç‰ˆ - ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ ")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹åˆæœŸåŒ–
    if 'agent' not in st.session_state:
        st.session_state.agent = SmartVoiceAIAgent()
        
        # AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
        with st.spinner("ğŸ¤– AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–ä¸­..."):
            if st.session_state.agent.initialize():
                st.success("âœ… AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
            else:
                st.error("âŒ AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•—")
                st.stop()
    
    # ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    render_smart_voice_interface(st.session_state.agent)

if __name__ == "__main__":
    main()
