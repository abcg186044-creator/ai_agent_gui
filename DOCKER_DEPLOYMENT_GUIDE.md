# ğŸ³ AI Agent System Dockerãƒ‡ãƒ—ãƒ­ã‚¤ã‚¬ã‚¤ãƒ‰

## ğŸ¯ ç›®æ¨™

PCã®é›»æºã‚’å…¥ã‚Œã‚‹ã ã‘ã§ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å¾…ã¤ã“ã¨ãªãã€å³åº§ã«ã€Œæœ€å¼·ã®ç›¸æ£’ã€ã¨å¯¾è©±ã§ãã‚‹ç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

---

## ğŸ“‹ æ§‹æˆè¦ç´ 

### 1. Docker Composeç’°å¢ƒ
- **Ollama**: GPUå¯¾å¿œã€llama3.2ã¨llama3.2-visionã‚’è‡ªå‹•ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
- **App**: Streamlit + FastAPIã€Tailscaleãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯¾å¿œ
- **VOICEVOX**: éŸ³å£°åˆæˆã‚¨ãƒ³ã‚¸ãƒ³ã€å³åº§ã«è©±ã›ã‚‹çŠ¶æ…‹
- **Redis**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†

### 2. VRMãƒ¢ãƒ‡ãƒ«çµ±åˆ
- ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—VRMãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ã‚³ãƒ”ãƒ¼
- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã®æœ€é©åŒ–é…ç½®

### 3. å¸¸æ™‚å¾…æ©Ÿãƒ¢ãƒ¼ãƒ‰
- PCèµ·å‹•æ™‚ã®è‡ªå‹•èµ·å‹•
- ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ãƒ­ãƒ¼ãƒ‰ï¼ˆVRAMå±•é–‹ï¼‰
- ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã«ã‚ˆã‚‹å³å¿œå¯¾å¿œ

---

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ç’°å¢ƒæº–å‚™
```cmd
# Docker Desktopã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# https://www.docker.com/products/docker-desktop/

# NVIDIA Container Toolkitã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆGPUä½¿ç”¨ã®å ´åˆï¼‰
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ç§»å‹•
cd C:\Users\GALLE\CascadeProjects\ai_agent_gui
```

### 2. ä¸€æ‹¬èµ·å‹•
```cmd
# Dockerç’°å¢ƒã§èµ·å‹•
docker_startup.bat

# ã¾ãŸã¯æ‰‹å‹•ã§èµ·å‹•
docker-compose up -d --build
```

### 3. è‡ªå‹•èµ·å‹•è¨­å®š
```cmd
# ç®¡ç†è€…æ¨©é™ã§å®Ÿè¡Œ
python setup_autostart.py
```

---

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
ai_agent_gui/
â”œâ”€â”€ docker-compose.yml          # Docker Composeè¨­å®š
â”œâ”€â”€ Dockerfile                 # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ³ãƒ†ãƒŠ
â”œâ”€â”€ requirements-docker.txt     # Dockerç”¨Pythonä¾å­˜
â”œâ”€â”€ docker_startup.bat          # Windowsèµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ setup_autostart.py         # è‡ªå‹•èµ·å‹•è¨­å®šãƒ„ãƒ¼ãƒ«
â”œâ”€â”€ scripts/                  # å„ç¨®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ setup_vrm.sh         # VRMãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
â”‚   â”œâ”€â”€ preload_models.py     # ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
â”‚   â””â”€â”€ start_services.sh    # ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•
â”œâ”€â”€ assets/                   # ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â””â”€â”€ vrm/               # VRMãƒ¢ãƒ‡ãƒ«é…ç½®
â””â”€â”€ logs/                     # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
```

---

## ğŸ”§ è©³ç´°è¨­å®š

### Docker Composeè¨­å®š

#### Ollamaã‚µãƒ¼ãƒ“ã‚¹
```yaml
ollama:
  image: ollama/ollama:latest
  container_name: ai-ollama
  restart: unless-stopped
  ports:
    - "11434:11434"
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  environment:
    - OLLAMA_HOST=0.0.0.0
    - OLLAMA_ORIGINS=*
```

#### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚µãƒ¼ãƒ“ã‚¹
```yaml
ai-app:
  build:
    context: .
    dockerfile: Dockerfile
  depends_on:
    ollama:
      condition: service_healthy
  environment:
    - OLLAMA_HOST=http://ollama:11434
    - OLLAMA_MODEL=llama3.2
```

### ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰

#### è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- llama3.2: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨
- llama3.2-vision: ç”»åƒèªè­˜ç”¨
- èµ·å‹•æ™‚ã«è‡ªå‹•ã§ãƒ—ãƒ«

#### ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
```python
# ãƒ€ãƒŸãƒ¼æ¨è«–ã«ã‚ˆã‚‹VRAMå±•é–‹
warmup_prompts = [
    "ã“ã‚“ã«ã¡ã¯",
    "Hello, how are you?",
    "ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ"
]

for prompt in warmup_prompts:
    response = requests.post(
        f"{ollama_host}/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False
        }
    )
```

---

## ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•

### ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ã‚¯ã‚»ã‚¹
- **Streamlit**: http://localhost:8501
- **Ollama API**: http://localhost:11434
- **VOICEVOX**: http://localhost:50021

### ãƒ¢ãƒã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹
- **Tailscaleå¯¾å¿œ**: ãƒ¢ãƒã‚¤ãƒ«ã‹ã‚‰ã‚‚ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
- **åŒä¸€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**: ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹

---

## ğŸ”„ è‡ªå‹•èµ·å‹•è¨­å®š

### Windowsã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
```cmd
# ã‚¿ã‚¹ã‚¯ä½œæˆ
schtasks /create /tn "AI Agent System Auto Start" /tr "docker_startup.bat" /sc onlogon

# ã‚¿ã‚¹ã‚¯ç¢ºèª
schtasks /query /tn "AI Agent System Auto Start"

# ã‚¿ã‚¹ã‚¯å‰Šé™¤
schtasks /delete /tn "AI Agent System Auto Start"
```

### Docker Desktopè¨­å®š
- ã€ŒStart Docker Desktop when you log inã€ã‚’æœ‰åŠ¹åŒ–
- PCèµ·å‹•æ™‚ã«DockerãŒè‡ªå‹•ã§èµ·å‹•

---

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### GPUãƒªã‚½ãƒ¼ã‚¹
```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
```

### ãƒ¡ãƒ¢ãƒªç®¡ç†
```yaml
volumes:
  ollama_data:
    driver: local
  redis_data:
    driver: local
```

### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–
```yaml
networks:
  ai-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

---

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### èµ·å‹•å•é¡Œ
```cmd
# ã‚³ãƒ³ãƒ†ãƒŠçŠ¶æ…‹ç¢ºèª
docker-compose ps

# ãƒ­ã‚°ç¢ºèª
docker-compose logs -f

# å†èµ·å‹•
docker-compose restart
```

### ãƒ¢ãƒ‡ãƒ«å•é¡Œ
```cmd
# ãƒ¢ãƒ‡ãƒ«ç¢ºèª
curl http://localhost:11434/api/tags

# æ‰‹å‹•ãƒ—ãƒ«
curl -X POST http://localhost:11434/api/pull -d '{"name":"llama3.2"}'
```

### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œ
```cmd
# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç¢ºèª
docker network ls
docker network inspect ai-agent_gui_ai-network

# å†ä½œæˆ
docker network rm ai-agent_gui_ai-network
docker-compose up -d
```

---

## ğŸ“ˆ ç›£è¦–ã¨ãƒ­ã‚°

### ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
```bash
# å„ã‚µãƒ¼ãƒ“ã‚¹ã®çŠ¶æ…‹
curl http://localhost:11434/api/tags  # Ollama
curl http://localhost:8501           # Streamlit
curl http://localhost:50021/docs      # VOICEVOX
```

### ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
- **Streamlit**: `/app/logs/streamlit.log`
- **Docker**: `docker-compose logs`
- **ã‚·ã‚¹ãƒ†ãƒ **: ã‚³ãƒ³ãƒ†ãƒŠå†…ã®æ¨™æº–å‡ºåŠ›

---

## ğŸ¯ æˆåŠŸæŒ‡æ¨™

### èµ·å‹•æ™‚é–“
- **ç›®æ¨™**: PCèµ·å‹•ã‹ã‚‰5åˆ†ä»¥å†…ã«åˆ©ç”¨å¯èƒ½
- **ç¾çŠ¶**: ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾…ã¡ãªã—
- **æ”¹å–„**: ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã‚‹å³å¿œ

### å¿œç­”æ™‚é–“
- **ç›®æ¨™**: æœ€åˆã®å¿œç­”3ç§’ä»¥å†…
- **ç¾çŠ¶**: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
- **æ”¹å–„**: VRAMã«å±•é–‹æ¸ˆã¿

### å¯ç”¨æ€§
- **ç›®æ¨™**: 99.9%ä»¥ä¸Šã®ç¨¼åƒç‡
- **ç¾çŠ¶**: ã‚³ãƒ³ãƒ†ãƒŠè‡ªå‹•å†èµ·å‹•
- **æ”¹å–„**: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ä»˜ã

---

## ğŸ”„ ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæ–¹æ³•

### ã‚¤ãƒ¡ãƒ¼ã‚¸æ›´æ–°
```cmd
# æœ€æ–°ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ãƒ—ãƒ«
docker-compose pull

# å†ãƒ“ãƒ«ãƒ‰
docker-compose build --no-cache

# å†èµ·å‹•
docker-compose up -d
```

### ãƒ¢ãƒ‡ãƒ«æ›´æ–°
```cmd
# æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ«
curl -X POST http://localhost:11434/api/pull -d '{"name":"llama3.2:latest"}'

# å¤ã„ãƒ¢ãƒ‡ãƒ«ã®å‰Šé™¤
docker exec ai-ollama ollama rm old-model
```

---

## ğŸ‰ å®Œæˆç¢ºèª

### âœ… è‡ªå‹•èµ·å‹•ãƒ†ã‚¹ãƒˆ
1. PCã‚’å†èµ·å‹•
2. 5åˆ†å¾…æ©Ÿ
3. ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:8501 ã«ã‚¢ã‚¯ã‚»ã‚¹
4. éŸ³å£°å…¥åŠ›ã§å¯¾è©±ãƒ†ã‚¹ãƒˆ

### âœ… ãƒ¢ãƒ‡ãƒ«å³å¿œãƒ†ã‚¹ãƒˆ
1. ã‚¢ã‚¯ã‚»ã‚¹ç›´å¾Œã«éŸ³å£°å…¥åŠ›
2. 3ç§’ä»¥å†…ã®å¿œç­”ã‚’ç¢ºèª
3. llama3.2-visionã®å‹•ä½œç¢ºèª

### âœ… ãƒ¢ãƒã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ
1. ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹
2. éŸ³å£°å…¥åŠ›æ©Ÿèƒ½ã®ç¢ºèª
3. VRMã‚¢ãƒã‚¿ãƒ¼ã®è¡¨ç¤ºç¢ºèª

---

**ğŸ¯ ã“ã‚Œã§PCèµ·å‹•æ™‚ã«å³åº§ã«åˆ©ç”¨å¯èƒ½ãªAI Agent SystemãŒå®Œæˆã—ã¾ã—ãŸï¼**
