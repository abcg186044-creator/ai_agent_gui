#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼·åŒ–ç‰ˆAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆé˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ 
åˆ†å‰²å‡¦ç†ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼å‰²ã‚Šè¾¼ã¿æ©Ÿèƒ½ã‚’å®Ÿè£…
"""

import sys
import json
import datetime
import os
import time
import threading
from pathlib import Path
from flask import Flask, request, jsonify, render_template_string
import queue

# ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ollama_vrm_integrated_app import OllamaClient

class EnhancedTimeoutResponder:
    def __init__(self):
        self.ollama_client = OllamaClient()
        self.response_queue = queue.Queue()
        self.progress_queue = queue.Queue()
        self.active_tasks = {}
        self.task_counter = 0
        self.timeout_threshold = 120  # 120ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆå»¶é•·ï¼‰
        self.progress_interval = 3  # 3ç§’ã”ã¨ã«é€²æ—å ±å‘Š
        
        # å‰²ã‚Šè¾¼ã¿æ©Ÿèƒ½
        self.interruptible_tasks = {}
        self.user_interrupts = {}
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ
        self.data_dir = Path("data")
        self.responses_file = self.data_dir / "enhanced_timeout_responses.json"
        self.progress_file = self.data_dir / "enhanced_progress_reports.json"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self.data_dir.mkdir(exist_ok=True)
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        self.load_responses()
        self.load_progress()
        
        # Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
        self.app = Flask(__name__)
        
        # ã‚µãƒ¼ãƒãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰
        self.server_thread = None
        
        print("ğŸ›¡ï¸ å¼·åŒ–ç‰ˆAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆé˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 70)
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ: {self.data_dir}")
        print(f"â±ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆé–¾å€¤: {self.timeout_threshold}ç§’ï¼ˆå»¶é•·ï¼‰")
        print(f"ğŸ“ˆ é€²æ—å ±å‘Šé–“éš”: {self.progress_interval}ç§’ï¼ˆé«˜é »åº¦ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        print(f"âš¡ ãƒ¦ãƒ¼ã‚¶ãƒ¼å‰²ã‚Šè¾¼ã¿æ©Ÿèƒ½: æœ‰åŠ¹")
        print("=" * 70)
    
    def load_responses(self):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            if self.responses_file.exists():
                with open(self.responses_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.response_queue.queue = data.get('responses', [])
                print(f"ğŸ“š ãƒ¬ã‚¹ãƒãƒ³ã‚¹å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(self.response_queue.queue)}ä»¶)")
        except Exception as e:
            print(f"âŒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹å±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def load_progress(self):
        """é€²æ—å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            if self.progress_file.exists():
                with open(self.progress_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.progress_queue.queue = data.get('progress', [])
                print(f"ğŸ“š é€²æ—å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(self.progress_queue.queue)}ä»¶)")
        except Exception as e:
            print(f"âŒ é€²æ—å±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def split_task_into_subtasks(self, prompt, task_description):
        """ã‚¿ã‚¹ã‚¯ã‚’åˆ†å‰²å‡¦ç†ç”¨ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£"""
        if "GUI" in task_description or "é›»å“" in task_description:
            return [
                {"name": "ç’°å¢ƒè¨­å®šã¨è¦ä»¶åˆ†æ", "prompt": "Python GUIé–‹ç™ºç’°å¢ƒè¨­å®šã¨é›»å“è¦ä»¶åˆ†æ", "time": 15},
                {"name": "åŸºæœ¬è¨­è¨ˆã¨æ§‹é€ ", "prompt": "é›»å“ã‚¢ãƒ—ãƒªã®åŸºæœ¬è¨­è¨ˆã¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ§‹é€ ", "time": 15},
                {"name": "UIå®Ÿè£…", "prompt": "Tkinterã§ã®ãƒœã‚¿ãƒ³é…ç½®ã¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå®Ÿè£…", "time": 20},
                {"name": "è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯", "prompt": "å››å‰‡æ¼”ç®—ãƒ­ã‚¸ãƒƒã‚¯ã¨ã‚¨ãƒ©ãƒ¼å‡¦ç†å®Ÿè£…", "time": 20},
                {"name": "ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†", "prompt": "ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚¤ãƒ™ãƒ³ãƒˆã¨å®Œæˆå½¢", "time": 20}
            ]
        elif "æ©Ÿæ¢°å­¦ç¿’" in task_description:
            return [
                {"name": "ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†", "prompt": "æ©Ÿæ¢°å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨åˆ†æ", "time": 20},
                {"name": "ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ", "prompt": "ãƒ¢ãƒ‡ãƒ«é¸å®šã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ", "time": 20},
                {"name": "å®Ÿè£…ã¨è¨“ç·´", "prompt": "TensorFlow/PyTorchå®Ÿè£…ã¨è¨“ç·´", "time": 25},
                {"name": "è©•ä¾¡ã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°", "prompt": "ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´", "time": 20},
                {"name": "ãƒ‡ãƒ—ãƒ­ã‚¤", "prompt": "æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ã¨é‹ç”¨", "time": 20}
            ]
        else:
            return [
                {"name": "è¦ä»¶åˆ†æ", "prompt": f"ã‚¿ã‚¹ã‚¯è¦ä»¶åˆ†æ: {task_description}", "time": 20},
                {"name": "åŸºæœ¬è¨­è¨ˆ", "prompt": f"åŸºæœ¬è¨­è¨ˆã¨æ§‹é€ : {task_description}", "time": 20},
                {"name": "å®Ÿè£…", "prompt": f"æ ¸å¿ƒæ©Ÿèƒ½å®Ÿè£…: {task_description}", "time": 25},
                {"name": "æœ€é©åŒ–", "prompt": f"æœ€é©åŒ–ã¨è¿½åŠ æ©Ÿèƒ½: {task_description}", "time": 20},
                {"name": "å®Œæˆ", "prompt": f"ãƒ†ã‚¹ãƒˆã¨å®Œæˆ: {task_description}", "time": 20}
            ]
    
    def execute_subtask(self, task_id, subtask, index, total):
        """ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ"""
        subtask_id = f"{task_id}_sub_{index}"
        
        try:
            # é€²æ—å ±å‘Š
            progress = {
                "task_id": task_id,
                "subtask": subtask["name"],
                "progress": (index / total) * 100,
                "status": f"ğŸ”€ å‡¦ç†ä¸­: {subtask['name']}"
            }
            self.progress_queue.put(progress)
            
            # APIå‘¼ã³å‡ºã—
            response = self.ollama_client.generate_response(subtask["prompt"])
            
            # å®Œäº†å ±å‘Š
            completion = {
                "task_id": task_id,
                "subtask": subtask["name"],
                "response": response,
                "progress": ((index + 1) / total) * 100,
                "status": f"âœ… å®Œäº†: {subtask['name']}"
            }
            self.response_queue.put(completion)
            
            return {"success": True, "response": response}
            
        except Exception as e:
            error = {
                "task_id": task_id,
                "subtask": subtask["name"],
                "error": str(e),
                "status": f"âŒ ã‚¨ãƒ©ãƒ¼: {subtask['name']}"
            }
            self.response_queue.put(error)
            return {"success": False, "error": str(e)}
    
    def interrupt_task(self, task_id):
        """ã‚¿ã‚¹ã‚¯ã‚’å‰²ã‚Šè¾¼ã¿"""
        self.user_interrupts[task_id] = True
        
        interrupt_msg = {
            "task_id": task_id,
            "status": "âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šå‰²ã‚Šè¾¼ã¿ã•ã‚Œã¾ã—ãŸ"
        }
        self.response_queue.put(interrupt_msg)
    
    def generate_response_with_split(self, prompt, task_description=""):
        """åˆ†å‰²å‡¦ç†ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ"""
        task_id = f"split_{self.task_counter}"
        self.task_counter += 1
        
        print(f"ğŸš€ åˆ†å‰²å‡¦ç†é–‹å§‹: {task_id}")
        
        # ã‚¿ã‚¹ã‚¯åˆ†å‰²
        subtasks = self.split_task_into_subtasks(prompt, task_description)
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
        def process_split():
            results = []
            for i, subtask in enumerate(subtasks):
                if task_id in self.user_interrupts:
                    break
                
                result = self.execute_subtask(task_id, subtask, i, len(subtasks))
                results.append(result)
                time.sleep(2)  # çŸ­ã„å¾…æ©Ÿ
            
            # æœ€çµ‚çµæœ
            final = {
                "task_id": task_id,
                "status": "ğŸ”€ åˆ†å‰²å‡¦ç†å®Œäº†",
                "results": results
            }
            self.response_queue.put(final)
        
        threading.Thread(target=process_split, daemon=True).start()
        
        return {
            "success": True,
            "task_id": task_id,
            "subtasks": len(subtasks),
            "message": f"ğŸ”€ {len(subtasks)}å€‹ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†å‰²ã—ã¦å‡¦ç†é–‹å§‹"
        }
    
    def start_server(self, host='0.0.0.0', port=8085):
        """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•"""
        @self.app.route('/')
        def index():
            return '''
<h1>ğŸ›¡ï¸ å¼·åŒ–ç‰ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆé˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ </h1>
<p>åˆ†å‰²å‡¦ç†ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼å‰²ã‚Šè¾¼ã¿æ©Ÿèƒ½ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é˜²æ­¢</p>
<ul>
<li>ğŸ”€ åˆ†å‰²å‡¦ç†: ã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•åˆ†å‰²</li>
<li>âš¡ é«˜é »åº¦å ±å‘Š: 3ç§’ã”ã¨é€²æ—</li>
<li>âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼å‰²ã‚Šè¾¼ã¿: ã„ã¤ã§ã‚‚ä¸­æ–­</li>
<li>â±ï¸ å»¶é•·ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 120ç§’</li>
</ul>
'''
        
        @self.app.route('/api/generate_split', methods=['POST'])
        def generate_split():
            data = request.get_json()
            result = self.generate_response_with_split(
                data.get('prompt', ''), 
                data.get('task_description', '')
            )
            return jsonify(result)
        
        @self.app.route('/api/interrupt', methods=['POST'])
        def interrupt():
            data = request.get_json()
            self.interrupt_task(data.get('task_id'))
            return jsonify({"success": True})
        
        def run_server():
            self.app.run(host=host, port=port, debug=False)
        
        threading.Thread(target=run_server, daemon=True).start()
        print(f"ğŸš€ å¼·åŒ–ç‰ˆã‚µãƒ¼ãƒãƒ¼èµ·å‹•: http://{host}:{port}")

def main():
    responder = EnhancedTimeoutResponder()
    responder.start_server()
    
    # ãƒ†ã‚¹ãƒˆ
    print("\nğŸ§ª å¼·åŒ–ç‰ˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
    result = responder.generate_response_with_split(
        "Pythonã§GUIé›»å“ã‚¢ãƒ—ãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„",
        "GUIé›»å“é–‹ç™º"
    )
    print(f"çµæœ: {result}")

if __name__ == "__main__":
    main()
