#!/usr/bin/env python3
"""
AI Agent System - llama3.2 å®Œå…¨ç§»è¡Œç‰ˆ
æœ€æ–°ã®llama3.2ã‚·ãƒªãƒ¼ã‚ºã‚’æ´»ç”¨ã—ãŸæœ€é©åŒ–AIã‚·ã‚¹ãƒ†ãƒ 
"""

import streamlit as st
import sys
import os
import json
import tempfile
import time
from datetime import datetime
from pathlib import Path
import threading
import queue

# åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import ollama
    import faster_whisper
    import pyttsx3
    import pyautogui
    import numpy as np
    import pandas as pd
    from openpyxl import load_workbook
    import pymupdf
    from PIL import Image
    import qrcode
    from duckduckgo_search import DDGS
    import chromadb
    from sentence_transformers import SentenceTransformer
    import faiss
    import psutil
    import schedule
    import base64
except ImportError as e:
    st.error(f"âŒ å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    st.stop()

# æœ€æ–°llama3.2è¨­å®š
class Config:
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    MAIN_MODEL = "llama3.2"           # ãƒ¡ã‚¤ãƒ³æ¨è«–ãƒ»é›‘è«‡ç”¨ï¼ˆ3b: é«˜é€Ÿã‹ã¤é«˜çŸ¥èƒ½ï¼‰
    VISION_MODEL = "llama3.2-vision"   # ç”»åƒãƒ»ç”»é¢è§£æç”¨ï¼ˆ11b: ç”»é¢ç›£è¦–æ©Ÿèƒ½ï¼‰
    EMBEDDING_MODEL = "nomic-embed-text:latest"
    
    # Ollamaè¨­å®š
    OLLAMA_HOST = "localhost"
    OLLAMA_PORT = 11434
    
    # éŸ³å£°è¨­å®š
    VOICE_RATE = 200
    VOICE_VOLUME = 0.9
    
    # é«˜é€Ÿå¿œç­”è¨­å®š
    STREAMING_ENABLED = True
    FAST_RESPONSE_TIMEOUT = 2.0  # ç§’
    MAX_TOKENS_FAST = 512
    MAX_TOKENS_FULL = 4096

class ModelRouter:
    """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ«ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.main_model = Config.MAIN_MODEL
        self.vision_model = Config.VISION_MODEL
        self.embedding_model = Config.EMBEDDING_MODEL
        
    def route_request(self, prompt, images=None, context="", fast_mode=False):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã«æŒ¯ã‚Šåˆ†ã‘ã‚‹"""
        
        # ç”»åƒãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«
        if images and len(images) > 0:
            return self.vision_model, self._prepare_vision_prompt(prompt, context)
        
        # çŸ­ã„å¿œç­”ãŒå¿…è¦ãªå ´åˆã¯é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰
        if fast_mode or self._is_fast_response_needed(prompt):
            return self.main_model, self._prepare_fast_prompt(prompt, context)
        
        # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
        return self.main_model, self._prepare_full_prompt(prompt, context)
    
    def _is_fast_response_needed(self, prompt):
        """é«˜é€Ÿå¿œç­”ãŒå¿…è¦ã‹åˆ¤å®š"""
        fast_keywords = ["ã“ã‚“ã«ã¡ã¯", "ãŠã¯ã‚ˆã†", "ã‚ã‚ŠãŒã¨ã†", "ã™ã¿ã¾ã›ã‚“", "ã¯ã„", "ã„ã„ãˆ", "ã†ã‚“", "ãã†", "ãªã‚‹ã»ã©"]
        prompt_lower = prompt.lower()
        return any(keyword in prompt_lower for keyword in fast_keywords)
    
    def _prepare_fast_prompt(self, prompt, context):
        """é«˜é€Ÿå¿œç­”ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™"""
        return f"""{context}

çŸ­ãç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚æœ€å¤§50æ–‡å­—ä»¥å†…ã§ï¼š
{prompt}"""
    
    def _prepare_full_prompt(self, prompt, context):
        """å®Œå…¨å¿œç­”ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™"""
        return f"""{context}

è©³ç´°ã«ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ï¼š
{prompt}"""
    
    def _prepare_vision_prompt(self, prompt, context):
        """ãƒ“ã‚¸ãƒ§ãƒ³ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™"""
        return f"""{context}

ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’çµ±åˆã—ã¦ç­”ãˆã¦ãã ã•ã„ï¼š
{prompt}"""

class StreamingResponse:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”å‡¦ç†"""
    
    def __init__(self, model, prompt, images=None):
        self.model = model
        self.prompt = prompt
        self.images = images
        self.response_queue = queue.Queue()
        self.is_complete = False
        
    def generate_streaming(self):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ"""
        try:
            if self.images:
                # ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                response = ollama.generate(
                    model=self.model,
                    prompt=self.prompt,
                    images=self.images,
                    options={
                        "temperature": 0.7,
                        "max_tokens": Config.MAX_TOKENS_FULL,
                        "stream": False  # ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœªå¯¾å¿œ
                    }
                )
                self.response_queue.put(response['response'])
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                response = ollama.generate(
                    model=self.model,
                    prompt=self.prompt,
                    options={
                        "temperature": 0.7,
                        "max_tokens": Config.MAX_TOKENS_FAST if self._is_fast_prompt() else Config.MAX_TOKENS_FULL,
                        "stream": False
                    }
                )
                self.response_queue.put(response['response'])
                
            self.is_complete = True
            
        except Exception as e:
            self.response_queue.put(f"âŒ å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.is_complete = True
    
    def _is_fast_prompt(self):
        """é«˜é€Ÿãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹åˆ¤å®š"""
        fast_keywords = ["ã“ã‚“ã«ã¡ã¯", "ãŠã¯ã‚ˆã†", "ã‚ã‚ŠãŒã¨ã†", "ã™ã¿ã¾ã›ã‚“", "ã¯ã„", "ã„ã„ãˆ"]
        return any(keyword in self.prompt for keyword in fast_keywords)
    
    def get_response(self):
        """å¿œç­”ã‚’å–å¾—"""
        try:
            return self.response_queue.get(timeout=30)
        except queue.Empty:
            return "å¿œç­”ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"

class StartupSelfCheck:
    """èµ·å‹•æ™‚ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯"""
    
    def __init__(self):
        self.checks = []
        
    def run_all_checks(self):
        """ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ"""
        results = {
            "models": self.check_models(),
            "dependencies": self.check_dependencies(),
            "system": self.check_system_resources(),
            "external_tools": self.check_external_tools()
        }
        
        self.checks = results
        return results
    
    def check_models(self):
        """ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯"""
        try:
            client = ollama.Client()
            models = client.list()
            model_names = [m.get('name', '') for m in models]
            
            checks = {
                "main_model": Config.MAIN_MODEL in model_names,
                "vision_model": Config.VISION_MODEL in model_names,
                "embedding_model": Config.EMBEDDING_MODEL in model_names,
                "available_models": model_names
            }
            
            return checks
            
        except Exception as e:
            return {"error": str(e)}
    
    def check_dependencies(self):
        """ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯"""
        dependencies = {
            "ollama": self._check_import("ollama"),
            "streamlit": self._check_import("streamlit"),
            "pyautogui": self._check_import("pyautogui"),
            "faster_whisper": self._check_import("faster_whisper"),
            "pyttsx3": self._check_import("pyttsx3"),
            "pillow": self._check_import("PIL"),
            "pandas": self._check_import("pandas"),
            "chromadb": self._check_import("chromadb")
        }
        return dependencies
    
    def _check_import(self, module_name):
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯"""
        try:
            __import__(module_name)
            return True
        except ImportError:
            return False
    
    def check_system_resources(self):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            return {
                "cpu_percent": cpu_percent,
                "memory_percent": memory.percent,
                "disk_free_gb": disk.free / (1024**3),
                "status": "healthy" if cpu_percent < 80 and memory.percent < 80 else "warning"
            }
        except Exception as e:
            return {"error": str(e)}
    
    def check_external_tools(self):
        """å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯"""
        tools = {
            "ollama_service": self._check_ollama_service(),
            "php": self._check_php()
        }
        return tools
    
    def _check_ollama_service(self):
        """Ollamaã‚µãƒ¼ãƒ“ã‚¹ãƒã‚§ãƒƒã‚¯"""
        try:
            client = ollama.Client()
            models = client.list()
            return len(models) > 0
        except:
            return False
    
    def _check_php(self):
        """PHPãƒã‚§ãƒƒã‚¯"""
        try:
            import subprocess
            result = subprocess.run(
                ["C:\\Program Files\\PHP\\current\\php.exe", "--version"], 
                capture_output=True, text=True, timeout=5
            )
            return result.returncode == 0
        except:
            return False

class OptimizedAISystem:
    """llama3.2æœ€é©åŒ–AIã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.ollama_client = None
        self.whisper_model = None
        self.tts_engine = None
        self.model_router = ModelRouter()
        self.startup_check = StartupSelfCheck()
        self.current_personality = "friend"
        
    def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # OllamaåˆæœŸåŒ–
            self.ollama_client = ollama.Client()
            
            # éŸ³å£°å‡¦ç†åˆæœŸåŒ–
            self.whisper_model = faster_whisper.WhisperModel("base")
            self.tts_engine = pyttsx3.init()
            self.tts_engine.setProperty('rate', str(Config.VOICE_RATE))
            self.tts_engine.setProperty('volume', str(Config.VOICE_VOLUME))
            
            # èµ·å‹•æ™‚ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
            check_results = self.startup_check.run_all_checks()
            
            return True, check_results
            
        except Exception as e:
            return False, {"error": str(e)}
    
    def generate_response(self, prompt, images=None, context="", fast_mode=False):
        """æœ€é©åŒ–ã•ã‚ŒãŸå¿œç­”ç”Ÿæˆ"""
        try:
            # ãƒ¢ãƒ‡ãƒ«æŒ¯ã‚Šåˆ†ã‘
            model, formatted_prompt = self.model_router.route_request(
                prompt, images, context, fast_mode
            )
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”
            if Config.STREAMING_ENABLED and not images:
                streaming = StreamingResponse(model, formatted_prompt, images)
                
                # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
                thread = threading.Thread(target=streaming.generate_streaming)
                thread.start()
                
                # å¿œç­”å–å¾—
                response = streaming.get_response()
                thread.join(timeout=30)
                
                return response
            else:
                # é€šå¸¸å¿œç­”
                if images:
                    response = self.ollama_client.generate(
                        model=model,
                        prompt=formatted_prompt,
                        images=images,
                        options={
                            "temperature": 0.7,
                            "max_tokens": Config.MAX_TOKENS_FULL
                        }
                    )
                else:
                    response = self.ollama_client.generate(
                        model=model,
                        prompt=formatted_prompt,
                        options={
                            "temperature": 0.7,
                            "max_tokens": Config.MAX_TOKENS_FAST if fast_mode else Config.MAX_TOKENS_FULL
                        }
                    )
                
                return response['response']
                
        except Exception as e:
            return f"âŒ å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def analyze_screen_with_vision(self, prompt="ã“ã®ç”»é¢ã«ã¤ã„ã¦è©³ç´°ã«åˆ†æã—ã¦ãã ã•ã„"):
        """ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã§ç”»é¢åˆ†æ"""
        try:
            # ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£
            screenshot = pyautogui.screenshot()
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            temp_path = f"vision_analysis_{timestamp}.png"
            screenshot.save(temp_path)
            
            # llama3.2-visionã§åˆ†æ
            response = self.ollama_client.generate(
                model=Config.VISION_MODEL,
                prompt=prompt,
                images=[temp_path],
                options={
                    "temperature": 0.7,
                    "max_tokens": Config.MAX_TOKENS_FULL
                }
            )
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            try:
                os.unlink(temp_path)
            except:
                pass
            
            return response['response']
            
        except Exception as e:
            return f"âŒ ç”»é¢åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def extract_text_from_screen(self):
        """llama3.2-visionã§OCR"""
        try:
            ocr_prompt = """ã“ã®ç”»åƒã‹ã‚‰ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
            èª­ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£ç¢ºã«ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä¿ã£ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
            ãƒœã‚¿ãƒ³ã€ãƒ©ãƒ™ãƒ«ã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼é …ç›®ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã©ã€ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã¦ãã ã•ã„ã€‚"""
            
            return self.analyze_screen_with_vision(ocr_prompt)
            
        except Exception as e:
            return f"âŒ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def text_to_speech(self, text):
        """éŸ³å£°åˆæˆ"""
        try:
            self.tts_engine.say(text)
            self.tts_engine.runAndWait()
            return True
        except Exception as e:
            st.error(f"âŒ éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

def render_startup_check(check_results):
    """èµ·å‹•æ™‚ãƒã‚§ãƒƒã‚¯çµæœè¡¨ç¤º"""
    st.header("ğŸ” èµ·å‹•æ™‚ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯
    if "models" in check_results:
        st.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯")
        models = check_results["models"]
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric(
                "ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«", 
                "âœ… OK" if models.get("main_model") else "âŒ å¤±æ•—",
                help=Config.MAIN_MODEL
            )
        with col2:
            st.metric(
                "ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«", 
                "âœ… OK" if models.get("vision_model") else "âŒ å¤±æ•—",
                help=Config.VISION_MODEL
            )
        
        if "available_models" in models:
            st.write("**åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«**:")
            for model in models["available_models"]:
                st.write(f"- {model}")
    
    # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
    if "dependencies" in check_results:
        st.subheader("ğŸ“¦ ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯")
        deps = check_results["dependencies"]
        
        for dep_name, status in deps.items():
            status_icon = "âœ…" if status else "âŒ"
            st.write(f"{status_icon} {dep_name}")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯
    if "system" in check_results:
        st.subheader("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹")
        sys_info = check_results["system"]
        
        if "error" not in sys_info:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("CPUä½¿ç”¨ç‡", f"{sys_info['cpu_percent']:.1f}%")
            with col2:
                st.metric("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡", f"{sys_info['memory_percent']:.1f}%")
            with col3:
                st.metric("ç©ºãå®¹é‡", f"{sys_info['disk_free_gb']:.1f}GB")
            
            status_color = "ğŸŸ¢" if sys_info["status"] == "healthy" else "ğŸŸ¡"
            st.write(f"**ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹**: {status_color} {sys_info['status']}")

def render_main_interface(ai_system):
    """ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ’¬ llama3.2 AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    
    # ä¼šè©±å±¥æ­´
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    for i, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"], avatar="ğŸ¤–" if message["role"] == "assistant" else "ğŸ‘¤"):
            st.markdown(message["content"])
    
    # å…¥åŠ›ã‚¨ãƒªã‚¢
    col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
    
    with col1:
        user_input = st.text_input(
            "ğŸ’¬ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›",
            placeholder="llama3.2ã¨ã®å¯¾è©±ã‚’é–‹å§‹...",
            key="user_input"
        )
    
    with col2:
        if st.button("ğŸ‘ï¸ ç”»é¢åˆ†æ", help="llama3.2-visionã§ç”»é¢åˆ†æ"):
            with st.spinner("ğŸ‘ï¸ llama3.2-visionã§ç”»é¢åˆ†æä¸­..."):
                result = ai_system.analyze_screen_with_vision()
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": f"ğŸ‘ï¸ **ç”»é¢åˆ†æçµæœ**:\n\n{result}"
                })
                st.rerun()
    
    with col3:
        if st.button("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º", help="llama3.2-visionã§OCR"):
            with st.spinner("ğŸ“ llama3.2-visionã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸­..."):
                result = ai_system.extract_text_from_screen()
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": f"ğŸ“ **ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºçµæœ**:\n\n{result}"
                })
                st.rerun()
    
    with col4:
        fast_mode = st.checkbox("âš¡ é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰", help="çŸ­ã„å¿œç­”ã‚’å„ªå…ˆ")
    
    # é€ä¿¡ãƒœã‚¿ãƒ³
    if st.button("ğŸ’¬ é€ä¿¡", type="primary"):
        if user_input:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜
            st.session_state.messages.append({"role": "user", "content": user_input})
            
            # AIå¿œç­”ç”Ÿæˆ
            with st.spinner("ğŸ¤– llama3.2ã§å¿œç­”ç”Ÿæˆä¸­..."):
                context = ""
                if len(st.session_state.messages) > 1:
                    recent_messages = st.session_state.messages[-3:]
                    context = "æœ€è¿‘ã®ä¼šè©±: " + " | ".join([msg["content"] for msg in recent_messages])
                
                ai_response = ai_system.generate_response(
                    user_input, 
                    context=context, 
                    fast_mode=fast_mode
                )
                st.session_state.messages.append({"role": "assistant", "content": ai_response})
            
            # è‡ªå‹•éŸ³å£°èª­ã¿ä¸Šã’
            if st.checkbox("ğŸ”Š éŸ³å£°èª­ã¿ä¸Šã’", value=True):
                ai_system.text_to_speech(ai_response)
            
            st.rerun()

def render_settings(ai_system):
    """è¨­å®šç”»é¢"""
    st.header("âš™ï¸ llama3.2 è¨­å®š")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
    st.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«æƒ…å ±")
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«", Config.MAIN_MODEL)
        st.write("**ç”¨é€”**: é«˜é€Ÿãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ»é›‘è«‡")
        st.write("**ç‰¹å¾´**: 3bãƒ¢ãƒ‡ãƒ«ã§è»½é‡ãƒ»é«˜é€Ÿ")
    
    with col2:
        st.metric("ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«", Config.VISION_MODEL)
        st.write("**ç”¨é€”**: ç”»åƒèªè­˜ãƒ»ç”»é¢åˆ†æ")
        st.write("**ç‰¹å¾´**: 11bãƒ¢ãƒ‡ãƒ«ã§é«˜ç²¾åº¦")
    
    # äººæ ¼é¸æŠ
    st.subheader("ğŸ­ äººæ ¼è¨­å®š")
    personalities = {
        "friend": "ğŸ‘¥ è¦ªå‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
        "copy": "ğŸª åˆ†èº«", 
        "expert": "ğŸ§‘â€ğŸ« ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ"
    }
    
    selected_personality = st.selectbox(
        "äººæ ¼é¸æŠ",
        list(personalities.keys()),
        format_func=lambda x: personalities[x],
        index=list(personalities.keys()).index(ai_system.current_personality)
    )
    
    if selected_personality != ai_system.current_personality:
        ai_system.current_personality = selected_personality
        st.success(f"äººæ ¼ã‚’ã€Œ{personalities[selected_personality]}ã€ã«å¤‰æ›´ã—ã¾ã—ãŸ")
    
    # é«˜é€Ÿå¿œç­”è¨­å®š
    st.subheader("âš¡ é«˜é€Ÿå¿œç­”è¨­å®š")
    col1, col2 = st.columns(2)
    
    with col1:
        st.checkbox(
            "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”",
            value=Config.STREAMING_ENABLED,
            help="ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®å¿œç­”è¡¨ç¤º"
        )
    
    with col2:
        st.number_input(
            "é«˜é€Ÿå¿œç­”ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰",
            value=Config.FAST_RESPONSE_TIMEOUT,
            min_value=1.0,
            max_value=10.0,
            step=0.5
        )

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    st.set_page_config(
        page_title="ğŸš€ llama3.2 AI System",
        page_icon="ğŸš€",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸš€ AI Agent System - llama3.2 å®Œå…¨ç§»è¡Œç‰ˆ")
    st.markdown("### ğŸ¯ æœ€æ–°llama3.2ã‚·ãƒªãƒ¼ã‚ºã§ã€Œé€Ÿãã€ã€Œæ­£ç¢ºã«ã€ã€Œä½•ã§ã‚‚è¦‹ãˆã‚‹ã€æœ€å¼·ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    if 'ai_system' not in st.session_state:
        with st.spinner("ğŸš€ llama3.2 AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­..."):
            ai_system = OptimizedAISystem()
            success, check_results = ai_system.initialize()
            
            if success:
                st.session_state.ai_system = ai_system
                st.success("âœ… llama3.2 AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            else:
                st.error("âŒ AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
                if "error" in check_results:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {check_results['error']}")
                st.stop()
    
    ai_system = st.session_state.ai_system
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        # èµ·å‹•æ™‚ãƒã‚§ãƒƒã‚¯çµæœ
        if 'startup_check' not in st.session_state:
            st.session_state.startup_check = ai_system.startup_check.checks
        
        render_startup_check(st.session_state.startup_check)
        
        # è¨­å®š
        render_settings(ai_system)
    
    # ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–
    tab1, tab2 = st.tabs(["ğŸ’¬ AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", "ğŸ‘ï¸ ãƒ“ã‚¸ãƒ§ãƒ³æ©Ÿèƒ½"])
    
    with tab1:
        render_main_interface(ai_system)
    
    with tab2:
        st.header("ğŸ‘ï¸ llama3.2-vision ãƒ“ã‚¸ãƒ§ãƒ³æ©Ÿèƒ½")
        st.markdown("### ğŸ¨ é«˜åº¦ãªç”»åƒèªè­˜ãƒ»ç”»é¢åˆ†æ")
        
        # ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ†æ
        uploaded_file = st.file_uploader(
            "ğŸ“ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            type=['png', 'jpg', 'jpeg', 'bmp', 'gif'],
            key="vision_image_file"
        )
        
        if uploaded_file:
            # ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            image = Image.open(uploaded_file)
            st.image(image, caption="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ", use_column_width=True)
            
            # åˆ†æã‚¿ã‚¤ãƒ—é¸æŠ
            analysis_type = st.selectbox(
                "ğŸ” åˆ†æã‚¿ã‚¤ãƒ—",
                ["è©³ç´°èª¬æ˜", "ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º", "UIè¦ç´ åˆ†æ", "ã‚¨ãƒ©ãƒ¼æ¤œå‡º", "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆèªè­˜"],
                key="analysis_type"
            )
            
            prompts = {
                "è©³ç´°èª¬æ˜": "ã“ã®ç”»åƒã«ã¤ã„ã¦è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„",
                "ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º": "ã“ã®ç”»åƒã‹ã‚‰ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„",
                "UIè¦ç´ åˆ†æ": "ã“ã®ç”»é¢ã®UIè¦ç´ ï¼ˆãƒœã‚¿ãƒ³ã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã©ï¼‰ã‚’åˆ†æã—ã¦ãã ã•ã„",
                "ã‚¨ãƒ©ãƒ¼æ¤œå‡º": "ã“ã®ç”»åƒã«ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚„è­¦å‘Šã€å•é¡Œç‚¹ãŒãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„",
                "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆèªè­˜": "ã“ã®ç”»åƒã«å«ã¾ã‚Œã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã™ã¹ã¦ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„"
            }
            
            if st.button("ğŸ‘ï¸ llama3.2-visionã§åˆ†æ", type="primary"):
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_file_path = tmp_file.name
                
                # llama3.2-visionã§åˆ†æ
                with st.spinner("ğŸ‘ï¸ llama3.2-visionã§åˆ†æä¸­..."):
                    response = ai_system.ollama_client.generate(
                        model=Config.VISION_MODEL,
                        prompt=prompts[analysis_type],
                        images=[tmp_file_path],
                        options={
                            "temperature": 0.7,
                            "max_tokens": Config.MAX_TOKENS_FULL
                        }
                    )
                
                st.subheader("ğŸ“Š llama3.2-vision åˆ†æçµæœ")
                st.write(response['response'])
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                try:
                    os.unlink(tmp_file_path)
                except:
                    pass
    
    # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
    st.markdown("---")
    st.markdown(f"**ğŸš€ llama3.2ã‚·ãƒªãƒ¼ã‚º**: {Config.MAIN_MODEL} + {Config.VISION_MODEL}")
    st.markdown(f"**æœ€çµ‚æ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    st.markdown("**ğŸ¯ ç›®æ¨™**: é€Ÿããƒ»æ­£ç¢ºã«ãƒ»ä½•ã§ã‚‚è¦‹ãˆã‚‹æœ€å¼·ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")

if __name__ == "__main__":
    main()
