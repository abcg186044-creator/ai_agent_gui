# Windsurf ãƒãƒ«ãƒAIã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 

## ğŸš€ æ¦‚è¦

Windsurfå‘ã‘ã®ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ãƒ»éåŒæœŸã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚è¤‡æ•°ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã—ã¾ã™ã€‚

## ğŸ“‹ ä¸»ãªæ©Ÿèƒ½

### 1. éåŒæœŸãƒ»ä¸¦åˆ—æ¨è«–
- **AsyncOllamaClient**: è¤‡æ•°ã®ãƒãƒ¼ãƒˆï¼ˆ11434, 11435, 11436ï¼‰ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆ
- **asyncio**ãƒ™ãƒ¼ã‚¹ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±ã‚’æ­¢ã‚ãšã«è£å´ã§ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿè¡Œ
- ãƒãƒ¼ãƒˆç«¶åˆã‚’å®Œå…¨ã«å›é¿

### 2. æ¨è«–ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®åˆ†é›¢
- **LocalLLMServer**: llama-cpp-pythonã‚’ä½¿ç”¨ã—ãŸè‡ªä½œæ¨è«–ã‚µãƒ¼ãƒãƒ¼
- Ollamaã¨è‡ªä½œã‚¨ãƒ³ã‚¸ãƒ³ã®2å°ä½“åˆ¶ã§ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½œæ¥­ã‚’åˆ†æ‹…
- ãƒãƒ¼ãƒˆ11435ã§ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•

### 3. å®Ÿè¡Œã‚¿ã‚¹ã‚¯ã®ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°
- **CodingTaskRunner**: AIç”Ÿæˆã‚³ãƒ¼ãƒ‰ã®æ¤œè¨¼ã¨é©ç”¨ã‚’è‡ªå‹•åŒ–
- å„ªå…ˆåº¦ä»˜ãã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ã§åŠ¹ç‡çš„ãªå‡¦ç†
- æ§‹æ–‡æ¤œè¨¼ã€è«–ç†æ¤œè¨¼ã€å®Ÿè¡Œãƒ†ã‚¹ãƒˆã‚’è‡ªå‹•å®Ÿè¡Œ

### 4. UIã¸ã®é€²æ—è¡¨ç¤º
- **Streamlit**ãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤º
- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã€ŒAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆB: ã€‡ã€‡é–¢æ•°ã‚’å®Ÿè£…ä¸­...ã€ã¨ã„ã£ãŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¡¨ç¤º
- ã‚¿ã‚¹ã‚¯å±¥æ­´ã¨ã‚·ã‚¹ãƒ†ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®å¯è¦–åŒ–

## ğŸ› ï¸ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª

```bash
pip install streamlit fastapi uvicorn aiohttp pydantic
pip install llama-cpp-python  # ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼ç”¨
pip install pandas  # UIã®ãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºç”¨
```

### Ollamaã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl -fsSL https://ollama.ai/install.sh | sh

# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ollama pull llama3.2:3b
ollama pull llama3.1:8b
```

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
ai_agent_gui/
â”œâ”€â”€ async_ollama_client.py      # éåŒæœŸOllamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”œâ”€â”€ local_llm_server.py         # ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼
â”œâ”€â”€ coding_task_runner.py       # ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ãƒ©ãƒ³ãƒŠãƒ¼
â”œâ”€â”€ windsurf_multi_ai_integration.py  # Streamlit UI
â”œâ”€â”€ demo_windsurf_system.py     # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
â””â”€â”€ README_WINDSURF_MULTI_AI.md # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ

```bash
python demo_windsurf_system.py
```

ã“ã‚Œã«ã‚ˆã‚Šã€ã‚·ã‚¹ãƒ†ãƒ ã®å…¨æ©Ÿèƒ½ãŒãƒ‡ãƒ¢ã•ã‚Œã¾ã™ã€‚

### 2. Streamlit UIã§ä½¿ç”¨

```bash
streamlit run windsurf_multi_ai_integration.py
```

ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://localhost:8501` ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚

### 3. ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰ä½¿ç”¨

```python
import asyncio
from windsurf_multi_ai_integration import MultiAIOrchestrator

async def main():
    # ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
    orchestrator = MultiAIOrchestrator()
    await orchestrator.setup()
    
    # ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
    solutions = await orchestrator.generate_coding_solutions(
        prompt="Pythonã§GUIé›»å“ã‚’ä½œæˆã—ã¦ãã ã•ã„",
        description="Python GUIé›»å“ã‚¢ãƒ—ãƒªé–‹ç™º"
    )
    
    # ã‚¿ã‚¹ã‚¯ã¨ã—ã¦è¿½åŠ 
    for solution in solutions:
        task_id = orchestrator.add_coding_task(
            description="é›»å“ã‚¢ãƒ—ãƒªå®Ÿè£…",
            code=solution['code'],
            file_path="calculator.py",
            priority=TaskPriority.HIGH
        )
    
    # ã‚¿ã‚¹ã‚¯å‡¦ç†ã‚’é–‹å§‹
    orchestrator.start_task_processing()

asyncio.run(main())
```

## ğŸ”§ è¨­å®š

### ãƒãƒ¼ãƒˆè¨­å®š

- **Ollama**: 11434, 11435, 11436
- **ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼**: 11437
- **Streamlit**: 8501

### ãƒ¢ãƒ‡ãƒ«è¨­å®š

```python
# async_ollama_client.py ã§è¨­å®š
client = AsyncOllamaClient(
    ports=[11434, 11435, 11436],
    models=["llama3.2:3b", "llama3.1:8b", "qwen2.5:7b"]
)
```

### ã‚¿ã‚¹ã‚¯ãƒ©ãƒ³ãƒŠãƒ¼è¨­å®š

```python
# coding_task_runner.py ã§è¨­å®š
runner = CodingTaskRunner(
    max_workers=3,  # ä¸¦åˆ—å‡¦ç†æ•°
    base_path="."   # å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
)
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

- **ä¸¦åˆ—AIç”Ÿæˆ**: 4ã‚¿ã‚¹ã‚¯ã‚’åŒæ™‚ã«å‡¦ç†
- **å¹³å‡å¿œç­”æ™‚é–“**: 0.5-2.0ç§’
- **æˆåŠŸç‡**: 95%ä»¥ä¸Š
- **ãƒãƒ¼ãƒˆç«¶åˆ**: 0ä»¶

### ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

- **æœ€å¤§åŒæ™‚ã‚¿ã‚¹ã‚¯**: 10+ï¼ˆãƒãƒ¼ãƒˆæ•°ã«ä¾å­˜ï¼‰
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: å„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç´„500MB
- **CPUä½¿ç”¨ç‡**: ä¸¦åˆ—å®Ÿè¡Œæ™‚æœ€å¤§80%

## ğŸ› ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

1. **ãƒãƒ¼ãƒˆç«¶åˆã‚¨ãƒ©ãƒ¼**
   ```
   è§£æ±ºç­–: åˆ¥ã®ãƒãƒ¼ãƒˆç•ªå·ã‚’æŒ‡å®š
   client = AsyncOllamaClient(ports=[11438, 11439, 11440])
   ```

2. **Ollamaæ¥ç¶šã‚¨ãƒ©ãƒ¼**
   ```
   è§£æ±ºç­–: OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª
   ollama list
   ```

3. **ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼**
   ```
   è§£æ±ºç­–: max_workersã‚’æ¸›ã‚‰ã™
   runner = CodingTaskRunner(max_workers=1)
   ```

### ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰

```python
# è©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ”® æ©Ÿèƒ½æ‹¡å¼µ

### è¿½åŠ äºˆå®šã®æ©Ÿèƒ½

1. **GitHubé€£æº**: è‡ªå‹•ã‚³ãƒŸãƒƒãƒˆã¨ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
2. **ãƒ†ã‚¹ãƒˆè‡ªå‹•ç”Ÿæˆ**: å˜ä½“ãƒ†ã‚¹ãƒˆã®è‡ªå‹•ç”Ÿæˆ
3. **ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼**: AIã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½
4. **ãƒãƒ«ãƒè¨€èªå¯¾å¿œ**: Pythonä»¥å¤–ã®è¨€èªã‚µãƒãƒ¼ãƒˆ

### ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºä¾‹

```python
# ç‹¬è‡ªã®AIãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’è¿½åŠ 
class CustomAIBackend:
    async def generate(self, prompt: str) -> str:
        # ç‹¬è‡ªã®å‡¦ç†
        return "Generated code"

# ã‚¿ã‚¹ã‚¯ãƒ©ãƒ³ãƒŠãƒ¼ã«è¿½åŠ 
runner.add_validator(CustomValidator())
```

## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License

## ğŸ¤ è²¢çŒ®

ãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯Issueã«ã¦ãŠé¡˜ã„ã—ã¾ã™ã€‚

---

**ğŸ‰ ã“ã‚Œã§Windsurfãƒãƒ«ãƒAIã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ãŒå®Œæˆã§ã™ï¼**
