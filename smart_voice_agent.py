#!/usr/bin/env python3
"""
Smart Voice AI Agent - ã‚¹ãƒãƒ¼ãƒˆãƒ»ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°æ©Ÿèƒ½ä»˜ãéŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ 
"""

import streamlit as st
import time
import threading
import numpy as np
import sounddevice as sd
from faster_whisper import WhisperModel
import torch
import torchaudio
import requests
import json
import hashlib

# åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import ollama
    import faster_whisper
    import pyttsx3
    import sounddevice as sd
    import soundfile as sf
    import torch
    import torchaudio
except ImportError as e:
    st.error(f"âŒ å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    st.stop()

# è¨­å®š
class Config:
    MAIN_MODEL = "llama3.2"
    WHISPER_MODEL = "large-v3"
    AUDIO_SAMPLE_RATE = 16000
    AUDIO_CHANNELS = 1
    AUDIO_FORMAT = "int16"
    
    # ã‚¹ãƒãƒ¼ãƒˆãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°è¨­å®š
    VAD_SILENCE_THRESHOLD = 0.5
    MIN_SPEECH_DURATION = 2.0  # æœ€å°ç™ºè©±æ™‚é–“ï¼ˆç§’ï¼‰
    MAX_PAUSE_DURATION = 2.0   # æœ€å¤§ä¼‘æ­¢æ™‚é–“ï¼ˆç§’ï¼‰
    BUFFER_TIMEOUT = 5.0       # ãƒãƒƒãƒ•ã‚¡ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
    
    # UIè¨­å®š
    NODDING_INTERVAL = 1.0  # ç›¸æ§Œé–“éš”ï¼ˆç§’ï¼‰

class SmartVoiceBuffer:
    """ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.is_active = False
        self.audio_buffer = []
        self.speech_segments = []
        self.last_speech_end = None
        self.current_segment_start = None
        self.total_duration = 0.0
        
    def start_segment(self):
        """éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–‹å§‹"""
        self.current_segment_start = time.time()
        
    def end_segment(self):
        """éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ‚äº†"""
        if self.current_segment_start:
            duration = time.time() - self.current_segment_start
            if duration >= Config.MIN_SPEECH_DURATION:
                self.speech_segments.append({
                    "start": self.current_segment_start,
                    "end": time.time(),
                    "duration": duration,
                    "audio_data": self.audio_buffer.copy()
                })
                self.total_duration += duration
            
            self.last_speech_end = time.time()
            self.current_segment_start = None
            self.audio_buffer = []
    
    def add_audio_data(self, audio_data):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        self.audio_buffer.extend(audio_data)
    
    def should_process_speech(self):
        """éŸ³å£°å‡¦ç†ã™ã¹ãã‹åˆ¤å®š"""
        if not self.speech_segments:
            return False
        
        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ‚äº†ã‹ã‚‰æ™‚é–“ã‚’ãƒã‚§ãƒƒã‚¯
        if self.last_speech_end:
            time_since_last_speech = time.time() - self.last_speech_end
            return time_since_last_speech >= Config.MAX_PAUSE_DURATION
        
        return False
    
    def get_combined_audio(self):
        """çµåˆã•ã‚ŒãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if not self.speech_segments:
            return None
        
        # å…¨ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
        combined_audio = []
        for segment in self.speech_segments:
            combined_audio.extend(segment["audio_data"])
        
        return combined_audio
    
    def get_speech_info(self):
        """éŸ³å£°æƒ…å ±ã‚’å–å¾—"""
        if not self.speech_segments:
            return None
        
        return {
            "segments_count": len(self.speech_segments),
            "total_duration": self.total_duration,
            "first_segment": self.speech_segments[0] if self.speech_segments else None,
            "last_segment": self.speech_segments[-1] if self.speech_segments else None
        }
    
    def reset(self):
        """ãƒãƒƒãƒ•ã‚¡ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.speech_segments = []
        self.audio_buffer = []
        self.last_speech_end = None
        self.current_segment_start = None
        self.total_duration = 0.0

class SmartVoiceInputHandler:
    """ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°å…¥åŠ›ãƒãƒ³ãƒ‰ãƒ©"""
    
    def __init__(self):
        self.is_recording = False
        self.audio_queue = queue.Queue()
        self.whisper_model = None
        self.vad_model = None
        self.voice_buffer = SmartVoiceBuffer()
        self.processing_thread = None
        self.last_nodding_time = 0
        
    def initialize(self):
        """éŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            self.whisper_model = faster_whisper.WhisperModel(
                Config.WHISPER_MODEL,
                device="cuda" if self._check_cuda() else "cpu",
                compute_type="float32"  # float16ã‚’float32ã«å¤‰æ›´
            )
            
            # VADãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ - torch Hubã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            try:
                self.vad_model, utils = torch.hub.load(
                    'snakers4/silero-vad',
                    'silero_vad',
                    force_reload=True
                )
                self.vad_utils = utils
            except Exception as vad_error:
                st.warning(f"âš ï¸ VADãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {vad_error}")
                st.warning("âš ï¸ VADæ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–ã—ã¾ã™ã€‚éŸ³å£°æ¤œå‡ºã¯ç°¡æ˜“çš„ãªæ–¹æ³•ã§è¡Œã„ã¾ã™ã€‚")
                self.vad_model = None
            
            return True
        except Exception as e:
            st.error(f"âŒ éŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _check_cuda(self):
        """CUDAåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            import torch
            return torch.cuda.is_available()
        except:
            return False
    
    def start_recording(self):
        """éŒ²éŸ³é–‹å§‹"""
        if self.is_recording:
            return False
        
        self.is_recording = True
        self.voice_buffer.reset()
        
        # éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        def audio_callback(indata, frame_count, time_info, status):
            if status:
                st.error(f"âŒ éŸ³å£°å…¥åŠ›ã‚¨ãƒ©ãƒ¼: {status}")
            self.audio_queue.put(indata.copy())
        
        try:
            # éŸ³å£°ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’è¡¨ç¤º
            device_info = sd.query_devices()
            st.info(f"ğŸ¤ï¸ éŸ³å£°ãƒ‡ãƒã‚¤ã‚¹: {device_info[0]['name']}")
            
            self.processing_thread = threading.Thread(
                target=self._smart_record_audio,
                args=(audio_callback,),
                daemon=True
            )
            self.processing_thread.start()
            return True
        except Exception as e:
            st.error(f"âŒ éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.is_recording = False
            return False
    
    def _smart_record_audio(self, callback):
        """ã‚¹ãƒãƒ¼ãƒˆéŒ²éŸ³å‡¦ç†"""
        try:
            st.info("ğŸ¤ï¸ éŒ²éŸ³ã‚’é–‹å§‹ã—ã¾ã—ãŸ...")
            
            with sd.InputStream(
                samplerate=Config.AUDIO_SAMPLE_RATE,
                channels=Config.AUDIO_CHANNELS,
                dtype=Config.AUDIO_FORMAT,
                blocksize=1024,
                callback=callback
            ) as stream:
                st.success("âœ… éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒé–‹å§‹ã•ã‚Œã¾ã—ãŸ")
                
                while self.is_recording:
                    try:
                        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                        audio_data = self.audio_queue.get(timeout=1.0)
                        
                        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
                        if audio_data is not None and len(audio_data) > 0:
                            # VADã§éŸ³å£°æ´»å‹•æ¤œå‡º
                            if self.vad_model is not None:
                                # torch Hubã®VADãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                                audio_tensor = torch.from_numpy(np.array(audio_data, dtype=np.float32))
                                speech_prob = self.vad_model(audio_tensor, Config.AUDIO_SAMPLE_RATE).item()
                            else:
                                # ç°¡æ˜“çš„ãªéŸ³å£°æ¤œå‡ºï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
                                audio_array = np.array(audio_data)
                                energy = np.sqrt(np.mean(audio_array**2))
                                speech_prob = 1.0 if energy > 0.01 else 0.0
                            
                            if speech_prob > Config.VAD_SILENCE_THRESHOLD:
                                # éŸ³å£°ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆ
                                if not self.voice_buffer.current_segment_start:
                                    self.voice_buffer.start_segment()
                                    st.info("ğŸ¤ï¸ éŸ³å£°ã‚’æ¤œå‡ºã—ã¾ã—ãŸ...")
                                
                                self.voice_buffer.add_audio_data(audio_data)
                                
                                # ç›¸æ§Œãƒã‚§ãƒƒã‚¯
                                self._check_nodding()
                                
                            else:
                                # éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œãªã„å ´åˆ
                                if self.voice_buffer.current_segment_start:
                                    self.voice_buffer.end_segment()
                                    st.info("â¸ï¸ éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµ‚äº†ã—ã¾ã—ãŸ")
                                
                                # å‡¦ç†ã™ã¹ãã‹ãƒã‚§ãƒƒã‚¯
                                if self.voice_buffer.should_process_speech():
                                    self._process_buffered_speech()
                    
                    except queue.Empty:
                        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - éŒ²éŸ³ç¶™ç¶š
                        continue
                    except Exception as e:
                        st.error(f"âŒ éŸ³å£°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        continue
                        
        except Exception as e:
            st.error(f"âŒ éŒ²éŸ³ã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.is_recording = False
    
    def _check_nodding(self):
        """ç›¸æ§Œãƒã‚§ãƒƒã‚¯"""
        current_time = time.time()
        if current_time - self.last_nodding_time >= Config.NODDING_INTERVAL:
            self.last_nodding_time = current_time
            # VRMã‚¢ãƒã‚¿ãƒ¼ã«ç›¸æ§ŒæŒ‡ç¤ºã‚’é€ä¿¡
            self._send_vrm_command("nod")
    
    def _send_vrm_command(self, command):
        """VRMã‚¢ãƒã‚¿ãƒ¼ã«ã‚³ãƒãƒ³ãƒ‰ã‚’é€ä¿¡"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯WebSocketã‚„APIçµŒç”±ã§VRMã«æŒ‡ç¤ºã‚’é€ä¿¡
        if command == "nod":
            # è»½ãé ·ããƒ¢ãƒ¼ã‚·ãƒ§ãƒ³
            pass
        elif command == "thinking":
            # è€ƒãˆä¸­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³
            pass
    
    def stop_recording(self):
        """éŒ²éŸ³åœæ­¢"""
        if not self.is_recording:
            return False
        
        self.is_recording = False
        
        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†
        if self.voice_buffer.current_segment_start:
            self.voice_buffer.end_segment()
        
        # æ®‹ã‚Šã®ãƒãƒƒãƒ•ã‚¡ã‚’å‡¦ç†
        if self.voice_buffer.speech_segments:
            self._process_buffered_speech()
        
        if self.processing_thread:
            self.processing_thread.join(timeout=5)
            self.processing_thread = None
        
        return True
    
    def _process_buffered_speech(self):
        """ãƒãƒƒãƒ•ã‚¡ã•ã‚ŒãŸéŸ³å£°ã‚’å‡¦ç†"""
        try:
            combined_audio = self.voice_buffer.get_combined_audio()
            speech_info = self.voice_buffer.get_speech_info()
            
            if combined_audio and speech_info:
                # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’WAVãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›
                with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                    with wave.open(tmp_file.name, 'wb') as wf:
                        wf.setnchannels(Config.AUDIO_CHANNELS)
                        wf.setsampwidth(2)  # 16-bit
                        wf.setframerate(Config.AUDIO_SAMPLE_RATE)
                        wf.writeframes(combined_audio)
                        wf.close()
                    
                    # Whisperã§è»¢è¨˜
                    result = self.whisper_model.transcribe(
                        tmp_file.name,
                        language="ja",
                        word_timestamps=True,
                        temperature=0.0,
                        beam_size=5
                    )
                    
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    os.unlink(tmp_file.name)
                    
                    # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    st.session_state.last_transcription = result
                    st.session_state.speech_info = speech_info
                    
                    # ãƒãƒƒãƒ•ã‚¡ã‚’ãƒªã‚»ãƒƒãƒˆ
                    self.voice_buffer.reset()
                    
                    return result
            
        except Exception as e:
            st.error(f"âŒ ãƒãƒƒãƒ•ã‚¡å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def get_status(self):
        """ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        return {
            "is_recording": self.is_recording,
            "buffer_segments": len(self.voice_buffer.speech_segments),
            "total_duration": self.voice_buffer.total_duration,
            "last_speech_end": self.voice_buffer.last_speech_end
        }

class SmartVoiceAIAgent:
    """ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.ai_agent = AIAgent()
        self.voice_input = SmartVoiceInputHandler()
        self.current_conversation = []
        
    def initialize(self):
        """AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–"""
        try:
            # éŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            if not self.voice_input.initialize():
                return False
            
            return True
        except Exception as e:
            return False
    
    def process_voice_input(self):
        """éŸ³å£°å…¥åŠ›å‡¦ç†"""
        try:
            if self.voice_input.is_recording:
                status = self.voice_input.get_status()
                
                # å¾…æ©ŸçŠ¶æ…‹ã®è¡¨ç¤º
                if status["last_speech_end"]:
                    time_since_last_speech = time.time() - status["last_speech_end"]
                    if time_since_last_speech < Config.MAX_PAUSE_DURATION:
                        return "ã¾ã èã„ã¦ã‚‹ã‚ˆ...ç¶šãã‚’å¾…æ©Ÿä¸­ã§ã™ã€‚"
                    else:
                        return "ã¾ã èã„ã¦ã‚‹ã‚ˆ...æ–°ã—ã„ç™ºè©±ã‚’å¾…ã£ã¦ã„ã¾ã™ã€‚"
                else:
                    return "ã¾ã èã„ã¦ã‚‹ã‚ˆ...è©±ã—ã¦ãã ã•ã„ã€‚"
            
            return None
            
        except Exception as e:
            return f"âŒ éŸ³å£°å…¥åŠ›å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def generate_response(self, transcription_text):
        """AIå¿œç­”ç”Ÿæˆ"""
        try:
            if not transcription_text:
                return "éŸ³å£°ãŒèªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            
            # llama3.2ã§å¿œç­”ç”Ÿæˆ
            prompt = f"""ã‚ãªãŸã¯ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°å…¥åŠ›ã«åŸºã¥ã„ã¦ã€è‡ªç„¶ã§ä¸å¯§ãªå¿œç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°å…¥åŠ›: {transcription_text}

éŸ³å£°ã®ç‰¹å¾´:
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(self.voice_input.voice_buffer.speech_segments)}
- ç·æ™‚é–“: {self.voice_input.voice_buffer.total_duration:.1f}ç§’

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒšãƒ¼ã‚¹ã‚’å°Šé‡ã—ã€é©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚è‡ªç„¶ãªå¯¾è©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"""
            
            response = self.ai_agent.generate_response(prompt)
            
            return response
            
        except Exception as e:
            return f"âŒ å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"

def render_smart_voice_interface(ai_agent):
    """ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ¤ï¸ ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ ")
    
    # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
    input_method = st.radio(
        "ğŸ¯ å…¥åŠ›æ–¹æ³•ã‚’é¸æŠ",
        ["ğŸ¤ï¸ éŸ³å£°å…¥åŠ›", "âŒ¨ï¸ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›", "ğŸ”„ ä¸¡æ–¹ä½¿ç”¨"],
        horizontal=True
    )
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        if input_method in ["ğŸ¤ï¸ éŸ³å£°å…¥åŠ›", "ğŸ”„ ä¸¡æ–¹ä½¿ç”¨"]:
            st.subheader("ğŸ¤ï¸ éŸ³å£°éŒ²éŸ³")
            
            # éŒ²éŸ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("ğŸ¤ï¸ éŒ²éŸ³é–‹å§‹", key="start_smart_recording"):
                    if ai_agent.voice_input.start_recording():
                        st.success("âœ… éŒ²éŸ³é–‹å§‹")
                        st.session_state.recording_status = "recording"
                    else:
                        st.error("âŒ éŒ²éŸ³é–‹å§‹å¤±æ•—")
            
            with col2:
                if st.button("â¹ï¸ éŒ²éŸ³åœæ­¢", key="stop_smart_recording"):
                    if ai_agent.voice_input.stop_recording():
                        st.success("âœ… éŒ²éŸ³åœæ­¢")
                        st.session_state.recording_status = "stopped"
                    else:
                        st.error("âŒ éŒ²éŸ³åœæ­¢å¤±æ•—")
            
            with col3:
                auto_process = st.checkbox("ğŸ”„ è‡ªå‹•å‡¦ç†", value=True, help="éŸ³å£°ã®é€”åˆ‡ã‚Œã‚’è‡ªå‹•ã§æ¤œå‡º")
            
            # éŒ²éŸ³çŠ¶æ…‹è¡¨ç¤º
            if st.session_state.get("recording_status") == "recording":
                st.info("ğŸ”´ éŒ²éŸ³ä¸­...")
                
                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
                status = ai_agent.voice_input.get_status()
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°", status["buffer_segments"])
                
                with col2:
                    st.metric("ç·æ™‚é–“", f"{status['total_duration']:.1f}ç§’")
                
                with col3:
                    if status["last_speech_end"]:
                        time_since = time.time() - status["last_speech_end"]
                        st.metric("å‰å›ç™ºè©±ã‹ã‚‰", f"{time_since:.1f}ç§’å‰")
                    else:
                        st.metric("çŠ¶æ…‹", "ç™ºè©±ä¸­")
                
                # å¾…æ©ŸçŠ¶æ…‹ã®è¡¨ç¤º
                if auto_process:
                    waiting_message = ai_agent.process_voice_input()
                    if waiting_message:
                        st.info(f"ğŸ’­ {waiting_message}")
            
            # è»¢è¨˜çµæœè¡¨ç¤º
            if st.session_state.get("last_transcription"):
                transcription = st.session_state.last_transcription
                speech_info = st.session_state.get("speech_info", {})
                
                st.subheader("ğŸ“ éŸ³å£°è»¢è¨˜çµæœ")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write(f"**èªè­˜ãƒ†ã‚­ã‚¹ãƒˆ**: {transcription['text']}")
                    st.write(f"**å‡¦ç†æ™‚é–“**: {transcription.get('time', 'N/A')}ç§’")
                    
                    if speech_info:
                        st.write(f"**ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°**: {speech_info['segments_count']}")
                        st.write(f"**ç·æ™‚é–“**: {speech_info['total_duration']:.1f}ç§’")
                
                with col2:
                    # AIå¿œç­”ç”Ÿæˆ
                    if st.button("ğŸ¤– AIå¿œç­”ç”Ÿæˆ", key="generate_ai_response_voice"):
                        with st.spinner("ğŸ¤– AIå¿œç­”ç”Ÿæˆä¸­..."):
                            ai_response = ai_agent.generate_response(transcription['text'])
                            st.session_state.ai_response = ai_response
                            st.success("âœ… AIå¿œç­”ç”Ÿæˆå®Œäº†")
                    
                    # AIå¿œç­”è¡¨ç¤º
                    if st.session_state.get("ai_response"):
                        st.subheader("ğŸ¤– AIå¿œç­”")
                        st.write(st.session_state.ai_response)
                        
                        # éŸ³å£°èª­ã¿ä¸Šã’
                        if st.button("ğŸ”Š éŸ³å£°èª­ã¿ä¸Šã’", key="speak_response"):
                            try:
                                import pyttsx3
                                engine = pyttsx3.init()
                                engine.say(st.session_state.ai_response)
                                engine.runAndWait()
                                st.success("âœ… éŸ³å£°èª­ã¿ä¸Šã’å®Œäº†")
                            except Exception as e:
                                st.error(f"éŸ³å£°èª­ã¿ä¸Šã’ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        if input_method in ["âŒ¨ï¸ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›", "ğŸ”„ ä¸¡æ–¹ä½¿ç”¨"]:
            st.subheader("âŒ¨ï¸ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›")
            
            # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›æ¬„
            user_input = st.text_area(
                "ğŸ’¬ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
                key="text_input",
                height=100,
                placeholder="ã“ã“ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›..."
            )
            
            # å…¥åŠ›ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("ğŸ“¤ é€ä¿¡", key="send_text", type="primary"):
                    if user_input.strip():
                        with st.spinner("ğŸ¤– AIå¿œç­”ç”Ÿæˆä¸­..."):
                            ai_response = ai_agent.generate_response(user_input)
                            st.session_state.text_ai_response = ai_response
                            st.session_state.last_text_input = user_input
                            st.success("âœ… AIå¿œç­”ç”Ÿæˆå®Œäº†")
                    else:
                        st.warning("âš ï¸ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            
            with col2:
                if st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢", key="clear_text"):
                    st.session_state.text_input = ""
                    st.session_state.text_ai_response = ""
                    st.success("âœ… å…¥åŠ›ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
            
            with col3:
                if st.button("ğŸ“‹ å±¥æ­´", key="show_history"):
                    if "conversation_history" not in st.session_state:
                        st.session_state.conversation_history = []
                    
                    if st.session_state.conversation_history:
                        st.write("ğŸ“œ å¯¾è©±å±¥æ­´:")
                        for i, (user_msg, ai_msg) in enumerate(st.session_state.conversation_history[-5:], 1):
                            st.write(f"{i}. **ãƒ¦ãƒ¼ã‚¶ãƒ¼**: {user_msg}")
                            st.write(f"   **AI**: {ai_msg}")
                    else:
                        st.info("ğŸ“ å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
            
            # ãƒ†ã‚­ã‚¹ãƒˆAIå¿œç­”è¡¨ç¤º
            if st.session_state.get("text_ai_response"):
                st.subheader("ğŸ¤– AIå¿œç­”")
                st.write(st.session_state.text_ai_response)
                
                # éŸ³å£°èª­ã¿ä¸Šã’
                if st.button("ğŸ”Š éŸ³å£°èª­ã¿ä¸Šã’", key="speak_text_response"):
                    try:
                        import pyttsx3
                        engine = pyttsx3.init()
                        engine.say(st.session_state.text_ai_response)
                        engine.runAndWait()
                        st.success("âœ… éŸ³å£°èª­ã¿ä¸Šã’å®Œäº†")
                    except Exception as e:
                        st.error(f"éŸ³å£°èª­ã¿ä¸Šã’ã‚¨ãƒ©ãƒ¼: {str(e)}")
                
                # å±¥æ­´ã«ä¿å­˜
                if st.session_state.get("last_text_input"):
                    if "conversation_history" not in st.session_state:
                        st.session_state.conversation_history = []
                    
                    st.session_state.conversation_history.append(
                        (st.session_state.last_text_input, st.session_state.text_ai_response)
                    )
                    
                    # å±¥æ­´ã‚’æœ€æ–°ã®10ä»¶ã«åˆ¶é™
                    if len(st.session_state.conversation_history) > 10:
                        st.session_state.conversation_history = st.session_state.conversation_history[-10:]
    
    with col2:
        st.subheader("ğŸ“Š ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°è¨­å®š")
        
        # è¨­å®šè¡¨ç¤º
        st.write("**ç¾åœ¨ã®è¨­å®š**:")
        st.write(f"- æœ€å°ç™ºè©±æ™‚é–“: {Config.MIN_SPEECH_DURATION}ç§’")
        st.write(f"- æœ€å¤§ä¼‘æ­¢æ™‚é–“: {Config.MAX_PAUSE_DURATION}ç§’")
        st.write(f"- ãƒãƒƒãƒ•ã‚¡ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {Config.BUFFER_TIMEOUT}ç§’")
        st.write(f"- VADé–¾å€¤: {Config.VAD_SILENCE_THRESHOLD}")
        st.write(f"- ç›¸æ§Œé–“éš”: {Config.NODDING_INTERVAL}ç§’")
        
        # è¨­å®šèª¿æ•´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        with st.expander("è©³ç´°è¨­å®š"):
            min_speech = st.slider("æœ€å°ç™ºè©±æ™‚é–“", 0.5, 5.0, Config.MIN_SPEECH_DURATION)
            max_pause = st.slider("æœ€å¤§ä¼‘æ­¢æ™‚é–“", 0.5, 5.0, Config.MAX_PAUSE_DURATION)
            nodding_interval = st.slider("ç›¸æ§Œé–“éš”", 0.5, 3.0, Config.NODDING_INTERVAL)
            
            if st.button("è¨­å®šã‚’ä¿å­˜", key="save_settings"):
                # è¨­å®šã‚’ä¿å­˜ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼‰
                st.success("âœ… è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ")
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å±¥æ­´
        if st.session_state.get("recording_history"):
            st.subheader("ğŸ“ˆ éŒ²éŸ³å±¥æ­´")
            for i, record in enumerate(st.session_state.recording_history[-5:]):
                st.write(f"{i+1}. {record}")
        
        # å¯¾è©±çµ±è¨ˆ
        if "conversation_history" in st.session_state and st.session_state.conversation_history:
            st.subheader("ğŸ“ˆ å¯¾è©±çµ±è¨ˆ")
            total_conversations = len(st.session_state.conversation_history)
            st.metric("ç·å¯¾è©±æ•°", total_conversations)
            
            # æœ€æ–°ã®å¯¾è©±
            if st.session_state.conversation_history:
                latest_user, latest_ai = st.session_state.conversation_history[-1]
                st.write("**æœ€æ–°ã®å¯¾è©±**:")
                st.write(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼: {latest_user[:50]}...")
                st.write(f"ğŸ¤– AI: {latest_ai[:50]}...")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    st.set_page_config(
        page_title="ğŸ¤ï¸ Smart Voice AI Agent",
        page_icon="ğŸ¤ï¸",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ¤ï¸ AI Agent System - ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°å…¥åŠ›ç‰ˆ")
    st.markdown("### ğŸ¯ ã‚¹ãƒãƒ¼ãƒˆãƒ»ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹è‡ªç„¶ãªå¯¾è©±")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    if 'ai_agent' not in st.session_state:
        with st.spinner("ğŸ¤ï¸ ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­..."):
            ai_agent = SmartVoiceAIAgent()
            if ai_agent.initialize():
                st.session_state.ai_agent = ai_agent
                st.success("âœ… ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            else:
                st.error("âŒ AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
                st.stop()
    
    ai_agent = st.session_state.ai_agent
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.subheader("âš™ï¸ ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°è¨­å®š")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        st.write("**ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«**:")
        st.write(f"- Whisper: {Config.WHISPER_MODEL}")
        st.write(f"- AI: {Config.MAIN_MODEL}")
        
        # ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°è¨­å®š
        st.write("**ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°è¨­å®š**:")
        st.write(f"- æœ€å°ç™ºè©±æ™‚é–“: {Config.MIN_SPEECH_DURATION}ç§’")
        st.write(f"- æœ€å¤§ä¼‘æ­¢æ™‚é–“: {Config.MAX_PAUSE_DURATION}ç§’")
        st.write(f"- VADé–¾å€¤: {Config.VAD_SILENCE_THRESHOLD}")
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        if ai_agent.voice_input.is_recording:
            status = ai_agent.voice_input.get_status()
            st.write("**ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**:")
            st.write(f"- éŒ²éŸ³ä¸­: {status['is_recording']}")
            st.write(f"- ãƒãƒƒãƒ•ã‚¡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {status['buffer_segments']}")
            st.write(f"- ç·æ™‚é–“: {status['total_duration']:.1f}ç§’")
    
    # ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    render_smart_voice_interface(ai_agent)
    
    # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
    st.markdown("---")
    st.markdown(f"**ğŸ¤ï¸ ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°AI**: {Config.MAIN_MODEL}")
    st.markdown(f"**æœ€çµ‚æ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    st.markdown("**ğŸ¯ ç›®æ¨™**: ã‚¹ãƒãƒ¼ãƒˆãƒ»ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹è‡ªç„¶ãªå¯¾è©±ã¨é«˜ã„åŒ…å®¹åŠ›")

if __name__ == "__main__":
    main()
