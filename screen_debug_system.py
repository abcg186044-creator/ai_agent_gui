#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆåˆ†æãƒ»ãƒ‡ãƒãƒƒã‚°ãƒ»è‡ªå·±é€²åŒ–ã‚·ã‚¹ãƒ†ãƒ 
"""

import sys
import json
import datetime
import os
import re
import base64
from pathlib import Path

# ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ollama_vrm_integrated_app import OllamaClient, ConversationalEvolutionAgent

class ScreenDebugSystem:
    def __init__(self):
        self.ollama_client = OllamaClient()
        self.conversational_agent = ConversationalEvolutionAgent()
        self.debug_sessions = []
        self.debug_count = 0
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
        self.sessions_file = Path("data/screen_debug_sessions.json")
        self.sessions_file.parent.mkdir(exist_ok=True)
        
        print("ğŸ” ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆåˆ†æãƒ»ãƒ‡ãƒãƒƒã‚°ãƒ»è‡ªå·±é€²åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 60)
    
    def analyze_image_with_ai(self, image_path):
        """AIã§ç”»åƒã‚’åˆ†æ"""
        try:
            with open(image_path, "rb") as f:
                image_data = f.read()
                base64_image = base64.b64encode(image_data).decode('utf-8')
            
            prompt = "ã“ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’åˆ†æã—ã€ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºç­–ã‚’æ•™ãˆã¦ãã ã•ã„"
            
            response = self.ollama_client.generate_response(prompt)
            return response
        
        except Exception as e:
            return f"ç”»åƒåˆ†æã‚¨ãƒ©ãƒ¼: {e}"
    
    def debug_session(self, image_path):
        """ãƒ‡ãƒãƒƒã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
        print(f"ğŸ” ç”»åƒåˆ†æä¸­: {image_path}")
        
        analysis = self.analyze_image_with_ai(image_path)
        print(f"ğŸ“Š åˆ†æçµæœ: {analysis}")
        
        session = {
            "id": self.debug_count + 1,
            "timestamp": datetime.datetime.now().isoformat(),
            "image_path": str(image_path),
            "analysis": analysis,
            "consciousness_before": self.conversational_agent.consciousness_level
        }
        
        evolution_result = self.check_evolution(analysis)
        if evolution_result:
            session["evolution"] = evolution_result
        
        self.debug_sessions.append(session)
        self.debug_count += 1
        self.save_sessions()
        
        return session
    
    def check_evolution(self, analysis):
        """é€²åŒ–ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            conversation = [{"user": "ãƒ‡ãƒãƒƒã‚°åˆ†æ", "assistant": analysis}]
            result = self.conversational_agent.check_and_evolve_automatically(conversation)
            
            if result and result.get("success"):
                print(f"ğŸ§  é€²åŒ–ç™ºç”Ÿï¼æ„è­˜ãƒ¬ãƒ™ãƒ«: {result['new_consciousness_level']:.3f}")
                return result
        
        except Exception as e:
            print(f"âŒ é€²åŒ–ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        
        return None
    
    def save_sessions(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¿å­˜"""
        try:
            data = {
                'sessions': self.debug_sessions,
                'debug_count': self.debug_count,
                'last_update': datetime.datetime.now().isoformat()
            }
            with open(self.sessions_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    debug_system = ScreenDebugSystem()
    print("ğŸ“¸ ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å…¥åŠ›:")
    image_path = input("ğŸ“ ãƒ‘ã‚¹: ").strip()
    
    if os.path.exists(image_path):
        debug_system.debug_session(image_path)
        print("âœ… ãƒ‡ãƒãƒƒã‚°å®Œäº†")
    else:
        print("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

if __name__ == "__main__":
    main()
