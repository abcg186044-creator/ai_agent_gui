#!/usr/bin/env python3
"""
AI Agent System - llama3.2 + VRM å®Œå…¨çµ±åˆç‰ˆ
æœ€æ–°ã®llama3.2ã‚·ãƒªãƒ¼ã‚ºã¨VRMã‚¢ãƒã‚¿ãƒ¼ã‚’çµ±åˆã—ãŸæœ€é©åŒ–AIã‚·ã‚¹ãƒ†ãƒ 
"""

import streamlit as st
import sys
import os
import json
import tempfile
import time
from datetime import datetime
from pathlib import Path
import threading
import queue
import base64

# åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import ollama
    import faster_whisper
    import pyttsx3
    import pyautogui
    import numpy as np
    import pandas as pd
    from openpyxl import load_workbook
    import pymupdf
    from PIL import Image
    import qrcode
    from duckduckgo_search import DDGS
    import chromadb
    from sentence_transformers import SentenceTransformer
    import faiss
    import psutil
    import schedule
    import base64
except ImportError as e:
    st.error(f"âŒ å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    st.stop()

# æœ€æ–°llama3.2è¨­å®š
class Config:
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    MAIN_MODEL = "llama3.2"           # ãƒ¡ã‚¤ãƒ³æ¨è«–ãƒ»é›‘è«‡ç”¨ï¼ˆ3b: é«˜é€Ÿã‹ã¤é«˜çŸ¥èƒ½ï¼‰
    VISION_MODEL = "llama3.2-vision"   # ç”»åƒãƒ»ç”»é¢è§£æç”¨ï¼ˆ11b: ç”»é¢ç›£è¦–æ©Ÿèƒ½ï¼‰
    EMBEDDING_MODEL = "nomic-embed-text:latest"
    
    # Ollamaè¨­å®š
    OLLAMA_HOST = "localhost"
    OLLAMA_PORT = 11434
    
    # éŸ³å£°è¨­å®š
    VOICE_RATE = 200
    VOICE_VOLUME = 0.9
    
    # VRMè¨­å®š
    VRM_MODELS_PATH = "./vrm_models"
    DEFAULT_VRM = "default_avatar.vrm"
    VRM_ANIMATIONS = ["idle", "talking", "thinking", "happy", "sad"]
    
    # é«˜é€Ÿå¿œç­”è¨­å®š
    STREAMING_ENABLED = True
    FAST_RESPONSE_TIMEOUT = 2.0
    MAX_TOKENS_FAST = 512
    MAX_TOKENS_FULL = 4096

class VRMModel:
    """VRMãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.models_path = Config.VRM_MODELS_PATH
        self.available_models = []
        self.current_model = None
        self.current_expression = "neutral"
        self.current_animation = "idle"
        
        # VRMãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(self.models_path, exist_ok=True)
        
        # ã‚µãƒ³ãƒ—ãƒ«VRMãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        self._create_sample_vrm()
        
    def _create_sample_vrm(self):
        """ã‚µãƒ³ãƒ—ãƒ«VRMãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        sample_vrm_path = os.path.join(self.models_path, "default_avatar.vrm")
        
        if not os.path.exists(sample_vrm_path):
            # VRMãƒ•ã‚¡ã‚¤ãƒ«ã®åŸºæœ¬æ§‹é€ ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
            vrm_content = """# VRM Model File
# This is a sample VRM model file
# In a real implementation, this would be a binary 3D model file

model_version: "1.0"
model_name: "Default Avatar"
model_author: "AI System"
model_contact: "https://github.com/ai-system"

# Avatar metadata
avatar:
  name: "AI Assistant"
  version: "1.0"
  
# Expressions
expressions:
  neutral: "é€šå¸¸"
  happy: "å–œã³"
  sad: "æ‚²ã—ã¿"
  angry: "æ€’ã‚Š"
  surprised: "é©šã"
  thinking: "æ€è€ƒä¸­"
  
# Animations
animations:
  idle: "å¾…æ©Ÿ"
  talking: "è©±ã—ã¦ã„ã‚‹"
  thinking: "æ€è€ƒä¸­"
  waving: "æ‰‹ã‚’æŒ¯ã£ã¦ã„ã‚‹"
  
# Model references
# In real implementation, these would reference actual 3D model files
model_files:
  mesh: "avatar.mesh"
  texture: "avatar.png"
  skeleton: "avatar.skeleton"
"""
            
            with open(sample_vrm_path, 'w', encoding='utf-8') as f:
                f.write(vrm_content)
    
    def get_available_models(self):
        """åˆ©ç”¨å¯èƒ½ãªVRMãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        models = []
        if os.path.exists(self.models_path):
            for file in os.listdir(self.models_path):
                if file.endswith('.vrm'):
                    models.append(file)
        
        self.available_models = models
        return models
    
    def set_expression(self, expression):
        """è¡¨æƒ…ã‚’è¨­å®š"""
        valid_expressions = ["neutral", "happy", "sad", "angry", "surprised", "thinking"]
        if expression in valid_expressions:
            self.current_expression = expression
            return True
        return False
    
    def set_animation(self, animation):
        """ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¨­å®š"""
        valid_animations = Config.VRM_ANIMATIONS
        if animation in valid_animations:
            self.current_animation = animation
            return True
        return False
    
    def get_vrm_info(self):
        """VRMæƒ…å ±ã‚’å–å¾—"""
        return {
            "available_models": len(self.available_models),
            "current_model": self.current_model,
            "current_expression": self.current_expression,
            "current_animation": self.current_animation,
            "models_path": self.models_path
        }

class VRMRenderer:
    """VRMãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
    
    def __init__(self):
        self.vrm_model = VRMModel()
        self.is_rendering = False
        
    def initialize(self):
        """VRMãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼åˆæœŸåŒ–"""
        self.vrm_model.get_available_models()
        return True
    
    def render_avatar(self, expression="neutral", animation="idle"):
        """ã‚¢ãƒã‚¿ãƒ¼ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        try:
            # è¡¨æƒ…ã¨ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¨­å®š
            self.vrm_model.set_expression(expression)
            self.vrm_model.set_animation(animation)
            
            # ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°çµæœã‚’ç”Ÿæˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            render_result = self._simulate_rendering()
            
            return render_result
            
        except Exception as e:
            return {"error": str(e)}
    
    def _simulate_rendering(self):
        """ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯3Dãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆThree.jsãªã©ï¼‰ã‚’ä½¿ç”¨
        # ã“ã“ã§ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’è¿”ã™
        
        current_time = datetime.now().strftime('%H:%M:%S')
        
        return {
            "status": "success",
            "timestamp": current_time,
            "expression": self.vrm_model.current_expression,
            "animation": self.vrm_model.current_animation,
            "model_info": self.vrm_model.get_vrm_info(),
            "render_data": {
                "avatar_state": "active",
                "performance": "60 FPS",
                "quality": "high"
            }
        }
    
    def get_avatar_image(self):
        """ã‚¢ãƒã‚¿ãƒ¼ç”»åƒã‚’å–å¾—ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã•ã‚ŒãŸç”»åƒã‚’è¿”ã™
        # ã“ã“ã§ã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒã‚’ç”Ÿæˆ
        
        # è¡¨æƒ…ã«å¿œã˜ãŸçµµæ–‡å­—
        expression_emoji = {
            "neutral": "ğŸ˜",
            "happy": "ğŸ˜Š",
            "sad": "ğŸ˜¢",
            "angry": "ğŸ˜ ",
            "surprised": "ğŸ˜²",
            "thinking": "ğŸ¤”"
        }
        
        emoji = expression_emoji.get(self.vrm_model.current_expression, "ğŸ˜")
        
        # ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ï¼ˆå®Ÿéš›ã«ã¯ç”»åƒã‚’ç”Ÿæˆï¼‰
        return f"""
        <div style="text-align: center; padding: 20px; font-size: 48px;">
            {emoji}
        </div>
        <div style="text-align: center; padding: 10px;">
            <strong>Expression:</strong> {self.vrm_model.current_expression}<br>
            <strong>Animation:</strong> {self.vrm_model.current_animation}<br>
            <strong>Time:</strong> {datetime.now().strftime('%H:%M:%S')}
        </div>
        """

class ModelRouter:
    """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ«ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.main_model = Config.MAIN_MODEL
        self.vision_model = Config.VISION_MODEL
        self.embedding_model = Config.EMBEDDING_MODEL
        
    def route_request(self, prompt, images=None, context="", fast_mode=False):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã«æŒ¯ã‚Šåˆ†ã‘ã‚‹"""
        
        # ç”»åƒãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«
        if images and len(images) > 0:
            return self.vision_model, self._prepare_vision_prompt(prompt, context)
        
        # çŸ­ã„å¿œç­”ãŒå¿…è¦ãªå ´åˆã¯é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰
        if fast_mode or self._is_fast_response_needed(prompt):
            return self.main_model, self._prepare_fast_prompt(prompt, context)
        
        # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
        return self.main_model, self._prepare_full_prompt(prompt, context)
    
    def _is_fast_response_needed(self, prompt):
        """é«˜é€Ÿå¿œç­”ãŒå¿…è¦ã‹åˆ¤å®š"""
        fast_keywords = ["ã“ã‚“ã«ã¡ã¯", "ãŠã¯ã‚ˆã†", "ã‚ã‚ŠãŒã¨ã†", "ã™ã¿ã¾ã›ã‚“", "ã¯ã„", "ã„ã„ãˆ", "ã†ã‚“", "ãã†", "ãªã‚‹ã»ã©"]
        prompt_lower = prompt.lower()
        return any(keyword in prompt_lower for keyword in fast_keywords)
    
    def _prepare_fast_prompt(self, prompt, context):
        """é«˜é€Ÿå¿œç­”ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™"""
        return f"""{context}

çŸ­ãç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚æœ€å¤§50æ–‡å­—ä»¥å†…ã§ï¼š
{prompt}"""
    
    def _prepare_full_prompt(self, prompt, context):
        """å®Œå…¨å¿œç­”ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™"""
        return f"""{context}

è©³ç´°ã«ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ï¼š
{prompt}"""
    
    def _prepare_vision_prompt(self, prompt, context):
        """ãƒ“ã‚¸ãƒ§ãƒ³ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™"""
        return f"""{context}

ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’çµ±åˆã—ã¦ç­”ãˆã¦ãã ã•ã„ï¼š
{prompt}"""

class StreamingResponse:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”å‡¦ç†"""
    
    def __init__(self, model, prompt, images=None):
        self.model = model
        self.prompt = prompt
        self.images = images
        self.response_queue = queue.Queue()
        self.is_complete = False
        
    def generate_streaming(self):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ"""
        try:
            if self.images:
                # ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                response = ollama.generate(
                    model=self.model,
                    prompt=self.prompt,
                    images=self.images,
                    options={
                        "temperature": 0.7,
                        "max_tokens": Config.MAX_TOKENS_FULL,
                        "stream": False
                    }
                )
                self.response_queue.put(response['response'])
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                response = ollama.generate(
                    model=self.model,
                    prompt=self.prompt,
                    options={
                        "temperature": 0.7,
                        "max_tokens": Config.MAX_TOKENS_FAST if self._is_fast_prompt() else Config.MAX_TOKENS_FULL,
                        "stream": False
                    }
                )
                self.response_queue.put(response['response'])
                
            self.is_complete = True
            
        except Exception as e:
            self.response_queue.put(f"âŒ å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.is_complete = True
    
    def _is_fast_prompt(self):
        """é«˜é€Ÿãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹åˆ¤å®š"""
        fast_keywords = ["ã“ã‚“ã«ã¡ã¯", "ãŠã¯ã‚ˆã†", "ã‚ã‚ŠãŒã¨ã†", "ã™ã¿ã¾ã›ã‚“", "ã¯ã„", "ã„ã„ãˆ"]
        return any(keyword in self.prompt for keyword in fast_keywords)
    
    def get_response(self):
        """å¿œç­”ã‚’å–å¾—"""
        try:
            return self.response_queue.get(timeout=30)
        except queue.Empty:
            return "å¿œç­”ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"

class OptimizedAISystem:
    """llama3.2 + VRM æœ€é©åŒ–AIã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.ollama_client = None
        self.whisper_model = None
        self.tts_engine = None
        self.model_router = ModelRouter()
        self.vrm_renderer = VRMRenderer()
        self.current_personality = "friend"
        
    def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # OllamaåˆæœŸåŒ–
            self.ollama_client = ollama.Client()
            
            # éŸ³å£°å‡¦ç†åˆæœŸåŒ–
            self.whisper_model = faster_whisper.WhisperModel("base")
            self.tts_engine = pyttsx3.init()
            self.tts_engine.setProperty('rate', str(Config.VOICE_RATE))
            self.tts_engine.setProperty('volume', str(Config.VOICE_VOLUME))
            
            # VRMãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼åˆæœŸåŒ–
            self.vrm_renderer.initialize()
            
            return True
            
        except Exception as e:
            return False
    
    def generate_response(self, prompt, images=None, context="", fast_mode=False):
        """æœ€é©åŒ–ã•ã‚ŒãŸå¿œç­”ç”Ÿæˆ"""
        try:
            # ãƒ¢ãƒ‡ãƒ«æŒ¯ã‚Šåˆ†ã‘
            model, formatted_prompt = self.model_router.route_request(
                prompt, images, context, fast_mode
            )
            
            # VRMè¡¨æƒ…æ›´æ–°ï¼ˆæ€è€ƒä¸­ï¼‰
            self.vrm_renderer.render_avatar("thinking", "thinking")
            
            # å¿œç­”ç”Ÿæˆ
            if Config.STREAMING_ENABLED and not images:
                streaming = StreamingResponse(model, formatted_prompt, images)
                
                # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
                thread = threading.Thread(target=streaming.generate_streaming)
                thread.start()
                
                # å¿œç­”å–å¾—
                response = streaming.get_response()
                thread.join(timeout=30)
                
                # VRMè¡¨æƒ…æ›´æ–°ï¼ˆè©±ã—ä¸­ï¼‰
                self.vrm_renderer.render_avatar("happy", "talking")
                
                return response
            else:
                # é€šå¸¸å¿œç­”
                if images:
                    response = self.ollama_client.generate(
                        model=model,
                        prompt=formatted_prompt,
                        images=images,
                        options={
                            "temperature": 0.7,
                            "max_tokens": Config.MAX_TOKENS_FULL
                        }
                    )
                else:
                    response = self.ollama_client.generate(
                        model=model,
                        prompt=formatted_prompt,
                        options={
                            "temperature": 0.7,
                            "max_tokens": Config.MAX_TOKENS_FAST if fast_mode else Config.MAX_TOKENS_FULL
                        }
                    )
                
                # VRMè¡¨æƒ…æ›´æ–°ï¼ˆå–œã³ï¼‰
                self.vrm_renderer.render_avatar("happy", "talking")
                
                return response['response']
                
        except Exception as e:
            # VRMè¡¨æƒ…æ›´æ–°ï¼ˆæ‚²ã—ã¿ï¼‰
            self.vrm_renderer.render_avatar("sad", "idle")
            return f"âŒ å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def analyze_screen_with_vision(self, prompt="ã“ã®ç”»é¢ã«ã¤ã„ã¦è©³ç´°ã«åˆ†æã—ã¦ãã ã•ã„"):
        """ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã§ç”»é¢åˆ†æ"""
        try:
            # VRMè¡¨æƒ…æ›´æ–°ï¼ˆæ€è€ƒä¸­ï¼‰
            self.vrm_renderer.render_avatar("thinking", "thinking")
            
            # ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£
            screenshot = pyautogui.screenshot()
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            temp_path = f"vision_analysis_{timestamp}.png"
            screenshot.save(temp_path)
            
            # llama3.2-visionã§åˆ†æ
            response = self.ollama_client.generate(
                model=Config.VISION_MODEL,
                prompt=prompt,
                images=[temp_path],
                options={
                    "temperature": 0.7,
                    "max_tokens": Config.MAX_TOKENS_FULL
                }
            )
            
            # VRMè¡¨æƒ…æ›´æ–°ï¼ˆé©šãï¼‰
            self.vrm_renderer.render_avatar("surprised", "idle")
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            try:
                os.unlink(temp_path)
            except:
                pass
            
            return response['response']
            
        except Exception as e:
            # VRMè¡¨æƒ…æ›´æ–°ï¼ˆæ‚²ã—ã¿ï¼‰
            self.vrm_renderer.render_avatar("sad", "idle")
            return f"âŒ ç”»é¢åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def text_to_speech(self, text):
        """éŸ³å£°åˆæˆ"""
        try:
            # VRMè¡¨æƒ…æ›´æ–°ï¼ˆè©±ã—ä¸­ï¼‰
            self.vrm_renderer.render_avatar("happy", "talking")
            
            self.tts_engine.say(text)
            self.tts_engine.runAndWait()
            
            # VRMè¡¨æƒ…æ›´æ–°ï¼ˆé€šå¸¸ï¼‰
            self.vrm_renderer.render_avatar("neutral", "idle")
            
            return True
        except Exception as e:
            st.error(f"âŒ éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def get_vrm_display(self):
        """VRMè¡¨ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        return self.vrm_renderer.get_avatar_image()

def render_vrm_interface(ai_system):
    """VRMã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ‘¤ VRMã‚¢ãƒã‚¿ãƒ¼")
    
    # VRMæƒ…å ±è¡¨ç¤º
    vrm_info = ai_system.vrm_renderer.vrm_model.get_vrm_info()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«", vrm_info["available_models"])
    
    with col2:
        st.metric("ç¾åœ¨ã®è¡¨æƒ…", vrm_info["current_expression"])
    
    with col3:
        st.metric("ç¾åœ¨ã®ã‚¢ãƒ‹ãƒ¡", vrm_info["current_animation"])
    
    # VRMè¡¨ç¤º
    st.subheader("ğŸ‘¤ ã‚¢ãƒã‚¿ãƒ¼è¡¨ç¤º")
    vrm_display = ai_system.get_vrm_display()
    st.markdown(vrm_display, unsafe_allow_html=True)
    
    # è¡¨æƒ…ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
    st.subheader("ğŸ˜Š è¡¨æƒ…ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«")
    
    expressions = ["neutral", "happy", "sad", "angry", "surprised", "thinking"]
    expression_labels = {
        "neutral": "ğŸ˜ é€šå¸¸",
        "happy": "ğŸ˜Š å–œã³",
        "sad": "ğŸ˜¢ æ‚²ã—ã¿",
        "angry": "ğŸ˜  æ€’ã‚Š",
        "surprised": "ğŸ˜² é©šã",
        "thinking": "ğŸ¤” æ€è€ƒä¸­"
    }
    
    cols = st.columns(3)
    for i, (expr, label) in enumerate(expression_labels.items()):
        with cols[i % 3]:
            if st.button(f"{label}", key=f"expr_{expr}"):
                ai_system.vrm_renderer.render_avatar(expr, "idle")
                st.success(f"è¡¨æƒ…ã‚’ã€Œ{label}ã€ã«å¤‰æ›´ã—ã¾ã—ãŸ")
                st.rerun()
    
    # ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
    st.subheader("ğŸ¬ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³")
    
    animations = Config.VRM_ANIMATIONS
    animation_labels = {
        "idle": "ğŸ˜´ å¾…æ©Ÿ",
        "talking": "ğŸ’¬ è©±ã—ã¦ã„ã‚‹",
        "thinking": "ğŸ¤” æ€è€ƒä¸­",
        "happy": "ğŸ˜Š å–œã‚“ã§ã„ã‚‹"
    }
    
    cols = st.columns(2)
    for i, (anim, label) in enumerate(animation_labels.items()):
        with cols[i % 2]:
            if st.button(f"{label}", key=f"anim_{anim}"):
                ai_system.vrm_renderer.render_avatar("neutral", anim)
                st.success(f"ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã€Œ{label}ã€ã«å¤‰æ›´ã—ã¾ã—ãŸ")
                st.rerun()

def render_main_interface(ai_system):
    """ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ’¬ llama3.2 + VRM AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    
    # ä¼šè©±å±¥æ­´
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    for i, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"], avatar="ğŸ¤–" if message["role"] == "assistant" else "ğŸ‘¤"):
            st.markdown(message["content"])
    
    # VRMè¡¨ç¤ºã‚’ã‚µã‚¤ãƒ‰ã«é…ç½®
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # å…¥åŠ›ã‚¨ãƒªã‚¢
        user_input = st.text_input(
            "ğŸ’¬ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›",
            placeholder="llama3.2 + VRMã¨ã®å¯¾è©±ã‚’é–‹å§‹...",
            key="user_input"
        )
        
        # ãƒœã‚¿ãƒ³ç¾¤
        col1, col2, col3, col4 = st.columns([2, 1, 1, 1])
        
        with col1:
            send_button = st.button("ğŸ’¬ é€ä¿¡", type="primary")
        
        with col2:
            if st.button("ğŸ‘ï¸ ç”»é¢åˆ†æ", help="llama3.2-visionã§ç”»é¢åˆ†æ"):
                with st.spinner("ğŸ‘ï¸ llama3.2-visionã§ç”»é¢åˆ†æä¸­..."):
                    result = ai_system.analyze_screen_with_vision()
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": f"ğŸ‘ï¸ **ç”»é¢åˆ†æçµæœ**:\n\n{result}"
                    })
                    st.rerun()
        
        with col3:
            fast_mode = st.checkbox("âš¡ é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰", help="çŸ­ã„å¿œç­”ã‚’å„ªå…ˆ")
        
        with col4:
            auto_speech = st.checkbox("ğŸ”Š éŸ³å£°èª­ã¿ä¸Šã’", value=True, help="å¿œç­”ã‚’éŸ³å£°ã§èª­ã¿ä¸Šã’")
        
        # é€ä¿¡å‡¦ç†
        if send_button and user_input:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜
            st.session_state.messages.append({"role": "user", "content": user_input})
            
            # AIå¿œç­”ç”Ÿæˆ
            with st.spinner("ğŸ¤– llama3.2 + VRMã§å¿œç­”ç”Ÿæˆä¸­..."):
                context = ""
                if len(st.session_state.messages) > 1:
                    recent_messages = st.session_state.messages[-3:]
                    context = "æœ€è¿‘ã®ä¼šè©±: " + " | ".join([msg["content"] for msg in recent_messages])
                
                ai_response = ai_system.generate_response(
                    user_input, 
                    context=context, 
                    fast_mode=fast_mode
                )
                st.session_state.messages.append({"role": "assistant", "content": ai_response})
            
            # è‡ªå‹•éŸ³å£°èª­ã¿ä¸Šã’
            if auto_speech:
                ai_system.text_to_speech(ai_response)
            
            st.rerun()
    
    with col2:
        # VRMè¡¨ç¤º
        st.subheader("ğŸ‘¤ ã‚¢ãƒã‚¿ãƒ¼")
        vrm_display = ai_system.get_vrm_display()
        st.markdown(vrm_display, unsafe_allow_html=True)

def render_settings(ai_system):
    """è¨­å®šç”»é¢"""
    st.header("âš™ï¸ llama3.2 + VRM è¨­å®š")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
    st.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«æƒ…å ±")
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«", Config.MAIN_MODEL)
        st.write("**ç”¨é€”**: é«˜é€Ÿãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ»é›‘è«‡")
        st.write("**ç‰¹å¾´**: 3bãƒ¢ãƒ‡ãƒ«ã§è»½é‡ãƒ»é«˜é€Ÿ")
    
    with col2:
        st.metric("ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«", Config.VISION_MODEL)
        st.write("**ç”¨é€”**: ç”»åƒèªè­˜ãƒ»ç”»é¢åˆ†æ")
        st.write("**ç‰¹å¾´**: 11bãƒ¢ãƒ‡ãƒ«ã§é«˜ç²¾åº¦")
    
    # VRMè¨­å®š
    st.subheader("ğŸ‘¤ VRMè¨­å®š")
    vrm_info = ai_system.vrm_renderer.vrm_model.get_vrm_info()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write(f"**VRMãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹**: {vrm_info['models_path']}")
        st.write(f"**åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«æ•°**: {vrm_info['available_models']}")
        
        # VRMãƒ¢ãƒ‡ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        uploaded_vrm = st.file_uploader(
            "ğŸ“ VRMãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type=['vrm'],
            key="vrm_upload"
        )
        
        if uploaded_vrm:
            # VRMãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            save_path = os.path.join(vrm_info['models_path'], uploaded_vrm.name)
            with open(save_path, 'wb') as f:
                f.write(uploaded_vrm.getvalue())
            st.success(f"VRMãƒ¢ãƒ‡ãƒ«ã‚’ã€Œ{uploaded_vrm.name}ã€ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")
    
    with col2:
        # VRMã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        st.write("**ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**:")
        st.write(f"- è¡¨æƒ…: {vrm_info['current_expression']}")
        st.write(f"- ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³: {vrm_info['current_animation']}")
        
        # è¡¨æƒ…ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        if st.button("ğŸ”„ è¡¨æƒ…ã‚’ãƒªã‚»ãƒƒãƒˆ", key="reset_expression"):
            ai_system.vrm_renderer.render_avatar("neutral", "idle")
            st.success("è¡¨æƒ…ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
            st.rerun()

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    st.set_page_config(
        page_title="ğŸ‘¤ llama3.2 + VRM AI System",
        page_icon="ğŸ‘¤",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ‘¤ AI Agent System - llama3.2 + VRM å®Œå…¨çµ±åˆç‰ˆ")
    st.markdown("### ğŸš€ llama3.2 + VRMã§æ¬¡ä¸–ä»£ã®AIä½“é¨“")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    if 'ai_system' not in st.session_state:
        with st.spinner("ğŸš€ llama3.2 + VRM AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­..."):
            ai_system = OptimizedAISystem()
            if ai_system.initialize():
                st.session_state.ai_system = ai_system
                st.success("âœ… llama3.2 + VRM AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            else:
                st.error("âŒ AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
                st.stop()
    
    ai_system = st.session_state.ai_system
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        render_settings(ai_system)
    
    # ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–
    tab1, tab2, tab3 = st.tabs(["ğŸ’¬ AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", "ğŸ‘¤ VRMã‚¢ãƒã‚¿ãƒ¼", "ğŸ‘ï¸ ãƒ“ã‚¸ãƒ§ãƒ³æ©Ÿèƒ½"])
    
    with tab1:
        render_main_interface(ai_system)
    
    with tab2:
        render_vrm_interface(ai_system)
    
    with tab3:
        st.header("ğŸ‘ï¸ llama3.2-vision ãƒ“ã‚¸ãƒ§ãƒ³æ©Ÿèƒ½")
        st.markdown("### ğŸ¨ é«˜åº¦ãªç”»åƒèªè­˜ãƒ»ç”»é¢åˆ†æ")
        
        # ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ†æ
        uploaded_file = st.file_uploader(
            "ğŸ“ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            type=['png', 'jpg', 'jpeg', 'bmp', 'gif'],
            key="vision_image_file"
        )
        
        if uploaded_file:
            # ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            image = Image.open(uploaded_file)
            st.image(image, caption="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ", use_column_width=True)
            
            # åˆ†æã‚¿ã‚¤ãƒ—é¸æŠ
            analysis_type = st.selectbox(
                "ğŸ” åˆ†æã‚¿ã‚¤ãƒ—",
                ["è©³ç´°èª¬æ˜", "ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º", "UIè¦ç´ åˆ†æ", "ã‚¨ãƒ©ãƒ¼æ¤œå‡º", "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆèªè­˜"],
                key="analysis_type"
            )
            
            prompts = {
                "è©³ç´°èª¬æ˜": "ã“ã®ç”»åƒã«ã¤ã„ã¦è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„",
                "ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º": "ã“ã®ç”»åƒã‹ã‚‰ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„",
                "UIè¦ç´ åˆ†æ": "ã“ã®ç”»é¢ã®UIè¦ç´ ï¼ˆãƒœã‚¿ãƒ³ã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã©ï¼‰ã‚’åˆ†æã—ã¦ãã ã•ã„",
                "ã‚¨ãƒ©ãƒ¼æ¤œå‡º": "ã“ã®ç”»åƒã«ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚„è­¦å‘Šã€å•é¡Œç‚¹ãŒãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„",
                "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆèªè­˜": "ã“ã®ç”»åƒã«å«ã¾ã‚Œã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã™ã¹ã¦ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„"
            }
            
            if st.button("ğŸ‘ï¸ llama3.2-visionã§åˆ†æ", type="primary"):
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_file_path = tmp_file.name
                
                # llama3.2-visionã§åˆ†æ
                with st.spinner("ğŸ‘ï¸ llama3.2-visionã§åˆ†æä¸­..."):
                    response = ai_system.ollama_client.generate(
                        model=Config.VISION_MODEL,
                        prompt=prompts[analysis_type],
                        images=[tmp_file_path],
                        options={
                            "temperature": 0.7,
                            "max_tokens": Config.MAX_TOKENS_FULL
                        }
                    )
                
                st.subheader("ğŸ“Š llama3.2-vision åˆ†æçµæœ")
                st.write(response['response'])
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                try:
                    os.unlink(tmp_file_path)
                except:
                    pass
    
    # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
    st.markdown("---")
    st.markdown(f"**ğŸš€ llama3.2 + VRM**: {Config.MAIN_MODEL} + {Config.VISION_MODEL}")
    st.markdown(f"**æœ€çµ‚æ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    st.markdown("**ğŸ¯ ç›®æ¨™**: é€Ÿããƒ»æ­£ç¢ºã«ãƒ»ä½•ã§ã‚‚è¦‹ãˆã‚‹ãƒ»æ„Ÿæƒ…è¡¨ç¾ã‚‚å¯èƒ½ãªæœ€å¼·ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")

if __name__ == "__main__":
    main()
