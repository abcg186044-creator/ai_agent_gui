#!/usr/bin/env python3
"""
é«˜åº¦çŸ¥è­˜ã‚·ã‚¹ãƒ†ãƒ 
ãƒãƒ«ãƒæ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€è‡ªå·±æ¤œè¨¼ã€RAGçµ±åˆã€é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†
"""

import streamlit as st
import requests
import json
import re
import hashlib
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from pathlib import Path
import numpy as np
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import aiohttp
from sentence_transformers import SentenceTransformer
import faiss
import pickle
from collections import defaultdict
import threading
import time

class SourceType(Enum):
    """æƒ…å ±ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—"""
    DUCKDUCKGO = "duckduckgo"
    ARXIV = "arxiv"
    GITHUB = "github"
    LOCAL_KNOWLEDGE = "local_knowledge"
    PERSONAL_MEMORY = "personal_memory"

@dataclass
class SearchResult:
    """æ¤œç´¢çµæœ"""
    source: SourceType
    title: str
    content: str
    url: Optional[str] = None
    confidence: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict = field(default_factory=dict)

@dataclass
class KnowledgeItem:
    """çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ """
    content: str
    embedding: np.ndarray
    source: SourceType
    metadata: Dict = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    access_count: int = 0
    last_accessed: datetime = field(default_factory=datetime.now)

class MultiSearchAgent:
    """ãƒãƒ«ãƒæ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.name = "multi_search_agent"
        self.description = "è¤‡æ•°ã®æƒ…å ±æºã‹ã‚‰æ¤œç´¢ãƒ»çµ±åˆã™ã‚‹é«˜åº¦æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ "
        
        # æ¤œç´¢ã‚½ãƒ¼ã‚¹è¨­å®š
        self.search_sources = {
            SourceType.DUCKDUCKGO: self._search_duckduckgo,
            SourceType.ARXIV: self._search_arxiv,
            SourceType.GITHUB: self._search_github
        }
        
        # æ¤œç´¢çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.search_cache = {}
        self.cache_ttl = 3600  # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
        # æ¤œç´¢çµ±è¨ˆ
        self.search_stats = defaultdict(int)
    
    async def search_all_sources(self, query: str, max_results_per_source: int = 5) -> List[SearchResult]:
        """ã™ã¹ã¦ã®æƒ…å ±æºã‹ã‚‰æ¤œç´¢"""
        all_results = []
        
        # ä¸¦åˆ—æ¤œç´¢
        tasks = []
        for source_type, search_func in self.search_sources.items():
            task = asyncio.create_task(self._safe_search(search_func, query, source_type, max_results_per_source))
            tasks.append(task)
        
        # çµæœã‚’å¾…æ©Ÿ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµæœã‚’é›†ç´„
        for result in results:
            if isinstance(result, list):
                all_results.extend(result)
            elif isinstance(result, Exception):
                print(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(result)}")
        
        # ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        all_results.sort(key=lambda x: x.confidence, reverse=True)
        
        # çµ±è¨ˆæ›´æ–°
        self.search_stats['total_searches'] += 1
        for result in all_results:
            self.search_stats[f'source_{result.source.value}'] += 1
        
        return all_results[:20]  # ä¸Šä½20ä»¶ã‚’è¿”å´
    
    async def _safe_search(self, search_func, query: str, source_type: SourceType, max_results: int) -> List[SearchResult]:
        """å®‰å…¨ãªæ¤œç´¢å®Ÿè¡Œ"""
        try:
            return await search_func(query, max_results)
        except Exception as e:
            print(f"{source_type.value}æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    async def _search_duckduckgo(self, query: str, max_results: int) -> List[SearchResult]:
        """DuckDuckGoæ¤œç´¢"""
        try:
            # DuckDuckGo HTMLæ¤œç´¢API
            url = "https://html.duckduckgo.com/html/"
            params = {
                'q': query,
                'kl': 'jp-jp'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        
                        # çµæœã‚’è§£æ
                        results = []
                        # ç°¡æ˜“çš„ãªHTMLè§£æï¼ˆå®Ÿéš›ã¯BeautifulSoupã‚’ä½¿ç”¨ï¼‰
                        matches = re.findall(r'<a[^>]*class="result__a"[^>]*href="([^"]*)"[^>]*>([^<]*)</a>', html)
                        
                        for i, (url, title) in enumerate(matches[:max_results]):
                            # å†…å®¹ã®æŠœç²‹ã‚’å–å¾—
                            content_match = re.search(f'<a[^>]*href="{re.escape(url)}"[^>]*>.*?</a>.*?<a[^>]*class="result__snippet"[^>]*>([^<]*)</a>', html, re.DOTALL)
                            content = content_match.group(1) if content_match else title
                            
                            results.append(SearchResult(
                                source=SourceType.DUCKDUCKGO,
                                title=title.strip(),
                                content=content.strip(),
                                url=url,
                                confidence=0.8
                            ))
                        
                        return results
            
        except Exception as e:
            print(f"DuckDuckGoæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return []
    
    async def _search_arxiv(self, query: str, max_results: int) -> List[SearchResult]:
        """arXivæ¤œç´¢"""
        try:
            # arXiv API
            url = "http://export.arxiv.org/api/query"
            params = {
                'search_query': f'all:"{query}"',
                'start': 0,
                'max_results': max_results,
                'sortBy': 'relevance',
                'sortOrder': 'descending'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=10) as response:
                    if response.status == 200:
                        xml = await response.text()
                        
                        # XMLè§£æ
                        results = []
                        entries = re.findall(r'<entry>.*?</entry>', xml, re.DOTALL)
                        
                        for entry in entries[:max_results]:
                            title_match = re.search(r'<title>([^<]*)</title>', entry)
                            summary_match = re.search(r'<summary>([^<]*)</summary>', entry)
                            id_match = re.search(r'<id>([^<]*)</id>', entry)
                            
                            if title_match and summary_match:
                                results.append(SearchResult(
                                    source=SourceType.ARXIV,
                                    title=title_match.group(1).strip(),
                                    content=summary_match.group(1).strip(),
                                    url=id_match.group(1).strip() if id_match else None,
                                    confidence=0.9,
                                    metadata={'type': 'academic_paper'}
                                ))
                        
                        return results
            
        except Exception as e:
            print(f"arXivæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return []
    
    async def _search_github(self, query: str, max_results: int) -> List[SearchResult]:
        """GitHubæ¤œç´¢"""
        try:
            # GitHub APIï¼ˆèªè¨¼ãªã—ã®å ´åˆåˆ¶é™ã‚ã‚Šï¼‰
            url = "https://api.github.com/search/repositories"
            params = {
                'q': query,
                'sort': 'stars',
                'order': 'desc',
                'per_page': max_results
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        results = []
                        for item in data.get('items', [])[:max_results]:
                            results.append(SearchResult(
                                source=SourceType.GITHUB,
                                title=item.get('name', ''),
                                content=item.get('description', ''),
                                url=item.get('html_url'),
                                confidence=0.7,
                                metadata={
                                    'stars': item.get('stargazers_count', 0),
                                    'language': item.get('language', ''),
                                    'updated_at': item.get('updated_at', '')
                                }
                            ))
                        
                        return results
            
        except Exception as e:
            print(f"GitHubæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return []

class SelfReflectionSystem:
    """è‡ªå·±æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.name = "self_reflection"
        self.description = "AIå›ç­”ã®è‡ªå·±æ¤œè¨¼ã¨æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ "
        
        # æ¤œè¨¼åŸºæº–
        self.validation_criteria = {
            'factual_accuracy': 'äº‹å®Ÿã®æ­£ç¢ºæ€§',
            'logical_consistency': 'è«–ç†çš„ä¸€è²«æ€§',
            'source_reliability': 'æƒ…å ±æºã®ä¿¡é ¼æ€§',
            'completeness': 'å›ç­”ã®å®Œå…¨æ€§',
            'clarity': 'æ˜ç¢ºã•'
        }
        
        # æ¤œè¨¼å±¥æ­´
        self.reflection_history = []
    
    def reflect_on_answer(self, answer: str, sources: List[SearchResult], original_query: str) -> Dict:
        """å›ç­”ã®è‡ªå·±æ¤œè¨¼"""
        reflection_result = {
            'original_answer': answer,
            'validation_scores': {},
            'issues_found': [],
            'improvements_suggested': [],
            'confidence_score': 0.0,
            'needs_revision': False,
            'timestamp': datetime.now()
        }
        
        # å„åŸºæº–ã§æ¤œè¨¼
        for criterion, description in self.validation_criteria.items():
            score = self._validate_criterion(criterion, answer, sources, original_query)
            reflection_result['validation_scores'][criterion] = {
                'score': score,
                'description': description,
                'status': 'good' if score >= 0.7 else 'needs_improvement'
            }
        
        # å•é¡Œç‚¹ã®ç‰¹å®š
        issues = self._identify_issues(reflection_result['validation_scores'])
        reflection_result['issues_found'] = issues
        
        # æ”¹å–„ææ¡ˆ
        if issues:
            improvements = self._suggest_improvements(issues, answer, sources, original_query)
            reflection_result['improvements_suggested'] = improvements
            reflection_result['needs_revision'] = True
        
        # å…¨ä½“çš„ãªä¿¡é ¼åº¦
        scores = [v['score'] for v in reflection_result['validation_scores'].values()]
        reflection_result['confidence_score'] = np.mean(scores)
        
        # å±¥æ­´ã«ä¿å­˜
        self.reflection_history.append(reflection_result)
        
        return reflection_result
    
    def _validate_criterion(self, criterion: str, answer: str, sources: List[SearchResult], query: str) -> float:
        """å€‹åˆ¥åŸºæº–ã®æ¤œè¨¼"""
        if criterion == 'factual_accuracy':
            return self._check_factual_accuracy(answer, sources)
        elif criterion == 'logical_consistency':
            return self._check_logical_consistency(answer)
        elif criterion == 'source_reliability':
            return self._check_source_reliability(sources)
        elif criterion == 'completeness':
            return self._check_completeness(answer, query)
        elif criterion == 'clarity':
            return self._check_clarity(answer)
        
        return 0.5
    
    def _check_factual_accuracy(self, answer: str, sources: List[SearchResult]) -> float:
        """äº‹å®Ÿã®æ­£ç¢ºæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        if not sources:
            return 0.3
        
        # æƒ…å ±æºã®ä¿¡é ¼æ€§ã‚’è€ƒæ…®
        reliable_sources = [s for s in sources if s.confidence >= 0.7]
        if not reliable_sources:
            return 0.4
        
        # å›ç­”ãŒæƒ…å ±æºã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã‚’ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
        accuracy_score = 0.0
        for source in reliable_sources:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸€è‡´åº¦ã‚’ãƒã‚§ãƒƒã‚¯
            source_words = set(source.content.lower().split())
            answer_words = set(answer.lower().split())
            
            if source_words:
                overlap = len(source_words & answer_words) / len(source_words)
                accuracy_score = max(accuracy_score, overlap)
        
        return min(1.0, accuracy_score + 0.3)  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ 
    
    def _check_logical_consistency(self, answer: str) -> float:
        """è«–ç†çš„ä¸€è²«æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        # çŸ›ç›¾è¡¨ç¾ã®æ¤œå‡º
        contradiction_patterns = [
            r'ã—ã‹ã—.*ã—ã‹ã—',
            r'ã ãŒ.*ã ãŒ',
            r'.*ã§ã¯ãªã„.*ã§ã™',
            r'å¸¸ã«.*æ™‚ã€…',
            r'ã™ã¹ã¦.*ãªã„'
        ]
        
        contradictions = 0
        for pattern in contradiction_patterns:
            if re.search(pattern, answer):
                contradictions += 1
        
        # çŸ›ç›¾ãŒå°‘ãªã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
        consistency_score = max(0.0, 1.0 - (contradictions * 0.2))
        
        return consistency_score
    
    def _check_source_reliability(self, sources: List[SearchResult]) -> float:
        """æƒ…å ±æºã®ä¿¡é ¼æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        if not sources:
            return 0.3
        
        # æƒ…å ±æºã‚¿ã‚¤ãƒ—ã”ã¨ã®ä¿¡é ¼æ€§
        reliability_weights = {
            SourceType.ARXIV: 0.9,
            SourceType.DUCKDUCKGO: 0.7,
            SourceType.GITHUB: 0.6,
            SourceType.LOCAL_KNOWLEDGE: 0.8,
            SourceType.PERSONAL_MEMORY: 0.5
        }
        
        total_weight = 0.0
        total_reliability = 0.0
        
        for source in sources:
            weight = reliability_weights.get(source.source, 0.5)
            total_weight += weight
            total_reliability += weight * source.confidence
        
        if total_weight > 0:
            return total_reliability / total_weight
        
        return 0.5
    
    def _check_completeness(self, answer: str, query: str) -> float:
        """å›ç­”ã®å®Œå…¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        # ã‚¯ã‚¨ãƒªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå›ç­”ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹
        query_words = set(query.lower().split())
        answer_words = set(answer.lower().split())
        
        if not query_words:
            return 0.5
        
        coverage = len(query_words & answer_words) / len(query_words)
        
        # å›ç­”ã®é•·ã•ã‚‚è€ƒæ…®
        length_factor = min(1.0, len(answer) / 200)  # 200æ–‡å­—ä»¥ä¸Šã§æº€ç‚¹
        
        return (coverage * 0.7) + (length_factor * 0.3)
    
    def _check_clarity(self, answer: str) -> float:
        """æ˜ç¢ºã•ã‚’ãƒã‚§ãƒƒã‚¯"""
        # æ–‡ç« ã®æ§‹é€ ã‚’è©•ä¾¡
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', answer)
        if not sentences:
            return 0.3
        
        # å¹³å‡æ–‡é•·
        avg_sentence_length = np.mean([len(s.strip()) for s in sentences if s.strip()])
        length_score = 1.0 if 10 <= avg_sentence_length <= 100 else 0.5
        
        # å°‚é–€ç”¨èªã®éåº¦ãªä½¿ç”¨ãƒã‚§ãƒƒã‚¯
        technical_terms = ['API', 'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ', 'ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯', 'ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£']
        tech_ratio = sum(1 for term in technical_terms if term in answer) / len(sentences)
        clarity_score = max(0.0, 1.0 - (tech_ratio * 0.2))
        
        return (length_score + clarity_score) / 2
    
    def _identify_issues(self, validation_scores: Dict) -> List[Dict]:
        """å•é¡Œç‚¹ã‚’ç‰¹å®š"""
        issues = []
        
        for criterion, score_info in validation_scores.items():
            if score_info['score'] < 0.7:
                issues.append({
                    'criterion': criterion,
                    'description': score_info['description'],
                    'score': score_info['score'],
                    'severity': 'high' if score_info['score'] < 0.5 else 'medium'
                })
        
        return issues
    
    def _suggest_improvements(self, issues: List[Dict], answer: str, sources: List[SearchResult], query: str) -> List[str]:
        """æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        improvements = []
        
        for issue in issues:
            criterion = issue['criterion']
            
            if criterion == 'factual_accuracy':
                improvements.append("ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æºã‚’å‚ç…§ã—ã¦ã€äº‹å®Ÿé–¢ä¿‚ã‚’å†ç¢ºèªã—ã¦ãã ã•ã„")
            elif criterion == 'logical_consistency':
                improvements.append("å›ç­”å…¨ä½“ã®è«–ç†çš„ãªä¸€è²«æ€§ã‚’ç¢ºèªã—ã€çŸ›ç›¾ã™ã‚‹è¡¨ç¾ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„")
            elif criterion == 'source_reliability':
                improvements.append("å­¦è¡“è«–æ–‡ã‚„å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãªã©ã€ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æºã‚’è¿½åŠ ã§æ¤œç´¢ã—ã¦ãã ã•ã„")
            elif criterion == 'completeness':
                improvements.append("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ã‚ˆã‚Šå®Œå…¨ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„")
            elif criterion == 'clarity':
                improvements.append("å›ç­”ã‚’ã‚ˆã‚Šæ˜ç¢ºã«ã€åˆ†ã‹ã‚Šã‚„ã™ã„è¡¨ç¾ã«ä¿®æ­£ã—ã¦ãã ã•ã„")
        
        return improvements

class AdvancedRAGSystem:
    """é«˜åº¦RAGã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, knowledge_base_path: str = "./knowledge_base"):
        self.name = "advanced_rag"
        self.description = "å®Œå…¨çµ±åˆRAGã‚·ã‚¹ãƒ†ãƒ "
        self.knowledge_base_path = Path(knowledge_base_path)
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.embedding_dim = 384
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.index = None
        self.knowledge_items = []
        
        # åˆæœŸåŒ–
        self._initialize_system()
    
    def _initialize_system(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
        self.knowledge_base_path.mkdir(exist_ok=True)
        
        # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
        self._load_index()
        
        # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚­ãƒ£ãƒ³
        self._scan_knowledge_base()
    
    def _load_index(self):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿"""
        index_file = self.knowledge_base_path / "faiss_index.bin"
        items_file = self.knowledge_base_path / "knowledge_items.pkl"
        
        if index_file.exists() and items_file.exists():
            try:
                self.index = faiss.read_index(str(index_file))
                with open(items_file, 'rb') as f:
                    self.knowledge_items = pickle.load(f)
                print(f"âœ… æ—¢å­˜ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿: {len(self.knowledge_items)}ä»¶")
            except Exception as e:
                print(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                self._create_new_index()
        else:
            self._create_new_index()
    
    def _create_new_index(self):
        """æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ"""
        self.index = faiss.IndexFlatL2(self.embedding_dim)
        self.knowledge_items = []
    
    def _scan_knowledge_base(self):
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’ã‚¹ã‚­ãƒ£ãƒ³"""
        if not self.knowledge_base_path.exists():
            return
        
        # ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
        supported_extensions = {'.txt', '.md', '.py', '.js', '.html', '.css', '.json'}
        
        for file_path in self.knowledge_base_path.rglob('*'):
            if file_path.is_file() and file_path.suffix in supported_extensions:
                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    if len(content.strip()) > 10:  # çŸ­ã™ãã‚‹å†…å®¹ã¯ç„¡è¦–
                        # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
                        chunks = self._split_content(content)
                        
                        for chunk in chunks:
                            self._add_knowledge_item(
                                content=chunk,
                                source=SourceType.LOCAL_KNOWLEDGE,
                                metadata={
                                    'file_path': str(file_path),
                                    'file_type': file_path.suffix,
                                    'original_file': file_path.name
                                }
                            )
                
                except Exception as e:
                    print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {str(e)}")
    
    def _split_content(self, content: str, chunk_size: int = 500) -> List[str]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        chunks = []
        
        # æ®µè½ã§åˆ†å‰²
        paragraphs = content.split('\n\n')
        
        current_chunk = ""
        for paragraph in paragraphs:
            if len(current_chunk + paragraph) <= chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = paragraph + "\n\n"
                else:
                    # é•·ã„æ®µè½ã¯æ–‡ã§åˆ†å‰²
                    sentences = paragraph.split('ã€‚')
                    for sentence in sentences:
                        if len(current_chunk + sentence) <= chunk_size:
                            current_chunk += sentence + "ã€‚"
                        else:
                            if current_chunk:
                                chunks.append(current_chunk.strip())
                                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _add_knowledge_item(self, content: str, source: SourceType, metadata: Dict = None):
        """çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿½åŠ """
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        embedding = self.embedding_model.encode([content])[0]
        
        # çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ ä½œæˆ
        item = KnowledgeItem(
            content=content,
            embedding=embedding,
            source=source,
            metadata=metadata or {}
        )
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
        self.index.add(np.array([embedding]).astype('float32'))
        self.knowledge_items.append(item)
    
    def search_knowledge(self, query: str, top_k: int = 5) -> List[SearchResult]:
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ¤œç´¢"""
        if len(self.knowledge_items) == 0:
            return []
        
        # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿
        query_embedding = self.embedding_model.encode([query])[0]
        query_embedding = np.array([query_embedding]).astype('float32')
        
        # æ¤œç´¢
        distances, indices = self.index.search(query_embedding, min(top_k, len(self.knowledge_items)))
        
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.knowledge_items):
                item = self.knowledge_items[idx]
                
                # ã‚¢ã‚¯ã‚»ã‚¹çµ±è¨ˆæ›´æ–°
                item.access_count += 1
                item.last_accessed = datetime.now()
                
                # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
                similarity = 1.0 / (1.0 + distance)
                
                results.append(SearchResult(
                    source=item.source,
                    title=item.metadata.get('original_file', 'ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è­˜'),
                    content=item.content,
                    confidence=similarity,
                    metadata=item.metadata
                ))
        
        return results
    
    def add_personal_memory(self, content: str, metadata: Dict = None):
        """å€‹äººãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ """
        self._add_knowledge_item(
            content=content,
            source=SourceType.PERSONAL_MEMORY,
            metadata=metadata or {'type': 'personal_memory'}
        )
        
        # å®šæœŸçš„ãªä¿å­˜
        self._save_index()
    
    def _save_index(self):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜"""
        try:
            index_file = self.knowledge_base_path / "faiss_index.bin"
            items_file = self.knowledge_base_path / "knowledge_items.pkl"
            
            faiss.write_index(self.index, str(index_file))
            with open(items_file, 'wb') as f:
                pickle.dump(self.knowledge_items, f)
            
            print(f"âœ… ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜: {len(self.knowledge_items)}ä»¶")
        except Exception as e:
            print(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")

class LongContextManager:
    """é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†"""
    
    def __init__(self, max_context_length: int = 8000):
        self.name = "long_context_manager"
        self.description = "é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ "
        self.max_context_length = max_context_length
        
        # ä¼šè©±å±¥æ­´
        self.conversation_history = []
        self.summaries = []
        
        # è¦ç´„ãƒ¢ãƒ‡ãƒ«
        self.summarization_threshold = 10  # 10ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã”ã¨ã«è¦ç´„
    
    def add_message(self, role: str, content: str, timestamp: datetime = None):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ """
        message = {
            'role': role,
            'content': content,
            'timestamp': timestamp or datetime.now()
        }
        
        self.conversation_history.append(message)
        
        # è¦ç´„ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
        if len(self.conversation_history) >= self.summarization_threshold:
            self._create_summary()
    
    def _create_summary(self):
        """è¦ç´„ã‚’ä½œæˆ"""
        if len(self.conversation_history) < self.summarization_threshold:
            return
        
        # æœ€è¿‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¦ç´„
        recent_messages = self.conversation_history[-self.summarization_threshold:]
        
        # ç°¡æ˜“çš„ãªè¦ç´„ï¼ˆå®Ÿéš›ã¯è¦ç´„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
        summary_content = self._generate_summary(recent_messages)
        
        summary = {
            'content': summary_content,
            'message_count': len(recent_messages),
            'timestamp': datetime.now(),
            'key_topics': self._extract_key_topics(recent_messages)
        }
        
        self.summaries.append(summary)
        
        # å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤
        self.conversation_history = self.conversation_history[:-self.summarization_threshold//2]
    
    def _generate_summary(self, messages: List[Dict]) -> str:
        """è¦ç´„ã‚’ç”Ÿæˆ"""
        # ç°¡æ˜“çš„ãªè¦ç´„ãƒ­ã‚¸ãƒƒã‚¯
        user_messages = [m['content'] for m in messages if m['role'] == 'user']
        assistant_messages = [m['content'] for m in messages if m['role'] == 'assistant']
        
        summary = f"ä¼šè©±è¦ç´„ï¼ˆ{len(messages)}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰:\n"
        
        if user_messages:
            summary += f"ä¸»ãªè³ªå•: {user_messages[0][:100]}...\n"
        
        if assistant_messages:
            summary += f"ä¸»ãªå›ç­”: {assistant_messages[0][:100]}...\n"
        
        summary += f"ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯: æŠ€è¡“é–‹ç™ºã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€å•é¡Œè§£æ±º"
        
        return summary
    
    def _extract_key_topics(self, messages: List[Dict]) -> List[str]:
        """ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º"""
        # ç°¡æ˜“çš„ãªãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
        all_content = " ".join([m['content'] for m in messages])
        
        # æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        tech_keywords = ['Python', 'JavaScript', 'API', 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹', 'AI', 'æ©Ÿæ¢°å­¦ç¿’', 'Webé–‹ç™º']
        topics = [keyword for keyword in tech_keywords if keyword.lower() in all_content.lower()]
        
        return topics[:5]  # ä¸Šä½5ãƒˆãƒ”ãƒƒã‚¯
    
    def get_context_summary(self) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ã‚’å–å¾—"""
        if not self.summaries:
            return ""
        
        # æœ€è¿‘ã®è¦ç´„ã‚’çµåˆ
        recent_summaries = self.summaries[-3:]  # æœ€è¿‘3ã¤ã®è¦ç´„
        
        context = "ã“ã‚Œã¾ã§ã®ä¼šè©±ã®è¦ç´„:\n"
        for i, summary in enumerate(recent_summaries, 1):
            context += f"{i}. {summary['content']}\n"
        
        return context
    
    def get_full_context(self) -> str:
        """ãƒ•ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        context_parts = []
        
        # è¦ç´„ã‚’è¿½åŠ 
        if self.summaries:
            context_parts.append(self.get_context_summary())
        
        # æœ€è¿‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        recent_messages = self.conversation_history[-5:]  # æœ€è¿‘5ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        for message in recent_messages:
            role_emoji = "ğŸ‘¤" if message['role'] == 'user' else "ğŸ¤–"
            context_parts.append(f"{role_emoji} {message['content']}")
        
        return "\n".join(context_parts)

class AdvancedKnowledgeSystem:
    """é«˜åº¦çŸ¥è­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ"""
    
    def __init__(self):
        self.name = "advanced_knowledge_system"
        self.description = "æƒ…å ±ã®æ­£ç¢ºæ€§ã¨å›ç­”ã®æ·±ã•ã‚’æ¥µå¤§åŒ–ã™ã‚‹çµ±åˆã‚·ã‚¹ãƒ†ãƒ "
        
        # ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ 
        self.multi_search = MultiSearchAgent()
        self.self_reflection = SelfReflectionSystem()
        self.rag_system = AdvancedRAGSystem()
        self.context_manager = LongContextManager()
        
        # çŸ¥è­˜çµ±åˆè¨­å®š
        self.source_priorities = {
            SourceType.LOCAL_KNOWLEDGE: 1.0,
            SourceType.ARXIV: 0.9,
            SourceType.DUCKDUCKGO: 0.7,
            SourceType.GITHUB: 0.6,
            SourceType.PERSONAL_MEMORY: 0.8
        }
    
    async def process_query(self, query: str, use_context: bool = True) -> Dict:
        """ã‚¯ã‚¨ãƒªå‡¦ç†"""
        start_time = time.time()
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
        context = ""
        if use_context:
            context = self.context_manager.get_full_context()
        
        # ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹æ¤œç´¢
        search_results = await self.multi_search.search_all_sources(query)
        
        # RAGæ¤œç´¢
        rag_results = self.rag_system.search_knowledge(query)
        
        # ã™ã¹ã¦ã®çµæœã‚’çµ±åˆ
        all_sources = search_results + rag_results
        
        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
        all_sources.sort(key=lambda x: (
            self.source_priorities.get(x.source, 0.5) * x.confidence
        ), reverse=True)
        
        return {
            'query': query,
            'context': context,
            'sources': all_sources[:10],  # ä¸Šä½10ä»¶
            'processing_time': time.time() - start_time,
            'source_counts': {
                source.value: len([s for s in all_sources if s.source == source])
                for source in SourceType
            }
        }
    
    def reflect_and_improve(self, answer: str, sources: List[SearchResult], query: str) -> Dict:
        """è‡ªå·±æ¤œè¨¼ã¨æ”¹å–„"""
        reflection = self.self_reflection.reflect_on_answer(answer, sources, query)
        
        # æ”¹å–„ãŒå¿…è¦ãªå ´åˆ
        if reflection['needs_revision']:
            # æ”¹å–„ææ¡ˆã‚’å…ƒã«å†æ¤œç´¢
            improvement_queries = self._generate_improvement_queries(reflection['improvements_suggested'])
            
            # è¿½åŠ æ¤œç´¢ï¼ˆéåŒæœŸï¼‰
            # asyncio.create_task(self._additional_search(improvement_queries))
        
        return reflection
    
    def _generate_improvement_queries(self, improvements: List[str]) -> List[str]:
        """æ”¹å–„ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ"""
        queries = []
        
        for improvement in improvements:
            if "ä¿¡é ¼æ€§" in improvement:
                queries.append("å­¦è¡“è«–æ–‡ å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
            elif "å®Œå…¨æ€§" in improvement:
                queries.append("è©³ç´°ãªæƒ…å ± å…·ä½“çš„ãªæ–¹æ³•")
            elif "äº‹å®Ÿé–¢ä¿‚" in improvement:
                queries.append("å…¬å¼æƒ…å ± æ­£ç¢ºãªãƒ‡ãƒ¼ã‚¿")
        
        return queries
    
    def add_conversation_message(self, role: str, content: str):
        """ä¼šè©±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ """
        self.context_manager.add_message(role, content)
    
    def get_system_prompt_enhancement(self) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‹¡å¼µ"""
        context_summary = self.context_manager.get_context_summary()
        
        if context_summary:
            return f"\n\nã€ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{context_summary}\n\nã“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        
        return ""
    
    def get_statistics(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            'multi_search': self.multi_search.search_stats,
            'self_reflection': {
                'total_reflections': len(self.self_reflection.reflection_history),
                'average_confidence': np.mean([
                    r['confidence_score'] for r in self.self_reflection.reflection_history
                ]) if self.self_reflection.reflection_history else 0.0
            },
            'rag_system': {
                'knowledge_items': len(self.rag_system.knowledge_items),
                'personal_memories': len([
                    item for item in self.rag_system.knowledge_items
                    if item.source == SourceType.PERSONAL_MEMORY
                ])
            },
            'context_manager': {
                'conversation_length': len(self.context_manager.conversation_history),
                'summary_count': len(self.context_manager.summaries)
            }
        }

# Streamlit GUIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
def create_advanced_knowledge_gui(advanced_system: AdvancedKnowledgeSystem):
    """é«˜åº¦çŸ¥è­˜ã‚·ã‚¹ãƒ†ãƒ GUI"""
    st.subheader("ğŸ§  é«˜åº¦çŸ¥è­˜ã‚·ã‚¹ãƒ†ãƒ ")
    
    # çµ±è¨ˆæƒ…å ±
    stats = advanced_system.get_statistics()
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ ",
            stats['rag_system']['knowledge_items'],
            help="ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®ã‚¢ã‚¤ãƒ†ãƒ æ•°"
        )
    
    with col2:
        st.metric(
            "è‡ªå·±æ¤œè¨¼å›æ•°",
            stats['self_reflection']['total_reflections'],
            help="å®Ÿè¡Œã•ã‚ŒãŸè‡ªå·±æ¤œè¨¼ã®å›æ•°"
        )
    
    with col3:
        st.metric(
            "å¹³å‡ä¿¡é ¼åº¦",
            f"{stats['self_reflection']['average_confidence']:.2f}",
            help="å›ç­”ã®å¹³å‡ä¿¡é ¼åº¦"
        )
    
    with col4:
        st.metric(
            "ä¼šè©±é•·",
            stats['context_manager']['conversation_length'],
            help="ç¾åœ¨ã®ä¼šè©±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°"
        )
    
    # æ¤œç´¢ã‚½ãƒ¼ã‚¹åˆ†å¸ƒ
    if stats['multi_search']:
        st.write("**æ¤œç´¢ã‚½ãƒ¼ã‚¹åˆ†å¸ƒ**")
        for source, count in stats['multi_search'].items():
            if source.startswith('source_') and count > 0:
                source_name = source.replace('source_', '').title()
                st.write(f"- {source_name}: {count}å›")
    
    # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ç®¡ç†
    st.write("**ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ç®¡ç†**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ”„ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹å†ã‚¹ã‚­ãƒ£ãƒ³"):
            advanced_system.rag_system._scan_knowledge_base()
            st.success("ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’å†ã‚¹ã‚­ãƒ£ãƒ³ã—ã¾ã—ãŸ")
    
    with col2:
        if st.button("ğŸ’¾ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜"):
            advanced_system.rag_system._save_index()
            st.success("ğŸ’¾ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    
    # å€‹äººãƒ¡ãƒ¢ãƒªè¿½åŠ 
    st.write("**å€‹äººãƒ¡ãƒ¢ãƒªè¿½åŠ **")
    memory_content = st.text_area("ãƒ¡ãƒ¢ãƒªã™ã‚‹å†…å®¹", height=100)
    
    if st.button("ğŸ§  ãƒ¡ãƒ¢ãƒªè¿½åŠ ") and memory_content:
        advanced_system.rag_system.add_personal_memory(
            memory_content,
            {'type': 'manual_addition', 'timestamp': datetime.now().isoformat()}
        )
        st.success("ğŸ§  å€‹äººãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„è¡¨ç¤º
    if st.button("ğŸ“ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„"):
        summary = advanced_system.context_manager.get_context_summary()
        if summary:
            st.info(summary)
        else:
            st.info("ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ãŒã‚ã‚Šã¾ã›ã‚“")
    
    # è©³ç´°çµ±è¨ˆ
    if st.button("ğŸ“Š è©³ç´°çµ±è¨ˆ"):
        st.json(stats)
