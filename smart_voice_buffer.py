#!/usr/bin/env python3
"""
ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
ä¼šè©±ã®é€”åˆ‡ã‚Œã‚’é˜²ãã€é«˜ã„åŒ…å®¹åŠ›ã‚’æŒã¤AIéŸ³å£°å…¥åŠ›
"""

import streamlit as st
import numpy as np
import librosa
import pyaudio
import threading
import time
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import webrtcvad
from faster_whisper import WhisperModel
from collections import deque
import queue

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›¸æ§Œã‚·ã‚¹ãƒ†ãƒ 
from realtime_aizuchi import RealTimeAizuchiSystem

class SmartVoiceBuffer:
    """ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.name = "smart_voice_buffer"
        self.description = "ä¼šè©±ã®é€”åˆ‡ã‚Œã‚’é˜²ãã‚¹ãƒãƒ¼ãƒˆéŸ³å£°ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°"
        
        # éŸ³å£°éŒ²éŸ³è¨­å®š
        self.format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        self.chunk = 1024
        
        # VADè¨­å®š
        self.vad = webrtcvad.Vad(2)  # ä¸­ç¨‹åº¦ã®æ„Ÿåº¦
        
        # Whisperãƒ¢ãƒ‡ãƒ«
        self.whisper_model = None
        self.model_loaded = False
        
        # ã‚¹ãƒãƒ¼ãƒˆãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.silence_threshold = 2.0  # 2ç§’ã®ç„¡éŸ³ã§ä¼šè©±çµ‚äº†ã¨åˆ¤å®š
        self.continuation_threshold = 2.0  # 2ç§’ä»¥å†…ã®å†é–‹å§‹ã¯ç¶™ç¶šã¨ã¿ãªã™
        self.nodding_threshold = 1.0  # 1ç§’ã®ç„¡éŸ³ã§ç›¸æ§Œ
        
        # ãƒãƒƒãƒ•ã‚¡ç®¡ç†
        self.audio_buffer = []
        self.is_speaking = False
        self.last_speech_time = None
        self.conversation_active = False
        self.waiting_for_continuation = False
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ç®¡ç†
        self.listening_thread = None
        self.buffer_thread = None
        self.is_listening = False
        
        # çŠ¶æ…‹ç®¡ç†
        self.current_status = "å¾…æ©Ÿä¸­"
        self.status_messages = {
            "listening": "ğŸ§ èã„ã¦ã„ã¾ã™...",
            "waiting": "ğŸ¤” ã¾ã èã„ã¦ã‚‹ã‚ˆ...",
            "processing": "ğŸ¤– å‡¦ç†ä¸­ã§ã™...",
            "nodding": "ğŸ˜Š ã†ã‚“ã€ã†ã‚“...",
            "aizuchi": "ğŸ‘‚ ç›¸æ§Œä¸­...",
            "ready": "âœ… æº–å‚™å®Œäº†"
        }
        
        # çµæœä¿å­˜
        self.last_recognition_result = None
        self.conversation_history = []
        
        # GUIæ›´æ–°ç”¨ã‚­ãƒ¥ãƒ¼
        self.gui_update_queue = queue.Queue()
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›¸æ§Œã‚·ã‚¹ãƒ†ãƒ 
        self.aizuchi_system = RealTimeAizuchiSystem()
        
        self._load_whisper_model()
    
    def _load_whisper_model(self):
        """Whisperãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            self.whisper_model = WhisperModel("large-v3", compute_type="int8")
            self.model_loaded = True
            print("âœ… Faster-Whisper large-v3 ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        except Exception as e:
            print(f"âŒ Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦baseãƒ¢ãƒ‡ãƒ«
            try:
                self.whisper_model = WhisperModel("base", compute_type="int8")
                self.model_loaded = True
                print("âœ… Whisper base ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
            except Exception as e2:
                print(f"âŒ Whisper base ãƒ¢ãƒ‡ãƒ«ã‚‚èª­ã¿è¾¼ã¿å¤±æ•—: {str(e2)}")
                self.model_loaded = False
    
    def start_smart_listening(self):
        """ã‚¹ãƒãƒ¼ãƒˆè´å–ã‚’é–‹å§‹"""
        if not self.is_listening:
            self.is_listening = True
            self.conversation_active = False
            self.waiting_for_continuation = False
            
            # ç›¸æ§Œã‚·ã‚¹ãƒ†ãƒ ã‚‚é–‹å§‹
            self.aizuchi_system.start_aizuchi_system()
            
            # è´å–ã‚¹ãƒ¬ãƒƒãƒ‰
            self.listening_thread = threading.Thread(target=self._smart_listening_loop, daemon=True)
            self.listening_thread.start()
            
            # ãƒãƒƒãƒ•ã‚¡å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰
            self.buffer_thread = threading.Thread(target=self._buffer_management_loop, daemon=True)
            self.buffer_thread.start()
            
            return True
        return False
    
    def stop_smart_listening(self):
        """ã‚¹ãƒãƒ¼ãƒˆè´å–ã‚’åœæ­¢"""
        self.is_listening = False
        self.conversation_active = False
        self.waiting_for_continuation = False
        
        # ç›¸æ§Œã‚·ã‚¹ãƒ†ãƒ ã‚‚åœæ­¢
        self.aizuchi_system.stop_aizuchi_system()
        
        return True
    
    def _smart_listening_loop(self):
        """ã‚¹ãƒãƒ¼ãƒˆè´å–ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        p = pyaudio.PyAudio()
        
        try:
            stream = p.open(
                format=self.format,
                channels=self.channels,
                rate=self.rate,
                input=True,
                frames_per_buffer=self.chunk
            )
            
            print("ğŸ§ ã‚¹ãƒãƒ¼ãƒˆè´å–é–‹å§‹...")
            
            while self.is_listening:
                try:
                    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                    data = stream.read(self.chunk)
                    audio_chunk = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
                    
                    # VADã§éŸ³å£°æ¤œå‡º
                    is_speech = self._detect_voice_activity(audio_chunk)
                    
                    current_time = time.time()
                    
                    if is_speech:
                        # éŸ³å£°æ¤œå‡ºæ™‚
                        if not self.is_speaking:
                            # æ–°ã—ã„ç™ºè©±ã®é–‹å§‹
                            self.is_speaking = True
                            self.last_speech_time = current_time
                            
                            # ç¶™ç¶šåˆ¤å®š
                            if (self.waiting_for_continuation and 
                                current_time - self.last_speech_time < self.continuation_threshold):
                                # å‰ã®ç™ºè©±ã®ç¶™ç¶š
                                self.current_status = self.status_messages["listening"]
                                print("ğŸ”„ ç™ºè©±ç¶™ç¶šã‚’æ¤œå‡º")
                            else:
                                # æ–°ã—ã„ä¼šè©±ã®é–‹å§‹
                                self.conversation_active = True
                                self.waiting_for_continuation = False
                                self.current_status = self.status_messages["listening"]
                                print("ğŸ¤ æ–°ã—ã„ç™ºè©±ã‚’æ¤œå‡º")
                        
                        # éŸ³å£°ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                        self.audio_buffer.append(audio_chunk)
                        
                        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›¸æ§Œã‚·ã‚¹ãƒ†ãƒ ã«éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’æ¸¡ã™
                        self.aizuchi_system.process_audio_chunk(audio_chunk)
                        
                    else:
                        # ç„¡éŸ³æ™‚
                        if self.is_speaking:
                            # ç™ºè©±ãŒé€”åˆ‡ã‚ŒãŸ
                            self.is_speaking = False
                            self.last_speech_time = current_time
                            
                            if self.conversation_active:
                                # ä¼šè©±ä¸­ã®é€”åˆ‡ã‚Œ â†’ å¾…æ©ŸçŠ¶æ…‹ã¸
                                self.waiting_for_continuation = True
                                self.current_status = self.status_messages["waiting"]
                                print("â¸ï¸ ç™ºè©±é€”åˆ‡ã‚Œã€å¾…æ©Ÿä¸­...")
                
                except Exception as e:
                    print(f"è´å–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    time.sleep(0.1)
            
        finally:
            stream.stop_stream()
            stream.close()
            p.terminate()
    
    def _buffer_management_loop(self):
        """ãƒãƒƒãƒ•ã‚¡ç®¡ç†ãƒ«ãƒ¼ãƒ—"""
        while self.is_listening:
            try:
                current_time = time.time()
                
                # ç›¸æ§Œåˆ¤å®šï¼ˆ1ç§’ã®ç„¡éŸ³ï¼‰
                if (self.waiting_for_continuation and 
                    self.last_speech_time and 
                    current_time - self.last_speech_time > self.nodding_threshold):
                    
                    # VRMã«ç›¸æ§Œã‚’æŒ‡ç¤º
                    self._send_vrm_command("nodding")
                    self.current_status = self.status_messages["nodding"]
                
                # ä¼šè©±çµ‚äº†åˆ¤å®šï¼ˆ2ç§’ã®ç„¡éŸ³ï¼‰
                if (self.waiting_for_continuation and 
                    self.last_speech_time and 
                    current_time - self.last_speech_time > self.silence_threshold):
                    
                    # ä¼šè©±çµ‚äº†ã¨åˆ¤å®š
                    self._finalize_conversation()
                
                time.sleep(0.1)  # 100msã”ã¨ã«ãƒã‚§ãƒƒã‚¯
                
            except Exception as e:
                print(f"ãƒãƒƒãƒ•ã‚¡ç®¡ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                time.sleep(0.5)
    
    def _finalize_conversation(self):
        """ä¼šè©±ã‚’ç¢ºå®šã—ã¦å‡¦ç†"""
        if len(self.audio_buffer) == 0:
            return
        
        try:
            self.current_status = self.status_messages["processing"]
            print("ğŸ¤– ä¼šè©±ç¢ºå®šã€å‡¦ç†é–‹å§‹...")
            
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            audio_data = np.concatenate(self.audio_buffer)
            
            # Whisperã§èªè­˜
            if self.model_loaded:
                segments, _ = self.whisper_model.transcribe(
                    audio_data, 
                    language="ja",
                    beam_size=5,
                    vad_filter=True
                )
                
                recognized_text = ""
                for segment in segments:
                    recognized_text += segment.text + " "
                
                recognized_text = recognized_text.strip()
                
                if recognized_text:
                    # çµæœã‚’ä¿å­˜
                    self.last_recognition_result = {
                        'text': recognized_text,
                        'timestamp': datetime.now().isoformat(),
                        'duration': len(audio_data) / self.rate,
                        'audio_length': len(self.audio_buffer)
                    }
                    
                    # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
                    self.conversation_history.append(self.last_recognition_result)
                    
                    print(f"âœ… èªè­˜å®Œäº†: {recognized_text}")
                    
                    # GUIã«é€šçŸ¥
                    self.gui_update_queue.put({
                        'type': 'recognition_complete',
                        'text': recognized_text
                    })
                    
                    # VRMã«é€šçŸ¥
                    self._send_vrm_command("recognition_complete", {
                        'text': recognized_text
                    })
                else:
                    print("âš ï¸ èªè­˜çµæœãŒç©ºã§ã—ãŸ")
            
            # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
            self.audio_buffer = []
            self.conversation_active = False
            self.waiting_for_continuation = False
            self.current_status = self.status_messages["ready"]
            
        except Exception as e:
            print(f"ä¼šè©±ç¢ºå®šã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.current_status = "ã‚¨ãƒ©ãƒ¼"
    
    def _detect_voice_activity(self, audio_chunk: np.ndarray) -> bool:
        """éŸ³å£°æ´»å‹•æ¤œå‡º"""
        try:
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’16bitã«å¤‰æ›
            audio_int16 = (audio_chunk * 32767).astype(np.int16)
            
            # VADã§åˆ¤å®š
            is_speech = self.vad.is_speech(audio_int16.tobytes(), self.rate)
            return is_speech
            
        except Exception as e:
            print(f"VADã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _send_vrm_command(self, command: str, data: Dict = None):
        """VRMã‚¢ãƒã‚¿ãƒ¼ã«ã‚³ãƒãƒ³ãƒ‰ã‚’é€ä¿¡"""
        try:
            # VRMé€£æºæ©Ÿèƒ½ã¯ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã§å®Ÿè£…
            if command == "aizuchi":
                # ç›¸æ§Œã‚³ãƒãƒ³ãƒ‰
                emotion = data.get('emotion', 'neutral') if data else 'neutral'
                js_code = f"""
                <script>
                    window.parent.postMessage({{
                        type: 'motion',
                        data: {{ motion: 'aizuchi', emotion: '{emotion}' }}
                    }}, '*');
                </script>
                """
                st.components.v1.html(js_code, height=0)
            
        except Exception as e:
            print(f"VRMã‚³ãƒãƒ³ãƒ‰é€ä¿¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def get_current_status(self) -> Dict:
        """ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        return {
            'status': self.current_status,
            'is_listening': self.is_listening,
            'is_speaking': self.is_speaking,
            'conversation_active': self.conversation_active,
            'waiting_for_continuation': self.waiting_for_continuation,
            'buffer_size': len(self.audio_buffer),
            'last_speech_time': self.last_speech_time,
            'last_result': self.last_recognition_result
        }
    
    def get_gui_updates(self) -> List[Dict]:
        """GUIæ›´æ–°æƒ…å ±ã‚’å–å¾—"""
        updates = []
        try:
            while not self.gui_update_queue.empty():
                updates.append(self.gui_update_queue.get_nowait())
        except queue.Empty:
            pass
        return updates
    
    def manual_record_with_buffer(self, max_duration: int = 30) -> Dict:
        """æ‰‹å‹•éŒ²éŸ³ï¼ˆã‚¹ãƒãƒ¼ãƒˆãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
        try:
            p = pyaudio.PyAudio()
            
            stream = p.open(
                format=self.format,
                channels=self.channels,
                rate=self.rate,
                input=True,
                frames_per_buffer=self.chunk
            )
            
            frames = []
            speech_detected = False
            last_speech_time = time.time()
            start_time = time.time()
            
            print(f"ğŸ¤ ã‚¹ãƒãƒ¼ãƒˆéŒ²éŸ³é–‹å§‹ï¼ˆæœ€å¤§{max_duration}ç§’ï¼‰...")
            
            while time.time() - start_time < max_duration:
                data = stream.read(self.chunk)
                audio_chunk = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
                
                # VADã§éŸ³å£°æ¤œå‡º
                is_speech = self._detect_voice_activity(audio_chunk)
                
                if is_speech:
                    speech_detected = True
                    last_speech_time = time.time()
                    frames.append(audio_chunk)
                elif speech_detected:
                    # éŸ³å£°ãŒæ¤œå‡ºã•ã‚ŒãŸå¾Œã®ç„¡éŸ³
                    if time.time() - last_speech_time > self.silence_threshold:
                        # 2ç§’ã®ç„¡éŸ³ã§éŒ²éŸ³çµ‚äº†
                        break
            
            print("ğŸ¤ éŒ²éŸ³å®Œäº†")
            
            stream.stop_stream()
            stream.close()
            p.terminate()
            
            if len(frames) == 0:
                return {'text': '', 'error': 'éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ'}
            
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            audio_data = np.concatenate(frames)
            
            # Whisperã§èªè­˜
            result = {'text': '', 'duration': len(audio_data) / self.rate}
            
            if self.model_loaded:
                segments, _ = self.whisper_model.transcribe(
                    audio_data, 
                    language="ja",
                    beam_size=5,
                    vad_filter=True
                )
                
                recognized_text = ""
                for segment in segments:
                    recognized_text += segment.text + " "
                
                result['text'] = recognized_text.strip()
                result['timestamp'] = datetime.now().isoformat()
                
                if result['text']:
                    print(f"âœ… èªè­˜çµæœ: {result['text']}")
                else:
                    print("âš ï¸ èªè­˜çµæœãŒç©ºã§ã—ãŸ")
            
            return result
            
        except Exception as e:
            print(f"æ‰‹å‹•éŒ²éŸ³ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'text': '', 'error': str(e)}
    
    def get_conversation_summary(self) -> Dict:
        """ä¼šè©±ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        return {
            'total_conversations': len(self.conversation_history),
            'total_duration': sum(conv.get('duration', 0) for conv in self.conversation_history),
            'average_duration': np.mean([conv.get('duration', 0) for conv in self.conversation_history]) if self.conversation_history else 0,
            'last_conversation': self.conversation_history[-1] if self.conversation_history else None,
            'buffer_efficiency': self._calculate_buffer_efficiency()
        }
    
    def _calculate_buffer_efficiency(self) -> float:
        """ãƒãƒƒãƒ•ã‚¡åŠ¹ç‡ã‚’è¨ˆç®—"""
        if not self.conversation_history:
            return 0.0
        
        # ç¶™ç¶šã•ã‚ŒãŸä¼šè©±ã®å‰²åˆã‚’è¨ˆç®—
        continued_count = 0
        for i, conv in enumerate(self.conversation_history):
            if i > 0:
                # å‰ã®ä¼šè©±ã¨ã®æ™‚é–“é–“éš”ã‚’ãƒã‚§ãƒƒã‚¯
                prev_time = datetime.fromisoformat(self.conversation_history[i-1]['timestamp'])
                curr_time = datetime.fromisoformat(conv['timestamp'])
                time_diff = (curr_time - prev_time).total_seconds()
                
                if time_diff < self.continuation_threshold * 2:  # ç¶™ç¶šã®ç¯„å›²å†…
                    continued_count += 1
        
        return continued_count / len(self.conversation_history) if self.conversation_history else 0.0
    
    def run(self, command: str) -> str:
        """ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        if command == "start_smart_listening":
            if self.start_smart_listening():
                return "ã‚¹ãƒãƒ¼ãƒˆè´å–ã‚’é–‹å§‹ã—ã¾ã—ãŸ"
            else:
                return "ã™ã§ã«è´å–ä¸­ã§ã™"
        
        elif command == "stop_smart_listening":
            if self.stop_smart_listening():
                return "ã‚¹ãƒãƒ¼ãƒˆè´å–ã‚’åœæ­¢ã—ã¾ã—ãŸ"
            else:
                return "è´å–ã—ã¦ã„ã¾ã›ã‚“"
        
        elif command.startswith("smart_record"):
            try:
                parts = command.split()
                duration = int(parts[1]) if len(parts) > 1 else 30
                result = self.manual_record_with_buffer(duration)
                if result.get('text'):
                    return f"èªè­˜çµæœ: {result['text']} (ç¶™ç¶šæ™‚é–“: {result['duration']:.1f}ç§’)"
                else:
                    return f"éŒ²éŸ³ã‚¨ãƒ©ãƒ¼: {result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}"
            except:
                return "éŒ²éŸ³ã‚³ãƒãƒ³ãƒ‰å½¢å¼: smart_record [æœ€å¤§ç§’æ•°]"
        
        elif command == "status":
            status = self.get_current_status()
            return f"ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°çŠ¶æ…‹: {status['status']}, è´å–ä¸­={status['is_listening']}, ä¼šè©±ä¸­={status['conversation_active']}, ãƒãƒƒãƒ•ã‚¡={status['buffer_size']}"
        
        elif command == "summary":
            summary = self.get_conversation_summary()
            return f"ä¼šè©±ã‚µãƒãƒªãƒ¼: ç·æ•°={summary['total_conversations']}, ç·æ™‚é–“={summary['total_duration']:.1f}ç§’, åŠ¹ç‡={summary['buffer_efficiency']:.2f}"
        
        elif command == "last_result":
            if self.last_recognition_result:
                return f"æœ€å¾Œã®èªè­˜: {self.last_recognition_result['text']} ({self.last_recognition_result['duration']:.1f}ç§’)"
            else:
                return "èªè­˜çµæœãŒã‚ã‚Šã¾ã›ã‚“"
        
        else:
            return "ã‚³ãƒãƒ³ãƒ‰å½¢å¼: start_smart_listening, stop_smart_listening, smart_record [ç§’æ•°], status, summary, last_result"

# Streamlit GUIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
def create_smart_voice_gui(smart_buffer: SmartVoiceBuffer):
    """ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°GUIã‚’ä½œæˆ"""
    st.subheader("ğŸ¤ ã‚¹ãƒãƒ¼ãƒˆéŸ³å£°ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°")
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
    status = smart_buffer.get_current_status()
    
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            "ç¾åœ¨ã®çŠ¶æ…‹",
            status['status'],
            help="ç¾åœ¨ã®éŸ³å£°èªè­˜çŠ¶æ…‹"
        )
    
    with col2:
        st.metric(
            "ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º",
            f"{status['buffer_size']} ãƒãƒ£ãƒ³ã‚¯",
            help="ç¾åœ¨ã®éŸ³å£°ãƒãƒƒãƒ•ã‚¡é‡"
        )
    
    with col3:
        is_active = "ä¼šè©±ä¸­" if status['conversation_active'] else "å¾…æ©Ÿä¸­"
        st.metric(
            "ä¼šè©±çŠ¶æ…‹",
            is_active,
            help="ä¼šè©±ã®é€²è¡ŒçŠ¶æ³"
        )
    
    # åˆ¶å¾¡ãƒœã‚¿ãƒ³
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ§ ã‚¹ãƒãƒ¼ãƒˆè´å–é–‹å§‹", type="primary", disabled=status['is_listening']):
            if smart_buffer.start_smart_listening():
                st.success("ğŸ§ ã‚¹ãƒãƒ¼ãƒˆè´å–ã‚’é–‹å§‹ã—ã¾ã—ãŸ")
                st.info("ğŸ—£ï¸ ã‚†ã£ãã‚Šè©±ã—ã¦ãã ã•ã„ã€‚AIãŒæœ€å¾Œã¾ã§ãŠå¾…ã¡ã—ã¾ã™")
            else:
                st.warning("ã™ã§ã«è´å–ä¸­ã§ã™")
    
    with col2:
        if st.button("â¹ï¸ è´å–åœæ­¢", type="secondary", disabled=not status['is_listening']):
            if smart_buffer.stop_smart_listening():
                st.info("â¹ï¸ ã‚¹ãƒãƒ¼ãƒˆè´å–ã‚’åœæ­¢ã—ã¾ã—ãŸ")
    
    # æ‰‹å‹•éŒ²éŸ³
    st.write("**æ‰‹å‹•éŒ²éŸ³ï¼ˆã‚¹ãƒãƒ¼ãƒˆãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ä»˜ãï¼‰**")
    col1, col2 = st.columns([2, 1])
    
    with col1:
        max_duration = st.slider("æœ€å¤§éŒ²éŸ³æ™‚é–“ï¼ˆç§’ï¼‰", 5, 60, 30)
    
    with col2:
        if st.button("ğŸ¤ éŒ²éŸ³é–‹å§‹", type="primary"):
            with st.spinner("ğŸ¤ ã‚¹ãƒãƒ¼ãƒˆéŒ²éŸ³ä¸­..."):
                result = smart_buffer.manual_record_with_buffer(max_duration)
                
                if result.get('text'):
                    st.success(f"âœ… èªè­˜çµæœ: {result['text']}")
                    st.info(f"â±ï¸ éŒ²éŸ³æ™‚é–“: {result['duration']:.1f}ç§’")
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›æ¬„ã«è‡ªå‹•å…¥åŠ›
                    st.session_state.smart_voice_text = result['text']
                else:
                    st.warning(f"âš ï¸ {result.get('error', 'éŸ³å£°ãŒèªè­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ')}")
    
    # GUIæ›´æ–°æƒ…å ±ã®è¡¨ç¤º
    gui_updates = smart_buffer.get_gui_updates()
    for update in gui_updates:
        if update['type'] == 'recognition_complete':
            st.success(f"ğŸ¤ è‡ªå‹•èªè­˜: {update['text']}")
            st.session_state.smart_voice_text = update['text']
    
    # æœ€å¾Œã®çµæœ
    if st.button("ğŸ“‹ æœ€å¾Œã®èªè­˜çµæœ"):
        if smart_buffer.last_recognition_result:
            st.info(f"æœ€å¾Œã®èªè­˜: {smart_buffer.last_recognition_result['text']}")
            st.info(f"æ™‚é–“: {smart_buffer.last_recognition_result['duration']:.1f}ç§’")
        else:
            st.info("èªè­˜çµæœãŒã‚ã‚Šã¾ã›ã‚“")
    
    # ä¼šè©±ã‚µãƒãƒªãƒ¼
    if st.button("ğŸ“Š ä¼šè©±ã‚µãƒãƒªãƒ¼"):
        summary = smart_buffer.get_conversation_summary()
        st.json(summary)
    
    # è©³ç´°ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    with st.expander("ğŸ” è©³ç´°ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"):
        st.json(status)
