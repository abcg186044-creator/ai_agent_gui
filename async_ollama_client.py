#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éåŒæœŸãƒ»ä¸¦åˆ—Ollamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
"""

import asyncio
import aiohttp
import json
import time
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass
from enum import Enum
import random

class ModelStatus(Enum):
    """ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    IDLE = "idle"
    BUSY = "busy"
    ERROR = "error"

@dataclass
class OllamaInstance:
    """Ollamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æƒ…å ±"""
    port: int
    model: str
    status: ModelStatus
    last_used: float
    current_task: Optional[str] = None
    response_time: float = 0.0

class AsyncOllamaClient:
    """éåŒæœŸOllamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, ports: List[int] = None, models: List[str] = None):
        self.ports = ports or [11434, 11435, 11436]
        self.models = models or ["llama3.2:3b", "llama3.1:8b", "qwen2.5:7b"]
        self.instances: Dict[int, OllamaInstance] = {}
        self.session = None
        self.request_queue = asyncio.Queue()
        self.processing = False
        
        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–
        self._initialize_instances()
    
    def _initialize_instances(self):
        """Ollamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–"""
        for port in self.ports:
            for model in self.models:
                instance_id = f"{port}_{model.replace(':', '_')}"
                self.instances[instance_id] = OllamaInstance(
                    port=port,
                    model=model,
                    status=ModelStatus.IDLE,
                    last_used=0.0
                )
    
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=300)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ‚äº†"""
        if self.session:
            await self.session.close()
    
    async def get_available_instance(self) -> Optional[OllamaInstance]:
        """åˆ©ç”¨å¯èƒ½ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
        available_instances = [
            inst for inst in self.instances.values() 
            if inst.status == ModelStatus.IDLE
        ]
        
        if not available_instances:
            return None
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ãŒæœ€ã‚‚é€Ÿã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’é¸æŠ
        return min(available_instances, key=lambda x: x.response_time)
    
    async def generate_response_async(
        self, 
        prompt: str, 
        progress_callback: Optional[Callable] = None,
        preferred_model: Optional[str] = None
    ) -> Dict[str, Any]:
        """éåŒæœŸã§å¿œç­”ã‚’ç”Ÿæˆ"""
        start_time = time.time()
        
        if progress_callback:
            progress_callback({
                "step": "ğŸ” åˆ©ç”¨å¯èƒ½ãªOllamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æ¤œç´¢ä¸­...",
                "progress": 0
            })
        
        # åˆ©ç”¨å¯èƒ½ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
        instance = await self.get_available_instance()
        
        if not instance:
            if progress_callback:
                progress_callback({
                    "step": "â³ ã™ã¹ã¦ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒä½¿ç”¨ä¸­ã€‚å¾…æ©Ÿä¸­...",
                    "progress": 10
                })
            
            # å°‘ã—å¾…ã£ã¦å†è©¦è¡Œ
            await asyncio.sleep(0.5)
            instance = await self.get_available_instance()
            
            if not instance:
                return {
                    "success": False,
                    "error": "åˆ©ç”¨å¯èƒ½ãªOllamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“",
                    "elapsed_time": time.time() - start_time
                }
        
        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ãƒ“ã‚¸ãƒ¼çŠ¶æ…‹ã«è¨­å®š
        instance.status = ModelStatus.BUSY
        instance.current_task = prompt[:50] + "..."
        
        try:
            if progress_callback:
                progress_callback({
                    "step": f"ğŸ”Œ ãƒãƒ¼ãƒˆ {instance.port} ã® {instance.model} ã§å®Ÿè¡Œä¸­...",
                    "progress": 20,
                    "port": instance.port,
                    "model": instance.model
                })
            
            # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            response = await self._call_ollama_api(
                instance.port, 
                instance.model, 
                prompt, 
                progress_callback
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ã‚’è¨˜éŒ²
            instance.response_time = time.time() - start_time
            instance.last_used = time.time()
            
            if progress_callback:
                progress_callback({
                    "step": f"âœ… å¿œç­”ç”Ÿæˆå®Œäº† (ãƒãƒ¼ãƒˆ: {instance.port})",
                    "progress": 100,
                    "port": instance.port,
                    "model": instance.model,
                    "response_time": instance.response_time
                })
            
            return {
                "success": True,
                "response": response,
                "model": instance.model,
                "port": instance.port,
                "elapsed_time": time.time() - start_time
            }
            
        except Exception as e:
            instance.status = ModelStatus.ERROR
            elapsed = time.time() - start_time
            
            if progress_callback:
                progress_callback({
                    "step": f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ (ãƒãƒ¼ãƒˆ: {instance.port}): {str(e)}",
                    "progress": 0,
                    "port": instance.port,
                    "error": str(e)
                })
            
            return {
                "success": False,
                "error": str(e),
                "port": instance.port,
                "elapsed_time": elapsed
            }
        
        finally:
            # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è§£æ”¾
            if instance.status == ModelStatus.BUSY:
                instance.status = ModelStatus.IDLE
            instance.current_task = None
    
    async def _call_ollama_api(
        self, 
        port: int, 
        model: str, 
        prompt: str, 
        progress_callback: Optional[Callable] = None
    ) -> str:
        """Ollama APIã‚’å‘¼ã³å‡ºã—"""
        url = f"http://localhost:{port}/api/generate"
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "max_tokens": 2048
            }
        }
        
        if progress_callback:
            progress_callback({
                "step": f"ğŸ“¡ APIãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­... (ãƒãƒ¼ãƒˆ: {port})",
                "progress": 40,
                "port": port
            })
        
        async with self.session.post(url, json=payload) as response:
            if response.status != 200:
                raise Exception(f"APIã‚¨ãƒ©ãƒ¼: {response.status}")
            
            if progress_callback:
                progress_callback({
                    "step": f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«å¿œç­”ã‚’å¾…æ©Ÿä¸­... (ãƒãƒ¼ãƒˆ: {port})",
                    "progress": 70,
                    "port": port
                })
            
            result = await response.json()
            
            if "response" not in result:
                raise Exception("ä¸æ­£ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼")
            
            return result["response"]
    
    async def generate_parallel_responses(
        self, 
        prompts: List[str], 
        progress_callback: Optional[Callable] = None
    ) -> List[Dict[str, Any]]:
        """ä¸¦åˆ—ã§è¤‡æ•°ã®å¿œç­”ã‚’ç”Ÿæˆ"""
        if progress_callback:
            progress_callback({
                "step": f"ğŸš€ {len(prompts)}å€‹ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œä¸­...",
                "progress": 0,
                "total_tasks": len(prompts)
            })
        
        # ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
        tasks = []
        for i, prompt in enumerate(prompts):
            def make_progress_callback(task_id):
                def callback(progress_info):
                    if progress_callback:
                        new_info = progress_info.copy()
                        new_info["task_id"] = task_id
                        progress_callback(new_info)
                return callback
            
            task = asyncio.create_task(
                self.generate_response_async(prompt, make_progress_callback(i))
            )
            tasks.append(task)
        
        # ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµæœã‚’æ•´å½¢
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "success": False,
                    "error": str(result),
                    "task_id": i
                })
            else:
                result["task_id"] = i
                formatted_results.append(result)
        
        if progress_callback:
            progress_callback({
                "step": "âœ… ã™ã¹ã¦ã®ä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Œäº†",
                "progress": 100,
                "total_tasks": len(prompts)
            })
        
        return formatted_results
    
    async def get_instance_status(self) -> Dict[str, Any]:
        """ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        status = {
            "total_instances": len(self.instances),
            "idle_instances": len([i for i in self.instances.values() if i.status == ModelStatus.IDLE]),
            "busy_instances": len([i for i in self.instances.values() if i.status == ModelStatus.BUSY]),
            "error_instances": len([i for i in self.instances.values() if i.status == ModelStatus.ERROR]),
            "instances": []
        }
        
        for instance_id, instance in self.instances.items():
            status["instances"].append({
                "id": instance_id,
                "port": instance.port,
                "model": instance.model,
                "status": instance.status.value,
                "current_task": instance.current_task,
                "last_used": instance.last_used,
                "response_time": instance.response_time
            })
        
        return status

# ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    async def test_async_client():
        """éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ"""
        print("ğŸš€ éåŒæœŸOllamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 60)
        
        async with AsyncOllamaClient() as client:
            # å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ
            print("\nğŸ“‹ å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ:")
            def progress_callback(progress_info):
                print(f"   {progress_info['step']} ({progress_info['progress']:.1f}%)")
            
            result = await client.generate_response_async(
                "Pythonã§ç°¡å˜ãªé›»å“ã‚¢ãƒ—ãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„",
                progress_callback
            )
            
            if result["success"]:
                print(f"âœ… æˆåŠŸ: {result['model']} (ãƒãƒ¼ãƒˆ: {result['port']}, æ™‚é–“: {result['elapsed_time']:.2f}ç§’)")
                print(f"ğŸ“ å¿œç­”: {result['response'][:100]}...")
            else:
                print(f"âŒ å¤±æ•—: {result['error']}")
            
            # ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ
            print("\nğŸ“‹ ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ:")
            prompts = [
                "HTMLã§é›»å“ã‚¢ãƒ—ãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„",
                "Androidã§é›»å“ã‚¢ãƒ—ãƒªã‚’é–‹ç™ºã—ã¦ãã ã•ã„",
                "Reactã§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„"
            ]
            
            def parallel_progress_callback(progress_info):
                if "task_id" in progress_info:
                    print(f"   ã‚¿ã‚¹ã‚¯{progress_info['task_id']}: {progress_info['step']} ({progress_info['progress']:.1f}%)")
            
            results = await client.generate_parallel_responses(prompts, parallel_progress_callback)
            
            for i, result in enumerate(results):
                if result["success"]:
                    print(f"âœ… ã‚¿ã‚¹ã‚¯{i}: {result['model']} (ãƒãƒ¼ãƒˆ: {result['port']}, æ™‚é–“: {result['elapsed_time']:.2f}ç§’)")
                else:
                    print(f"âŒ ã‚¿ã‚¹ã‚¯{i}: {result['error']}")
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
            print("\nğŸ“Š æœ€çµ‚ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:")
            status = await client.get_instance_status()
            print(f"   ç·ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: {status['total_instances']}")
            print(f"   ã‚¢ã‚¤ãƒ‰ãƒ«: {status['idle_instances']}")
            print(f"   ãƒ“ã‚¸ãƒ¼: {status['busy_instances']}")
            print(f"   ã‚¨ãƒ©ãƒ¼: {status['error_instances']}")
        
        print("\nğŸ‰ éåŒæœŸOllamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    
    asyncio.run(test_async_client())
