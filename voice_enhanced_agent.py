#!/usr/bin/env python3
"""
Voice Enhanced Autonomous AI Agent - éŸ³å£°å…¥åŠ›ãƒ»æ„Ÿæƒ…åˆ†æãƒ»éŸ³å£°åˆæˆå­¦ç¿’
"""

import streamlit as st
import sys
import os
import json
import tempfile
import time
import threading
import queue
import wave
import numpy as np
from datetime import datetime
import hashlib

# åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import ollama
    import faster_whisper
    import pyttsx3
    import pyautogui
    import chromadb
    from sentence_transformers import SentenceTransformer
    import faiss
    import psutil
    import schedule
    import sounddevice as sd
    import soundfile as sf
    import speech_recognition as sr
except ImportError as e:
    st.error(f"âŒ å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    st.stop()

# è¨­å®š
class Config:
    MAIN_MODEL = "llama3.2"
    VISION_MODEL = "llama3.2-vision"
    EMBEDD_MODEL = "all-MiniLM-L6-v2"
    
    # éŸ³å£°è¨­å®š
    AUDIO_SAMPLE_RATE = 16000
    AUDIO_CHANNELS = 1
    AUDIO_FORMAT = "int16"
    AUDIO_CHUNK_DURATION = 30  # ç§’
    
    # Whisperè¨­å®š
    WHISPER_MODEL = "large-v3"
    
    # éŸ³å£°èªè­˜è¨­å®š
    VAD_MODEL = "silero-vad"
    
    # éŸ³å£°åˆæˆè¨­å®š
    TTS_ENGINE = "sapi5"  # Windowsæ¨™æº–
    TTS_RATE = 200
    TTS_VOLUME = 0.9
    
    # éŸ³å£°ç‰¹å¾´é‡
    VOICE_FEATURES = {
        "pitch": "ãƒ”ãƒƒãƒï¼ˆéŸ³ã®é«˜ã•ï¼‰",
        "energy": "ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆå£°ã®å¤§ãã•ï¼‰",
        "spectral_centroid": "éŸ³è‰²ï¼ˆæ˜ã‚‹ã•ï¼‰",
        "zero_crossing_rate": "ç„¡éŸ³åŒºé–“ã®äº¤å·®",
        "speaking_rate": "è©±ã™é€Ÿåº¦"
    }
    
    # æ„Ÿæƒ…åˆ†æè¨­å®š
    EMOTION_KEYWORDS = {
        "positive": ["å¬‰ã—ã„", "æ¥½ã—ã„", "ã‚ã‚ŠãŒã¨ã†", "ç´ æ™´ã‚‰ã—ã„", "æˆåŠŸ", "æº€è¶³", "æœ€é«˜", "è‰¯ã„", "ç´ æ•µ"],
        "negative": ["æ‚²ã—ã„", "ã¤ã‚‰ã„", "æ®‹å¿µ", "å¤±æ•—", "å›°ã‚‹", "å¤§å¤‰", "æœ€æ‚ª", "å«Œã„", "ç–²ã‚ŒãŸ", "ä¸å®‰", "å¿ƒé…"],
        "neutral": ["æ™®é€š", "é€šå¸¸", "ã¾ã‚", "ãªã‚‹ã»ã©", "ãã†", "ã©ã†"]
    }

class VoiceInputHandler:
    """éŸ³å£°å…¥åŠ›ãƒãƒ³ãƒ‰ãƒ©"""
    
    def __init__(self):
        self.is_recording = False
        self.audio_queue = queue.Queue()
        self.recording_thread = None
        self.whisper_model = None
        self.vad_model = None
        
    def initialize(self):
        """éŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            self.whisper_model = faster_whisper.WhisperModel(
                Config.WHISPER_MODEL,
                device="cuda" if torch.cuda.is_available() else "cpu",
                compute_type="float16"
            )
            
            # VADãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            try:
                import silero_vad
                self.vad_model = silero_vad.VAD(model_path=Config.VAD_MODEL)
            except ImportError:
                st.warning("âš ï¸ VADãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚éŸ³å£°åŒºåˆ¥ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                self.vad_model = None
            
            return True
        except Exception as e:
            st.error(f"âŒ éŸ³å£°å…¥åŠ›ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def start_recording(self):
        """éŒ²éŸ³é–‹å§‹"""
        if self.is_recording:
            return False
        
        self.is_recording = True
        
        # éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        def audio_callback(indata, frame_count, time_info):
            self.audio_queue.put(indata)
        
        try:
            self.recording_thread = threading.Thread(
                target=self._record_audio,
                args=(audio_callback,),
                daemon=True
            )
            self.recording_thread.start()
            return True
        except Exception as e:
            st.error(f"âŒ éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _record_audio(self, callback):
        """éŸ³å£°éŒ²éŸ³ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰"""
        try:
            with sd.InputStream(
                samplerate=Config.AUDIO_SAMPLE_RATE,
                channels=Config.AUDIO_CHANNELS,
                dtype=Config.AUDIO_FORMAT,
                blocksize=1024,
                callback=callback
            ) as stream:
                self.audio_queue.put(stream)
        except Exception as e:
            st.error(f"âŒ éŒ²éŸ³ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def stop_recording(self):
        """éŒ²éŸ³åœæ­¢"""
        if not self.is_recording:
            return False
        
        self.is_recording = False
        
        if self.recording_thread:
            self.recording_thread.join(timeout=5)
            self.recording_thread = None
    
    def get_audio_data(self):
        """éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        audio_data = []
        try:
            while not self.audio_queue.empty():
                audio_data.append(self.audio_queue.get())
        return audio_data
        except Exception as e:
            st.error(f"âŒ éŸ³å£°ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def transcribe_audio(self, audio_data):
        """éŸ³å£°èªè­˜"""
        try:
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’WAVãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›
            with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                with wave.open(tmp_file.name, 'wb') as wf:
                    wf.setnchannels(Config.AUDIO_CHANNELS)
                    wf.setsampwidth(Config.AUDIO_SAMPLE_RATE)
                    wf.setframerate(Config.AUDIO_SAMPLE_RATE)
                    wf.writeframes(audio_data)
                    wf.close()
                
                # Whisperã§è»¢è¨˜
                result = self.whisper_model.transcribe(
                    tmp_file.name,
                    language="ja",
                    word_timestamps=True,
                    temperature=0.0,
                    beam_size=5
                )
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                os.unlink(tmp_file.name)
                
                return result
        except Exception as e:
            return {"error": str(e)}
    
    def detect_voice_activity(self, audio_data):
        """éŸ³å£°æ´»å‹•æ¤œå‡º"""
        if not self.vad_model:
            return {"activity": "unknown"}
        
        try:
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã«å¤‰æ›
            audio_array = np.array(audio_data)
            
            # VADã§éŸ³å£°æ´»å‹•æ¤œå‡º
            speech_prob = self.vad_model(audio_array, sample_rate=Config.AUDIO_SAMPLE_RATE)
            
            if speech_prob > 0.5:
                return {"activity": "speaking"}
            else:
                return {"activity": "silent"}
        except Exception as e:
            return {"activity": "error", "message": str(e)}

class VoiceFeatureExtractor:
    """éŸ³å£°ç‰¹å¾´é‡æŠ½å‡º"""
    
    def __init__(self):
        self.sample_rate = Config.AUDIO_SAMPLE_RATE
        
    def extract_features(self, audio_data):
        """éŸ³å£°ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        try:
            audio_array = np.array(audio_data)
            
            # åŸºæœ¬çµ±è¨ˆ
            duration = len(audio_array) / self.sample_rate
            
            # ãƒ”ãƒƒãƒï¼ˆéŸ³ã®é«˜ã•ï¼‰
            pitches, magnitudes = librosa.pyin(audio_array, sr=self.sample_rate)
            avg_pitch = np.mean(pitches)
            
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆå£°ã®å¤§ãã•ï¼‰
            energy = np.sqrt(np.mean(audio_array**2))
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«ï¼ˆæ˜ã‚‹ã•ï¼‰
            spec = np.abs(np.fft(audio_array))
            spectral_centroid = np.mean(spec[:len(spec)//2])
            
            # ã‚¼ãƒ­ã‚¹ãƒ¬ãƒ¼ãƒˆï¼ˆè©±ã™é€Ÿåº¦ï¼‰
            zero_crossings = np.sum(audio_array[:-1] != 0) & (audio_array[1:] != 0)
            
            return {
                "duration": duration,
                "avg_pitch": avg_pitch,
                "energy": energy,
                "spectral_centroid": spectral_centroid,
                "zero_crossing_rate": zero_crossings,
                "speaking_rate": len(audio_array) / self.sample_rate if audio_array else 0
            }
        except Exception as e:
            return {"error": str(e)}

class EmotionAnalyzer:
    """æ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.emotion_keywords = Config.EMOTION_KEYWORDS
        
    def analyze_emotion(self, text, voice_features=None):
        """æ„Ÿæƒ…åˆ†æ"""
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆæ„Ÿæƒ…åˆ†æ
            text_lower = text.lower()
            text_sentiment = "neutral"
            text_score = 0.0
            
            positive_count = sum(1 for word in self.emotion_keywords["positive"] if word in text_lower)
            negative_count = sum(1 for word in self.emotion_keywords["negative"] if word in text_lower)
            
            if positive_count > negative_count:
                text_sentiment = "positive"
                text_score = min(1.0, positive_count / (positive_count + negative_count))
            elif negative_count > positive_count:
                text_sentiment = "negative"
                text_score = -min(1.0, negative_count / (positive_count + negative_count))
            
            # éŸ³å£°ç‰¹å¾´é‡ã‹ã‚‰ã®æ„Ÿæƒ…åˆ†æ
            voice_sentiment = "neutral"
            voice_score = 0.0
            
            if voice_features and "error" not in voice_features:
                # éŸ³å£°ã®é«˜ã•ã‚„é€Ÿã•ã‹ã‚‰æ„Ÿæƒ…ã‚’æ¨å®š
                if voice_features["avg_pitch"] > 200:  # é«˜ã„å£°
                    voice_sentiment = "excited"
                    voice_score = 0.8
                elif voice_features["energy"] > 0.7:  # å¤§ããªå£°
                    voice_sentiment = "angry"
                    voice_score = -0.6
                elif voice_features["speaking_rate"] > 4: 0:  # é€Ÿã„è©±
                    voice_sentiment = "nervous"
                    voice_score = 0.6
                elif voice_features["avg_pitch"] < 100: 0:  # ä½ã„å£°
                    voice_sentiment = "sad"
                    voice_score = -0.6
            
            # ç·åˆæ„Ÿæƒ…åˆ¤å®š
            if text_score > 0.5:
                final_sentiment = "positive"
                final_score = text_score * 0.7 + voice_score * 0.3
            elif text_score < -0.5:
                final_sentiment = "negative"
                final_score = text_score * 0.7 + voice_score * 0.3
            else:
                final_sentiment = "neutral"
                final_score = text_score * 0.7 + voice_score * 0.3
            
            return {
                "text_sentiment": text_sentiment,
                "text_score": text_score,
                "voice_sentiment": voice_sentiment,
                "voice_score": voice_score,
                "final_sentiment": final_sentiment,
                "confidence": max(abs(text_score), abs(voice_score))
            }
            
        except Exception as:
            return {"error": str(e)}

class VoiceSynthesisLearner:
    """éŸ³å£°åˆæˆå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.tts_engine = None
        self.voice_profiles = {}
        self.learning_data = {}
        
    def initialize(self):
        """éŸ³å£°åˆæˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            self.tts_engine = pyttsx3.init()
            self.tts_engine.setProperty('rate', str(Config.TTS_RATE))
            self.tts_engine.setProperty('volume', str(Config.TTS_VOLUME))
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            self._load_learning_data()
            
            return True
        except Exception as e:
            return False
    
    def _load_learning_data(self):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            learning_file = "voice_learning.json"
            if os.path.exists(learning_file):
                with open(learning_file, 'r', encoding='utf-8') as f:
                    self.learning_data = json.load(f)
        except Exception:
            self.learning_data = {}
    
    def _save_learning_data(self):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        try:
            learning_file = "voice_learning.json"
            with open(learning_file, 'w', encoding='utf-8') as f:
                json.dump(self.learning_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            st.error(f"âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def learn_voice_profile(self, text, voice_features):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å­¦ç¿’"""
        try:
            profile_id = hashlib.md5(f"{text}{datetime.now().isoformat()}".encode()).hexdigest()
            
            if profile_id not in self.voice_profiles:
                self.voice_profiles[profile_id] = {
                    "text": text,
                    "voice_features": voice_features,
                    "created_at": datetime.now().isoformat(),
                    "usage_count": 0
                }
                self.learning_data["profiles"][profile_id] = self.voice_profiles[profile_id]
                self._save_learning_data()
            
        except Exception as e:
            st.error(f"âŒ éŸ³å£°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def synthesize_speech(self, text, profile_id=None):
        """å­¦ç¿’ã—ãŸéŸ³å£°ã§åˆæˆ"""
        try:
            if profile_id and profile_id in self.voice_profiles:
                profile = self.voice_profiles[profile_id]
                # å­¦ç¿’ã—ãŸç‰¹å¾´ã‚’åæ˜ 
                if profile["voice_features"]:
                    if profile["voice_features"]["avg_pitch"] > 0:
                        self.tts_engine.setProperty('rate', str(int(profile["voice_features"]["avg_pitch"]))
                
                self.learning_data["profiles"][profile_id]["usage_count"] += 1
                self._save_learning_data()
                
                return True
        else:
            return False
        except Exception as e:
            st.error(f"âŒ éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

class VoiceEnhancedAIAgent:
    """éŸ³å£°å¼·åŒ–è‡ªå¾‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.ollama_client = None
        self.voice_input = VoiceInputHandler()
        self.knowledge_base = None
        self.emotion_analyzer = EmotionAnalyzer()
        self.voice_feature_extractor = VoiceFeatureExtractor()
        self.voice_synthesis_learner = VoiceSynthesisLearner()
        self.current_conversation = []
        
    def initialize(self):
        """AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–"""
        try:
            # OllamaåˆæœŸåŒ–
            self.ollama_client = ollama.Client()
            
            # å„ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            self.voice_input.initialize()
            self.emotion_analyzer.initialize()
            self.voice_feature_extractor.initialize()
            self.voice_synthesis_learner.initialize()
            
            return True
        except Exception as e:
            return False
    
    def process_voice_input(self, user_input):
        """éŸ³å£°å…¥åŠ›å‡¦ç†"""
        try:
            # éŒ²éŸ³é–‹å§‹
            if not self.voice_input.start_recording():
                st.error("âŒ éŒ²éŸ³é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return "éŸ³å£°å…¥åŠ›ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
            
            st.info("ğŸ¤ï¸ éŒ²éŸ³ä¸­...è©±ã—ã¦ãã ã•ã„")
            
            # éŒ²éŸ³åœæ­¢
            time.sleep(3)  # 3ç§’é–“éŒ²éŸ³
            
            audio_data = self.voice_input.get_audio_data()
            self.voice_input.stop_recording()
            
            if not audio_data:
                return "éŸ³å£°ãŒèªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            
            # éŸ³å£°èªè­˜
            transcription = self.voice_input.transcribe_audio(audio_data)
            
            if "error" in transcription:
                return f"éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {transcription['error']}"
            
            # éŸ³å£°ç‰¹å¾´é‡æŠ½å‡º
            voice_features = self.voice_feature_extractor.extract_features(audio_data)
            
            # æ„Ÿæƒ…åˆ†æ
            emotion_result = self.emotion_analyzer.analyze_emotion(transcription["text"], voice_features)
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ä¿å­˜
            self.current_conversation.append({
                "timestamp": datetime.now().isoformat(),
                "user_input": user_input,
                "transcription": transcription,
                "voice_features": voice_features,
                "emotion_analysis": emotion_result
            })
            
            # éŸ³å£°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å­¦ç¿’
            self.voice_synthesis_learner.learn_voice_profile(transcription["text"], voice_features)
            
            return {
                "transcription": transcription,
                "voice_features": voice_features,
                "emotion_analysis": emotion_result
            }
            
        except Exception as e:
            return f"âŒ éŸ³å£°å…¥åŠ›å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"

def render_voice_interface(ai_agent):
    """éŸ³å£°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ¤ï¸ éŸ³å£°å…¥åŠ›ãƒ»éŸ³å£°åˆ†æ")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ¤ï¸ éŸ³å£°å…¥åŠ›")
        
        # éŒ²éŸ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
        if st.button("ğŸ¤ï¸ éŒ²éŸ³é–‹å§‹", key="start_recording"):
            result = ai_agent.process_voice_input("éŸ³å£°å…¥åŠ›ãƒ†ã‚¹ãƒˆ")
            
            if "error" in result:
                st.error(result)
            elif "transcription" in result:
                st.success(f"âœ… éŸ³å£°èªè­˜å®Œäº†: {result['transcription']['text'][:100]}...")
            else:
                st.success(result)
        
        # éŸ³å£°å…¥åŠ›ä¸­ã®è¡¨ç¤º
        if ai_agent.voice_input.is_recording:
            st.info("ğŸ”´ éŒ²éŸ³ä¸­... è©±ã‚Šè¾¼ã‚“ã§ãã ã•ã„")
            
        # éŸ³å£°ç‰¹å¾´é‡è¡¨ç¤º
        if st.session_state.get("last_voice_features"):
            features = st.session_state["last_voice_features"]
            st.subheader("ğŸ“Š éŸ³å£°ç‰¹å¾´é‡")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("å¹³å‡ãƒ”ãƒƒãƒ", f"{features['avg_pitch']:.1f} Hz")
                st.metric("ã‚¨ãƒãƒ«ã‚®ãƒ¼", f"{features['energy']:.2f}")
                st.metric("ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡å¿ƒ", f"{features['spectral_centroid']:.1f}")
                st.metric("ã‚¼ãƒ­äº¤å·®", f"{features['zero_crossing_rate']}")
            
            with col2:
                st.metric("è©±ã™é€Ÿåº¦", f"{features['speaking_rate']:.1f} å­—/ç§’")
                st.metric("éŒ²éŸ³æ™‚é–“", f"{features['duration']:.1f} ç§’")
        
        # æ„Ÿæƒ…åˆ†æçµæœ
        if st.session_state.get("last_emotion_analysis"):
            emotion = st.session_state["last_emotion_analysis"]
            st.subheader("ğŸ˜Š æ„Ÿæƒ…åˆ†æ")
            
            col1, col2 = st2, col3 = st.columns(3)
            
            with col1:
                st.metric("ãƒ†ã‚­ã‚¹ãƒˆæ„Ÿæƒ…", emotion["text_sentiment"])
                st.metric("ãƒ†ã‚­ã‚¹ãƒˆã‚¹ã‚³ã‚¢", f"{emotion['text_score']:.2f}")
            
            with col2:
                st.metric("éŸ³å£°æ„Ÿæƒ…", emotion["voice_sentiment"])
                st.metric("éŸ³å£°ã‚¹ã‚³ã‚¢", f"{emotion['voice_score']:.2f}")
            
            with col3:
                st.metric("æœ€çµ‚æ„Ÿæƒ…", emotion["final_sentiment"])
                st.metric("ä¿¡é ¼åº¦", f"{emotion['confidence']:.2f}")
    
    with col2:
        st.subheader("ğŸ™ï¸ éŸ³å£°åˆæˆå­¦ç¿’")
        
        # å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        profiles = ai_agent.voice_synthesis_learner.voice_profiles
        if profiles:
            st.write("**å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«**:")
            for profile_id, profile_data in list(profiles.values())[:5]:
                st.write(f"- {profile_id}: ä½¿ç”¨å›æ•°: {profile_data['usage_count']}")
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
        selected_profile = st.selectbox(
            "éŸ³å£°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ",
            options=["ãªã—"] + list(profiles.keys()),
            key="voice_profile_select"
        )
        
        # ãƒ†ã‚¹ãƒˆå…¥åŠ›
        test_text = st.text_area("ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ", key="test_text")
        
        if st.button("ğŸ”Š éŸ³å£°åˆæˆãƒ†ã‚¹ãƒˆ", key="test_synthesis"):
            if selected_profile != "ãªã—":
                success = ai_agent.voice_synthesis_learner.synthesize_speech(test_text, selected_profile)
                if success:
                    st.success("âœ… éŸ³å£°åˆæˆå®Œäº†")
                else:
                    st.error("âŒ éŸ³å£°åˆæˆå¤±æ•—")
            else:
                st.warning("âš ï¸ éŸ³å£°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
    
    with col2:
        st.subheader("ğŸ“Š å¯¾è©±å±¥æ­´")
        
        if ai_agent.current_conversation:
            st.write("**æœ€è¿‘ã®å¯¾è©±**:")
            for conv in ai_agent.current_conversation[-5:]:
                timestamp = datetime.fromisoformat(conv["timestamp"])
                time_str = timestamp.strftime('%H:%M:%S')
                
                st.write(f"**{time_str}**: {conv['transcription']['text'][:50]}...")
                
                # æ„Ÿæƒ…åˆ†æ
                if "emotion_analysis" in conv:
                    emotion = conv["emotion_analysis"]
                    st.write(f"æ„Ÿæƒ…: {emotion['final_sentiment']} (ä¿¡é ¼åº¦: {emotion['confidence']:.2f})")
    
    # éŸ³å£°å…¥åŠ›ä¸­ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
    if ai_agent.voice_input.is_recording:
        st.warning("ğŸ”´ éŒ²éŸ³ä¸­...")

def render_main_interface(ai_agent):
    """ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    st.header("ğŸ’¬ éŸ³å£°å¼·åŒ–AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    
    # ä¼šè©±å±¥æ­´
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    for i, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"], avatar="ğŸ¤–" if message["role"] == "assistant" else "ğŸ‘¤"):
            st.markdown(message["content"])
    
    # å…¥åŠ›ã‚¨ãƒªã‚¢
    col1, col2 = st.columns([3, 1])
    
    with col1:
        user_input = st.text_input(
            "ğŸ’¬ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›",
            placeholder="éŸ³å£°ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆã§å…¥åŠ›...",
            key="user_input"
        )
        
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            send_button = st.button("ğŸ’¬ é€ä¿¡", type="primary")
        
        with col2:
            voice_mode = st.checkbox("ğŸ¤ï¸ éŸ³å£°å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰", value=False)
            auto_speech = st.checkbox("ğŸ”Š éŸ³å£°èª­ã¿ä¸Šã’", value=True)
        
        with col3:
            show_analysis = st.checkbox("ğŸ” åˆ†æè¡¨ç¤º", value=False)
        
        # é€ä¿¡å‡¦ç†
        if send_button and user_input:
            if voice_mode:
                # éŸ³å£°å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰
                result = ai_agent.process_voice_input(user_input)
                st.session_state.messages.append({"role": "user", "content": f"ğŸ¤ï¸ éŸ³å£°å…¥åŠ›: {result['transcription']['text']}"})
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ¢ãƒ¼ãƒ‰
                # llama3.2ã§å¿œç­”ç”Ÿæˆ
                with st.spinner("ğŸ¤– AIå¿œç­”ç”Ÿæˆä¸­..."):
                    response = ai_agent.ollama_client.generate(
                        model=Config.MAIN_MODEL,
                        prompt=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›: {user_input}",
                        options={"temperature": 0.7, "max_tokens": 4096}
                    )
                    st.session_state.messages.append({"role": "assistant", "content": response['response']})
            
            # è‡ªå‹•éŸ³å£°èª­ã¿ä¸Šã’
            if auto_speech:
                try:
                    import pyttsx3
                    engine = pyttsx3.init()
                    engine.say(response['response'])
                    engine.runAndWait()
                except Exception as e:
                    st.error(f"éŸ³å£°èª­ã¿ä¸Šã’ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
            st.rerun()
    
    with col2:
        # VRMè¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        st.subheader("ğŸ‘¤ ã‚¢ãƒã‚¿ãƒ¼çŠ¶æ…‹")
        
        # ç¾åœ¨ã®æ„Ÿæƒ…çŠ¶æ…‹ã‚’è¡¨ç¤º
        if st.session_state.get("last_emotion_analysis"):
            emotion = st.session_state["last_emotion_analysis"]
            st.write(f"**ç¾åœ¨ã®æ„Ÿæƒ…**: {emotion['final_sentiment']}")
            
            # æ„Ÿæƒ…ã«å¿œã˜ãŸèª¿æ•´
            if emotion["final_sentiment"] == "positive":
                st.success("ğŸ˜Š ãƒã‚¸ãƒ†ã‚£ãƒ–ãªå¿œç­”ã§ã™ï¼")
            elif emotion["final_sentiment"] == "negative":
                st.warning("ğŸ˜” å…±æ„Ÿçš„ã«å¯¾å¿œã—ã¾ã™")
            elif emotion["final_sentiment"] == "neutral":
                st.info("ğŸ˜ é€šå¸¸ã®å¿œç­”ã§ã™")
        
        # ã‚¢ãƒã‚¿ãƒ¼è¡¨ç¤º
        avatar_display = f"""
        <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; margin: 10px 0;">
            <div style="color: white; font-weight: bold;">
                <div style="font-size: 64px; margin-bottom: 10px;">ğŸ˜</div>
                <div>AIã‚¢ãƒã‚¿ãƒ¼</div>
                <div>çŠ¶æ…‹: æº–å‚™ä¸­</div>
                <div>æ™‚åˆ»: {datetime.now().strftime('%H:%M:%S')}</div>
            </div>
        </div>
        """
        st.markdown(avatar_display, unsafe_allow_html=True)

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    st.set_page_config(
        page_title="ğŸ¤ï¸ Voice Enhanced AI Agent",
        page_icon="ğŸ¤ï¸",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ¤ï¸ AI Agent System - éŸ³å£°å¼·åŒ–ç‰ˆ")
    st.markdown("### ğŸ¯ éŸ³å£°å…¥åŠ›ãƒ»æ„Ÿæƒ…åˆ†æãƒ»éŸ³å£°åˆæˆå­¦ç¿’")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    if 'ai_agent' not in st.session_state:
        with st.spinner("ğŸ¤ï¸ éŸ³å£°å¼·åŒ–AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­..."):
            ai_agent = VoiceEnhancedAIAgent()
            if ai_agent.initialize():
                st.session_state.ai_agent = ai_agent
                st.success("âœ… éŸ³å£°å¼·åŒ–AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            else:
                st.error("âŒ AIã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
                st.stop()
    
    ai_agent = st.session_state.ai_agent
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.subheader("âš™ï¸ éŸ³å£°è¨­å®š")
        
        # éŸ³å£°èªè­˜è¨­å®š
        st.write("**Whisperãƒ¢ãƒ‡ãƒ«**: large-v3")
        st.write("**ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ**: 16000Hz")
        st.write("**ãƒãƒ£ãƒ³ãƒãƒ«**: 1")
        
        # éŸ³å£°åˆæˆè¨­å®š
        st.write("**ã‚¨ãƒ³ã‚¸ãƒ³**: SAPI5")
        st.write(f"ãƒ¬ãƒ¼ãƒˆ: {Config.TTS_RATE}")
        st.write(f"éŸ³é‡: {Config.TTS_VOLUME}")
        
        # å­¦ç¿’çŠ¶æ³
        profiles = ai_agent.voice_synthesis_learner.voice_profiles
        st.write(f"**å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«**: {len(profiles)}")
        
        # æœ€æ–°ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        if ai_agent.voice_synthesis_learner.learning_data:
            st.write("**å­¦ç¿’ãƒ‡ãƒ¼ã‚¿**:")
            st.json(ai_agent.voice_synthesis_learner.learning_data, indent=2)
    
    # ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–
    tab1, tab2 = st.tabs(["ğŸ’¬ éŸ³å£°å¼·åŒ–AIå¯¾è©±", "ğŸ¤ï¸ éŸ³å£°å…¥åŠ›ãƒ»åˆ†æ"])
    
    with tab1:
        render_main_interface(ai_agent)
    
    with tab2:
        render_voice_interface(ai_agent)
    
    # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
    st.markdown("---")
    st.markdown(f"**ğŸ¤ï¸ éŸ³å£°å¼·åŒ–AI**: {Config.MAIN_MODEL}")
    st.markdown(f"**æœ€çµ‚æ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    st.markdown("**ğŸ¯ ç›®æ¨™**: éŸ³å£°ã§è‡ªç„¶ãªå¯¾è©±ãƒ»æ„Ÿæƒ…ç†è§£ãƒ»éŸ³å£°åˆæˆå­¦ç¿’")

if __name__ == "__main__":
    main()
