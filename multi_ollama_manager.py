#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒãƒ«ãƒOllamaãƒãƒ¼ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import time
import threading
import random
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass
from enum import Enum
import queue

class OllamaInstance:
    """Ollamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç®¡ç†"""
    
    def __init__(self, port: int, name: str):
        self.port = port
        self.name = name
        self.is_busy = False
        self.last_used = time.time()
        self.request_queue = queue.Queue()
        self.lock = threading.Lock()
    
    def acquire(self) -> bool:
        """ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç¢ºä¿"""
        with self.lock:
            if not self.is_busy:
                self.is_busy = True
                self.last_used = time.time()
                return True
            return False
    
    def release(self):
        """ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è§£æ”¾"""
        with self.lock:
            self.is_busy = False
            self.last_used = time.time()

class MultiOllamaManager:
    """ãƒãƒ«ãƒOllamaãƒãƒ¼ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, base_port: int = 11434, max_instances: int = 3):
        self.base_port = base_port
        self.max_instances = max_instances
        self.instances = []
        self.current_index = 0
        
        # è¤‡æ•°ã®Ollamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–
        self._initialize_instances()
    
    def _initialize_instances(self):
        """Ollamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–"""
        for i in range(self.max_instances):
            port = self.base_port + i
            instance = OllamaInstance(port, f"ollama_{i}")
            self.instances.append(instance)
    
    def get_available_instance(self) -> Optional[OllamaInstance]:
        """åˆ©ç”¨å¯èƒ½ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
        # ã¾ãšç©ºã„ã¦ã„ã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æ¢ã™
        for instance in self.instances:
            if instance.acquire():
                return instance
        
        # ã™ã¹ã¦ Busy ã®å ´åˆã¯æœ€ã‚‚å¤ãä½¿ç”¨ã•ã‚ŒãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å¾…æ©Ÿ
        oldest_instance = min(self.instances, key=lambda x: x.last_used)
        if oldest_instance.acquire():
            return oldest_instance
        
        return None
    
    def release_instance(self, instance: OllamaInstance):
        """ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è§£æ”¾"""
        instance.release()
    
    def get_instance_status(self) -> Dict[str, Any]:
        """ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        return {
            "total_instances": len(self.instances),
            "busy_instances": sum(1 for i in self.instances if i.is_busy),
            "available_instances": sum(1 for i in self.instances if not i.is_busy),
            "instances": [
                {
                    "name": i.name,
                    "port": i.port,
                    "is_busy": i.is_busy,
                    "last_used": i.last_used
                }
                for i in self.instances
            ]
        }

class SafeOllamaClient:
    """å®‰å…¨ãªOllamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, port: int, timeout: int = 60, model: str = "llama3.2:3b"):
        self.port = port
        self.timeout = timeout
        self.model = model
        self.base_url = f"http://localhost:{port}"
    
    async def generate_response_async(self, prompt: str, progress_callback: Optional[Callable] = None) -> str:
        """éåŒæœŸã§å¿œç­”ã‚’ç”Ÿæˆ"""
        try:
            # å®Ÿéš›ã®Ollama APIå‘¼ã³å‡ºã—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            if progress_callback:
                progress_callback({
                    "step": f"ğŸ” Ollamaã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šä¸­... (ãƒãƒ¼ãƒˆ: {self.port})",
                    "progress": 0,
                    "port": self.port,
                    "model": self.model
                })
            
            await asyncio.sleep(0.1)  # æ¥ç¶šé…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            
            if progress_callback:
                progress_callback({
                    "step": f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ä¸­... ({self.model})",
                    "progress": 20,
                    "port": self.port
                })
            
            await asyncio.sleep(0.2)  # å‡¦ç†é…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            
            if progress_callback:
                progress_callback({
                    "step": f"ğŸ¤– AIãƒ¢ãƒ‡ãƒ«ãŒå¿œç­”ã‚’ç”Ÿæˆä¸­...",
                    "progress": 60,
                    "port": self.port
                })
            
            await asyncio.sleep(0.1)  # ç”Ÿæˆé…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            
            response = self._generate_mock_response(prompt)
            
            if progress_callback:
                progress_callback({
                    "step": f"âœ… å¿œç­”ç”Ÿæˆå®Œäº† (ãƒãƒ¼ãƒˆ: {self.port})",
                    "progress": 100,
                    "port": self.port
                })
            
            return response
            
        except Exception as e:
            if progress_callback:
                progress_callback({
                    "step": f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ (ãƒãƒ¼ãƒˆ: {self.port}): {str(e)}",
                    "progress": 0,
                    "port": self.port,
                    "error": str(e)
                })
            return f"Ollama APIã‚¨ãƒ©ãƒ¼ (ãƒãƒ¼ãƒˆ: {self.port}): {str(e)}"
    
    def _generate_mock_response(self, prompt: str) -> str:
        """ãƒ¢ãƒƒã‚¯å¿œç­”ã‚’ç”Ÿæˆ"""
        if "é›»å“" in prompt:
            return f'''# Ollamaå¿œç­” (ãƒãƒ¼ãƒˆ: {self.port}, ãƒ¢ãƒ‡ãƒ«: {self.model})

## Python GUIé›»å“ã‚¢ãƒ—ãƒª

å®Œå…¨ãªé›»å“ã‚¢ãƒ—ãƒªã®ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚

```python
import tkinter as tk

class Calculator:
    def __init__(self, root):
        self.root = root
        self.root.title("é›»å“")
        self.setup_ui()
    
    def setup_ui(self):
        # UIå®Ÿè£…
        pass

if __name__ == "__main__":
    app = Calculator()
    app.run()
```

## å®Ÿè¡Œæ–¹æ³•
1. ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜
2. å®Ÿè¡Œã—ã¦èµ·å‹•'''
        
        return f"# Ollamaå¿œç­” (ãƒãƒ¼ãƒˆ: {self.port})\n\n{prompt} ã«ã¤ã„ã¦ã®å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"

class AsyncMultiOllamaSystem:
    """éåŒæœŸãƒãƒ«ãƒOllamaã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, max_instances: int = 3):
        self.ollama_manager = MultiOllamaManager(max_instances=max_instances)
        self.semaphore = asyncio.Semaphore(max_instances)
    
    async def generate_response_async(self, prompt: str, task_description: str = "", progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """éåŒæœŸã§å¿œç­”ã‚’ç”Ÿæˆï¼ˆãƒãƒ¼ãƒˆç«¶åˆã‚’å›é¿ï¼‰"""
        start_time = time.time()
        
        if progress_callback:
            progress_callback({
                "step": "ğŸš€ ãƒãƒ«ãƒOllamaã‚·ã‚¹ãƒ†ãƒ ã‚’èµ·å‹•ä¸­...",
                "progress": 0
            })
        
        # åˆ©ç”¨å¯èƒ½ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
        instance = self.ollama_manager.get_available_instance()
        
        if not instance:
            if progress_callback:
                progress_callback({
                    "step": "â³ ã™ã¹ã¦ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒä½¿ç”¨ä¸­ã€‚å¾…æ©Ÿä¸­...",
                    "progress": 10
                })
            
            # å°‘ã—å¾…ã£ã¦å†è©¦è¡Œ
            await asyncio.sleep(0.5)
            instance = self.ollama_manager.get_available_instance()
            
            if not instance:
                # ãã‚Œã§ã‚‚å–å¾—ã§ããªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
                elapsed = time.time() - start_time
                return {
                    "success": False,
                    "error": "ã™ã¹ã¦ã®Ollamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒãƒ“ã‚¸ãƒ¼çŠ¶æ…‹ã§ã™",
                    "elapsed_time": elapsed,
                    "port_status": self.ollama_manager.get_instance_status()
                }
        
        try:
            if progress_callback:
                progress_callback({
                    "step": f"ğŸ”Œ ãƒãƒ¼ãƒˆ {instance.port} ã‚’ä½¿ç”¨ã—ã¦å‡¦ç†ä¸­...",
                    "progress": 20,
                    "port": instance.port
                })
            
            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆã—ã¦å®Ÿè¡Œ
            client = SafeOllamaClient(
                port=instance.port,
                timeout=60,
                model="llama3.2:3b"
            )
            
            response = await client.generate_response_async(prompt, progress_callback)
            
            elapsed = time.time() - start_time
            
            if progress_callback:
                progress_callback({
                    "step": f"âœ… ãƒãƒ¼ãƒˆ {instance.port} ã§å‡¦ç†å®Œäº†",
                    "progress": 100,
                    "port": instance.port,
                    "total_time": elapsed
                })
            
            return {
                "success": True,
                "response": response,
                "elapsed_time": elapsed,
                "port": instance.port,
                "model": client.model,
                "port_status": self.ollama_manager.get_instance_status()
            }
            
        except Exception as e:
            elapsed = time.time() - start_time
            return {
                "success": False,
                "error": str(e),
                "elapsed_time": elapsed,
                "port": instance.port if instance else None,
                "port_status": self.ollama_manager.get_instance_status()
            }
        
        finally:
            # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è§£æ”¾
            if instance:
                self.ollama_manager.release_instance(instance)
    
    def generate_response_sync(self, prompt: str, task_description: str = "", progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """åŒæœŸå®Ÿè¡Œ"""
        return asyncio.run(self.generate_response_async(prompt, task_description, progress_callback))

# ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    system = AsyncMultiOllamaSystem(max_instances=3)
    
    test_cases = [
        ("Pythonã§GUIã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦æ“ä½œã§ãã‚‹é›»å“ã‚¢ãƒ—ãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„", "Python GUIé›»å“ã‚¢ãƒ—ãƒªé–‹ç™º"),
        ("HTMLã§é›»å“ã‚¢ãƒ—ãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„", "Webé›»å“ã‚¢ãƒ—ãƒªé–‹ç™º"),
        ("Androidã§é›»å“ã‚¢ãƒ—ãƒªã‚’é–‹ç™ºã—ã¦ãã ã•ã„", "Androidé›»å“ã‚¢ãƒ—ãƒªé–‹ç™º"),
        ("è¤‡é›‘ãªæ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„", "æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ"),
        ("Reactã§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„", "Reactãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰é–‹ç™º")
    ]
    
    print("ğŸš€ ãƒãƒ«ãƒOllamaãƒãƒ¼ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    async def test_parallel():
        """ä¸¦åˆ—ãƒ†ã‚¹ãƒˆ"""
        tasks = []
        
        for i, (prompt, task) in enumerate(test_cases, 1):
            def progress_callback_factory(test_id):
                def progress_callback(progress_info):
                    timestamp = time.strftime("%H:%M:%S")
                    print(f"[{timestamp}] ğŸ“Š ãƒ†ã‚¹ãƒˆ{test_id}: {progress_info['step']} ({progress_info['progress']:.1f}%)")
                    if 'port' in progress_info:
                        print(f"           ğŸ”Œ ãƒãƒ¼ãƒˆ: {progress_info['port']}")
                    if 'error' in progress_info:
                        print(f"           âŒ ã‚¨ãƒ©ãƒ¼: {progress_info['error']}")
                    print("-" * 30)
                return progress_callback
            
            task = asyncio.create_task(
                system.generate_response_async(prompt, task, progress_callback_factory(i))
            )
            tasks.append(task)
        
        # ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        print(f"\nğŸ“Š ä¸¦åˆ—å®Ÿè¡Œçµæœ:")
        for i, result in enumerate(results, 1):
            if isinstance(result, Exception):
                print(f"   ãƒ†ã‚¹ãƒˆ{i}: âŒ ä¾‹å¤– - {str(result)}")
            elif result['success']:
                print(f"   ãƒ†ã‚¹ãƒˆ{i}: âœ… æˆåŠŸ (ãƒãƒ¼ãƒˆ: {result['port']}, æ™‚é–“: {result['elapsed_time']:.2f}ç§’)")
            else:
                print(f"   ãƒ†ã‚¹ãƒˆ{i}: âŒ å¤±æ•— - {result['error']}")
        
        # æœ€çµ‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        print(f"\nğŸ“Š æœ€çµ‚ãƒãƒ¼ãƒˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:")
        status = system.ollama_manager.get_instance_status()
        print(f"   ç·ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: {status['total_instances']}")
        print(f"   ãƒ“ã‚¸ãƒ¼: {status['busy_instances']}")
        print(f"   åˆ©ç”¨å¯èƒ½: {status['available_instances']}")
        
        for instance in status['instances']:
            status_text = "ğŸ”´ ä½¿ç”¨ä¸­" if instance['is_busy'] else "ğŸŸ¢ åˆ©ç”¨å¯èƒ½"
            print(f"   {instance['name']} (ãƒãƒ¼ãƒˆ: {instance['port']}): {status_text}")
    
    # ä¸¦åˆ—ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(test_parallel())
    
    print(f"\nğŸ‰ ãƒãƒ«ãƒOllamaãƒãƒ¼ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
